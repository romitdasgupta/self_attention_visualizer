<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Self-Attention Visualizer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .controls {
            padding: 25px;
            background: #f8f9fa;
            border-bottom: 2px solid #dee2e6;
        }

        .control-group {
            margin-bottom: 20px;
        }

        label {
            display: block;
            font-weight: 600;
            margin-bottom: 8px;
            color: #495057;
        }

        input[type="text"],
        input[type="number"],
        select {
            width: 100%;
            padding: 10px;
            border: 2px solid #dee2e6;
            border-radius: 6px;
            font-size: 14px;
            transition: border-color 0.3s;
        }

        input[type="text"]:focus,
        input[type="number"]:focus,
        select:focus {
            outline: none;
            border-color: #667eea;
        }

        .control-row {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }

        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        button:active {
            transform: translateY(0);
        }

        .content {
            padding: 25px;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }

        .section-title {
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #495057;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background: #667eea;
            border-radius: 50%;
        }

        .visualization-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 15px;
        }

        .viz-card {
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .viz-card h3 {
            margin-bottom: 12px;
            color: #495057;
            font-size: 1.1em;
        }

        canvas {
            display: block;
            width: 100%;
            border-radius: 4px;
            border: 1px solid #dee2e6;
        }

        .matrix-display {
            font-family: 'Courier New', monospace;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 12px;
            line-height: 1.6;
        }

        .matrix-row {
            white-space: nowrap;
        }

        .matrix-value {
            display: inline-block;
            min-width: 60px;
            text-align: right;
            padding: 2px 4px;
        }

        .token-display {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin: 15px 0;
        }

        .token {
            background: white;
            padding: 8px 16px;
            border-radius: 20px;
            border: 2px solid #667eea;
            font-weight: 600;
            color: #667eea;
        }

        .math-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 6px;
            border-left: 3px solid #667eea;
        }

        .math-step h4 {
            color: #667eea;
            margin-bottom: 8px;
        }

        .formula {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            margin: 8px 0;
            overflow-x: auto;
        }

        .info-box {
            background: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: #1976D2;
        }

        .attention-grid {
            display: grid;
            gap: 2px;
            margin: 15px 0;
        }

        .attention-cell {
            padding: 8px;
            text-align: center;
            font-size: 11px;
            border-radius: 2px;
            color: white;
            font-weight: 600;
        }

        .head-container {
            margin-bottom: 20px;
            padding: 15px;
            background: white;
            border-radius: 8px;
            border: 2px solid #dee2e6;
        }

        .head-title {
            font-weight: 700;
            color: #667eea;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
            border-bottom: 2px solid #dee2e6;
        }

        .tab {
            padding: 10px 20px;
            background: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            cursor: pointer;
            font-weight: 600;
            color: #6c757d;
            transition: all 0.3s;
        }

        .tab.active {
            color: #667eea;
            border-bottom-color: #667eea;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .flow-diagram {
            display: flex;
            align-items: center;
            justify-content: space-around;
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
            overflow-x: auto;
        }

        .flow-step {
            text-align: center;
            padding: 15px;
            min-width: 120px;
        }

        .flow-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            font-weight: 600;
            margin-bottom: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .flow-arrow {
            font-size: 2em;
            color: #667eea;
            margin: 0 10px;
        }

        .qkv-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .qkv-card {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .qkv-card h3 {
            margin-bottom: 15px;
            color: #495057;
        }

        .qkv-query {
            border-left: 4px solid #FF6B6B;
        }

        .qkv-key {
            border-left: 4px solid #4ECDC4;
        }

        .qkv-value {
            border-left: 4px solid #95E1D3;
        }

        .vector-display {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            font-size: 11px;
            margin: 8px 0;
            overflow-x: auto;
            max-height: 150px;
            overflow-y: auto;
        }

        .vector-row {
            margin: 4px 0;
        }

        .interaction-canvas {
            width: 100%;
            max-width: 600px;
            margin: 20px auto;
            display: block;
        }

        .meaning-box {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
        }

        .meaning-box h4 {
            margin-bottom: 10px;
        }

        .highlight-query {
            background: rgba(255, 107, 107, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .highlight-key {
            background: rgba(78, 205, 196, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .highlight-value {
            background: rgba(149, 225, 211, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .token-embedding-viz {
            display: flex;
            align-items: center;
            margin: 15px 0;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
        }

        .token-box {
            background: #667eea;
            color: white;
            padding: 10px 15px;
            border-radius: 6px;
            font-weight: 600;
            min-width: 80px;
            text-align: center;
        }

        .embedding-viz {
            display: flex;
            gap: 2px;
            margin: 0 15px;
        }

        .embedding-bar {
            width: 8px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 2px;
        }

        .projection-arrow {
            margin: 0 10px;
            font-size: 1.5em;
            color: #667eea;
        }

        /* Sequence Visualization Styles */
        .sequence-controls {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
        }

        .seq-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .seq-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        .seq-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .seq-btn-secondary {
            background: #6c757d;
            color: white;
            padding: 10px 25px;
            border: none;
            border-radius: 6px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .seq-btn-secondary:hover {
            background: #5a6268;
        }

        .step-indicator {
            font-size: 18px;
            font-weight: 700;
            color: #667eea;
            min-width: 150px;
            text-align: center;
        }

        .step-progress {
            width: 100%;
            height: 8px;
            background: #e9ecef;
            border-radius: 4px;
            overflow: hidden;
            margin: 10px 0 30px 0;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.4s ease;
            width: 11.11%;
        }

        .sequence-visualization {
            background: white;
            border-radius: 8px;
            padding: 30px;
            min-height: 500px;
            margin: 20px 0;
        }

        .step-content {
            animation: fadeIn 0.5s ease-in;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .step-title {
            font-size: 24px;
            font-weight: 700;
            color: #667eea;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .step-number {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 18px;
            font-weight: 700;
        }

        .step-description {
            font-size: 16px;
            line-height: 1.8;
            color: #495057;
            margin-bottom: 20px;
        }

        .computation-visual {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .data-flow {
            display: flex;
            align-items: center;
            justify-content: space-around;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .data-box {
            background: white;
            border: 3px solid #667eea;
            border-radius: 8px;
            padding: 15px 20px;
            min-width: 120px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            transition: all 0.3s;
        }

        .data-box.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(102, 126, 234, 0.4);
        }

        .data-label {
            font-size: 12px;
            font-weight: 600;
            text-transform: uppercase;
            opacity: 0.7;
            margin-bottom: 5px;
        }

        .data-value {
            font-size: 18px;
            font-weight: 700;
        }

        .operation-symbol {
            font-size: 36px;
            color: #667eea;
            font-weight: 700;
        }

        .matrix-viz-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .matrix-card {
            background: white;
            border-radius: 8px;
            padding: 15px;
            border: 2px solid #dee2e6;
        }

        .matrix-card.highlight {
            border-color: #667eea;
            box-shadow: 0 0 20px rgba(102, 126, 234, 0.3);
        }

        .matrix-label {
            font-weight: 700;
            color: #495057;
            margin-bottom: 10px;
            font-size: 14px;
        }

        .mini-matrix {
            font-family: 'Courier New', monospace;
            font-size: 10px;
            background: #f8f9fa;
            padding: 8px;
            border-radius: 4px;
            overflow-x: auto;
        }

        .step-navigator {
            margin-top: 30px;
        }

        .nav-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 10px;
        }

        .nav-step {
            background: #f8f9fa;
            border: 2px solid #dee2e6;
            padding: 12px;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.3s;
            text-align: center;
        }

        .nav-step:hover {
            border-color: #667eea;
            background: #e7f3ff;
        }

        .nav-step.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: #667eea;
        }

        .nav-step-num {
            font-weight: 700;
            font-size: 12px;
            margin-bottom: 5px;
        }

        .nav-step-name {
            font-size: 11px;
        }

        .attention-computation-viz {
            margin: 20px 0;
        }

        .token-row {
            display: flex;
            gap: 10px;
            margin: 10px 0;
            align-items: center;
        }

        .token-cell {
            flex: 1;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 4px;
            text-align: center;
            font-size: 12px;
            border: 2px solid transparent;
            transition: all 0.3s;
        }

        .token-cell.highlight {
            border-color: #667eea;
            background: #e7f3ff;
            transform: scale(1.05);
        }

        .formula-box {
            background: #fffbf0;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }

        .visual-equation {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            margin: 20px 0;
            font-size: 18px;
            font-weight: 600;
        }

        .eq-component {
            padding: 15px 25px;
            background: white;
            border: 2px solid #667eea;
            border-radius: 8px;
        }

        .eq-operator {
            color: #667eea;
            font-size: 28px;
        }

        .result-box {
            background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
            color: #1a1a1a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            font-weight: 600;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Multi-Head Self-Attention Visualizer</h1>
            <p class="subtitle">Understanding Transformer Attention from First Principles</p>
        </header>

        <div class="controls">
            <div class="control-group">
                <label for="inputText">Input Sequence (space-separated tokens):</label>
                <input type="text" id="inputText" value="The cat sat on the mat" placeholder="Enter your text...">
            </div>

            <div class="control-row">
                <div class="control-group">
                    <label for="embedDim">Embedding Dimension:</label>
                    <input type="number" id="embedDim" value="64" min="8" max="512" step="8">
                </div>

                <div class="control-group">
                    <label for="numHeads">Number of Heads:</label>
                    <input type="number" id="numHeads" value="4" min="1" max="8">
                </div>

                <div class="control-group">
                    <label for="temperature">Temperature (for attention):</label>
                    <input type="number" id="temperature" value="1.0" min="0.1" max="2.0" step="0.1">
                </div>
            </div>

            <div class="control-group" style="margin-top: 15px;">
                <button onclick="runAttention()">Compute Attention</button>
            </div>
        </div>

        <div class="content">
            <div class="section">
                <div class="section-title">Input Tokens</div>
                <div id="tokenDisplay" class="token-display"></div>
                <div class="info-box">
                    <strong>What's happening:</strong> Each token is converted to a dense vector representation (embedding) of dimension d_model.
                </div>
            </div>

            <div class="section">
                <div class="section-title">Architecture Overview</div>
                <div id="architectureInfo"></div>
            </div>

            <div class="tabs">
                <button class="tab active" onclick="switchTab('overview')">Overview</button>
                <button class="tab" onclick="switchTab('sequence')">Step-by-Step Sequence</button>
                <button class="tab" onclick="switchTab('heads')">Individual Heads</button>
                <button class="tab" onclick="switchTab('mathematics')">Mathematics</button>
            </div>

            <div id="overview" class="tab-content active">
                <div class="section">
                    <div class="section-title">Combined Attention Pattern</div>
                    <div class="viz-card">
                        <h3>Average Attention Weights Across All Heads</h3>
                        <canvas id="combinedAttentionCanvas"></canvas>
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> This shows the average attention pattern across all heads. Each cell (i,j) shows how much token i attends to token j.
                    </div>
                </div>
            </div>

            <div id="sequence" class="tab-content">
                <div class="section">
                    <div class="section-title">Step-by-Step Computation Sequence</div>
                    
                    <div class="sequence-controls">
                        <button class="seq-btn" onclick="previousStep()">‚óÄ Previous</button>
                        <span id="stepIndicator" class="step-indicator">Step 1 of 9</span>
                        <button class="seq-btn" onclick="nextStep()">Next ‚ñ∂</button>
                        <button class="seq-btn-secondary" onclick="resetSequence()">Reset</button>
                    </div>

                    <div class="step-progress">
                        <div id="progressBar" class="progress-bar"></div>
                    </div>

                    <div id="sequenceVisualization" class="sequence-visualization">
                        <!-- Dynamic content will be inserted here -->
                    </div>

                    <div class="step-navigator">
                        <div class="nav-grid" id="stepNavGrid">
                            <!-- Step navigation buttons will be inserted here -->
                        </div>
                    </div>
                </div>
            </div>

            <div id="heads" class="tab-content">
                <div class="section">
                    <div class="section-title">Individual Head Attention Patterns</div>
                    <div id="headsContainer"></div>
                    <div class="info-box">
                        <strong>What's happening:</strong> Each head learns to attend to different aspects of the input. Some might focus on local patterns, others on long-range dependencies.
                    </div>
                </div>
            </div>

            <div id="mathematics" class="tab-content">
                <div class="section">
                    <div class="section-title">Step-by-Step Mathematical Breakdown</div>
                    <div id="mathBreakdown"></div>
                </div>
            </div>

            <div class="section">
                <div class="section-title">Deep Dive: Q, K, V Transformation</div>
                <div class="info-box">
                    <strong>Conceptual Understanding:</strong><br>
                    ‚Ä¢ <strong>Query (Q):</strong> "What am I looking for?" - Represents what each token wants to know<br>
                    ‚Ä¢ <strong>Key (K):</strong> "What do I offer?" - Represents what information each token provides<br>
                    ‚Ä¢ <strong>Value (V):</strong> "Here's my actual content" - The actual information to be aggregated<br><br>
                    The attention mechanism is like a database lookup: Q asks questions, K provides indices, and V contains the actual data.
                </div>
                <div id="qkvVisualization"></div>
            </div>

            <div class="section">
                <div class="section-title">Attention Mechanism Flow</div>
                <div id="attentionFlow"></div>
            </div>
        </div>
    </div>

    <script>
        let currentAttention = null;
        let currentStep = 0;
        let totalSteps = 9;
        let sequenceData = null;

        function switchTab(tabName) {
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));

            event.target.classList.add('active');
            document.getElementById(tabName).classList.add('active');
        }

        function nextStep() {
            if (currentStep < totalSteps - 1) {
                currentStep++;
                renderStep();
            }
        }

        function previousStep() {
            if (currentStep > 0) {
                currentStep--;
                renderStep();
            }
        }

        function goToStep(step) {
            currentStep = step;
            renderStep();
        }

        function resetSequence() {
            currentStep = 0;
            renderStep();
        }

        function renderStep() {
            if (!sequenceData) return;

            const progress = ((currentStep + 1) / totalSteps) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
            document.getElementById('stepIndicator').textContent = `Step ${currentStep + 1} of ${totalSteps}`;

            // Update navigator
            document.querySelectorAll('.nav-step').forEach((el, idx) => {
                if (idx === currentStep) {
                    el.classList.add('active');
                } else {
                    el.classList.remove('active');
                }
            });

            // Render current step content
            const container = document.getElementById('sequenceVisualization');
            container.innerHTML = generateStepContent(currentStep, sequenceData);

            // Render step-specific canvases after DOM update
            setTimeout(() => {
                if (currentStep === 5) {
                    const canvas = document.getElementById('stepSoftmaxCanvas');
                    if (canvas) {
                        const head0 = sequenceData.result.headOutputs[0];
                        drawStepSoftmaxCanvas(canvas, sequenceData.tokens, sequenceData.focusToken, head0);
                    }
                }
            }, 10);
        }

        class MultiHeadSelfAttention {
            constructor(embedDim, numHeads, temperature = 1.0) {
                this.embedDim = embedDim;
                this.numHeads = numHeads;
                this.headDim = Math.floor(embedDim / numHeads);
                this.temperature = temperature;

                // Initialize weight matrices for each head
                this.initializeWeights();
            }

            initializeWeights() {
                this.heads = [];
                for (let h = 0; h < this.numHeads; h++) {
                    this.heads.push({
                        Wq: this.randomMatrix(this.embedDim, this.headDim),
                        Wk: this.randomMatrix(this.embedDim, this.headDim),
                        Wv: this.randomMatrix(this.embedDim, this.headDim)
                    });
                }
                // Output projection
                this.Wo = this.randomMatrix(this.numHeads * this.headDim, this.embedDim);
            }

            randomMatrix(rows, cols) {
                const matrix = [];
                const scale = Math.sqrt(2.0 / (rows + cols)); // Xavier initialization
                for (let i = 0; i < rows; i++) {
                    matrix[i] = [];
                    for (let j = 0; j < cols; j++) {
                        matrix[i][j] = (Math.random() - 0.5) * 2 * scale;
                    }
                }
                return matrix;
            }

            matmul(A, B) {
                const rowsA = A.length;
                const colsA = A[0].length;
                const colsB = B[0].length;
                const C = [];

                for (let i = 0; i < rowsA; i++) {
                    C[i] = [];
                    for (let j = 0; j < colsB; j++) {
                        let sum = 0;
                        for (let k = 0; k < colsA; k++) {
                            sum += A[i][k] * B[k][j];
                        }
                        C[i][j] = sum;
                    }
                }
                return C;
            }

            transpose(A) {
                const rows = A.length;
                const cols = A[0].length;
                const T = [];
                for (let j = 0; j < cols; j++) {
                    T[j] = [];
                    for (let i = 0; i < rows; i++) {
                        T[j][i] = A[i][j];
                    }
                }
                return T;
            }

            softmax(scores) {
                const result = [];
                for (let i = 0; i < scores.length; i++) {
                    const row = scores[i];
                    const maxScore = Math.max(...row);
                    const expScores = row.map(s => Math.exp((s - maxScore) / this.temperature));
                    const sumExp = expScores.reduce((a, b) => a + b, 0);
                    result[i] = expScores.map(e => e / sumExp);
                }
                return result;
            }

            scaledDotProductAttention(Q, K, V) {
                // Compute attention scores: Q @ K^T
                const KT = this.transpose(K);
                const scores = this.matmul(Q, KT);

                // Scale by sqrt(d_k)
                const scale = Math.sqrt(this.headDim);
                const scaledScores = scores.map(row =>
                    row.map(val => val / scale)
                );

                // Apply softmax
                const attentionWeights = this.softmax(scaledScores);

                // Apply attention to values: attention @ V
                const output = this.matmul(attentionWeights, V);

                return {
                    output,
                    attentionWeights,
                    scores: scaledScores
                };
            }

            forward(embeddings) {
                const seqLen = embeddings.length;
                const headOutputs = [];
                const headAttentions = [];

                // Process each head
                for (let h = 0; h < this.numHeads; h++) {
                    const head = this.heads[h];

                    // Project to Q, K, V
                    const Q = this.matmul(embeddings, head.Wq);
                    const K = this.matmul(embeddings, head.Wk);
                    const V = this.matmul(embeddings, head.Wv);

                    // Compute attention
                    const { output, attentionWeights, scores } =
                        this.scaledDotProductAttention(Q, K, V);

                    headOutputs.push({
                        Q, K, V, output, attentionWeights, scores
                    });
                    headAttentions.push(attentionWeights);
                }

                // Concatenate heads
                const concatenated = [];
                for (let i = 0; i < seqLen; i++) {
                    concatenated[i] = [];
                    for (let h = 0; h < this.numHeads; h++) {
                        concatenated[i].push(...headOutputs[h].output[i]);
                    }
                }

                // Final linear projection
                const finalOutput = this.matmul(concatenated, this.Wo);

                return {
                    output: finalOutput,
                    headOutputs,
                    headAttentions
                };
            }
        }

        function generateEmbeddings(tokens, embedDim) {
            const embeddings = [];
            for (let i = 0; i < tokens.length; i++) {
                const embedding = [];
                // Generate deterministic embeddings based on token hash
                const hash = tokens[i].split('').reduce((acc, char) =>
                    acc + char.charCodeAt(0), 0);
                for (let j = 0; j < embedDim; j++) {
                    const angle = (hash * 0.1 + j * 0.5) % (2 * Math.PI);
                    embedding[j] = Math.sin(angle) * 0.5;
                }
                embeddings.push(embedding);
            }
            return embeddings;
        }

        function drawAttentionHeatmap(canvas, attentionWeights, tokens) {
            const ctx = canvas.getContext('2d');
            const size = attentionWeights.length;
            const cellSize = Math.min(400 / size, 50);
            const margin = 80;

            canvas.width = size * cellSize + margin * 2;
            canvas.height = size * cellSize + margin * 2;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw heatmap
            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size; j++) {
                    const weight = attentionWeights[i][j];
                    const intensity = Math.floor(weight * 255);
                    ctx.fillStyle = `rgb(${255-intensity}, ${255-intensity/2}, 255)`;
                    ctx.fillRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw border
                    ctx.strokeStyle = '#dee2e6';
                    ctx.strokeRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw value
                    ctx.fillStyle = weight > 0.5 ? 'white' : 'black';
                    ctx.font = `${Math.min(cellSize/3, 12)}px Arial`;
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    ctx.fillText(
                        weight.toFixed(2),
                        margin + j * cellSize + cellSize/2,
                        margin + i * cellSize + cellSize/2
                    );
                }
            }

            // Draw labels
            ctx.fillStyle = '#495057';
            ctx.font = '14px Arial';
            ctx.textAlign = 'right';
            for (let i = 0; i < size; i++) {
                ctx.fillText(tokens[i], margin - 10, margin + i * cellSize + cellSize/2);
            }

            ctx.textAlign = 'center';
            for (let j = 0; j < size; j++) {
                ctx.save();
                ctx.translate(margin + j * cellSize + cellSize/2, margin - 10);
                ctx.rotate(-Math.PI / 4);
                ctx.fillText(tokens[j], 0, 0);
                ctx.restore();
            }

            // Draw axis labels
            ctx.font = 'bold 16px Arial';
            ctx.fillStyle = '#667eea';
            ctx.textAlign = 'center';
            ctx.fillText('Keys (attending to)', canvas.width / 2, 30);
            ctx.save();
            ctx.translate(30, canvas.height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Queries (attending from)', 0, 0);
            ctx.restore();
        }

        function formatMatrix(matrix, label) {
            let html = `<h4>${label}</h4><div class="matrix-display">`;
            html += matrix.map((row, i) =>
                `<div class="matrix-row">[${row.map(v =>
                    `<span class="matrix-value">${v.toFixed(4)}</span>`
                ).join(' ')}]</div>`
            ).join('');
            html += '</div>';
            return html;
        }

        function runAttention() {
            const inputText = document.getElementById('inputText').value;
            const embedDim = parseInt(document.getElementById('embedDim').value);
            const numHeads = parseInt(document.getElementById('numHeads').value);
            const temperature = parseFloat(document.getElementById('temperature').value);

            const tokens = inputText.trim().split(/\s+/);

            // Display tokens
            document.getElementById('tokenDisplay').innerHTML = tokens.map(
                (t, i) => `<div class="token">${i+1}. ${t}</div>`
            ).join('');

            // Generate embeddings
            const embeddings = generateEmbeddings(tokens, embedDim);

            // Create and run attention
            const mhsa = new MultiHeadSelfAttention(embedDim, numHeads, temperature);
            const result = mhsa.forward(embeddings);
            currentAttention = result;

            // Display architecture info
            document.getElementById('architectureInfo').innerHTML = `
                <div class="info-box">
                    <strong>Configuration:</strong><br>
                    ‚Ä¢ Sequence Length: ${tokens.length} tokens<br>
                    ‚Ä¢ Embedding Dimension (d_model): ${embedDim}<br>
                    ‚Ä¢ Number of Heads: ${numHeads}<br>
                    ‚Ä¢ Head Dimension (d_k = d_model/num_heads): ${mhsa.headDim}<br>
                    ‚Ä¢ Temperature: ${temperature}
                </div>
            `;

            // Draw combined attention
            const avgAttention = averageAttentions(result.headAttentions);
            const combinedCanvas = document.getElementById('combinedAttentionCanvas');
            drawAttentionHeatmap(combinedCanvas, avgAttention, tokens);

            // Draw individual heads
            const headsContainer = document.getElementById('headsContainer');
            headsContainer.innerHTML = '';
            for (let h = 0; h < numHeads; h++) {
                const headDiv = document.createElement('div');
                headDiv.className = 'head-container';
                headDiv.innerHTML = `
                    <div class="head-title">Head ${h + 1}</div>
                    <canvas id="headCanvas${h}"></canvas>
                `;
                headsContainer.appendChild(headDiv);

                setTimeout(() => {
                    const canvas = document.getElementById(`headCanvas${h}`);
                    drawAttentionHeatmap(canvas, result.headAttentions[h], tokens);
                }, 10);
            }

            // Generate mathematical breakdown
            generateMathBreakdown(tokens, embeddings, mhsa, result);

            // Generate Q/K/V visualizations
            generateQKVVisualization(tokens, embeddings, mhsa, result);

            // Generate attention flow visualization
            generateAttentionFlowVisualization(tokens, embeddings, mhsa, result);

            // Prepare sequence data
            prepareSequenceData(tokens, embeddings, mhsa, result);

            // Initialize step navigator
            initializeStepNavigator();

            // Render initial step
            renderStep();
        }

        function averageAttentions(attentions) {
            const numHeads = attentions.length;
            const seqLen = attentions[0].length;
            const avg = [];

            for (let i = 0; i < seqLen; i++) {
                avg[i] = [];
                for (let j = 0; j < seqLen; j++) {
                    let sum = 0;
                    for (let h = 0; h < numHeads; h++) {
                        sum += attentions[h][i][j];
                    }
                    avg[i][j] = sum / numHeads;
                }
            }
            return avg;
        }

        function generateMathBreakdown(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('mathBreakdown');

            let html = `
                <div class="math-step">
                    <h4>Step 1: Input Embeddings</h4>
                    <p>Each token is represented as a ${mhsa.embedDim}-dimensional vector.</p>
                    <div class="formula">X ‚àà ‚Ñù^(${tokens.length} √ó ${mhsa.embedDim})</div>
                    ${formatMatrix(embeddings.map(e => e.slice(0, 8)), 'Embeddings (first 8 dimensions)')}
                </div>

                <div class="math-step">
                    <h4>Step 2: Linear Projections</h4>
                    <p>For each head h, we project the input into Query, Key, and Value spaces:</p>
                    <div class="formula">
                        Q_h = X ¬∑ W^Q_h<br>
                        K_h = X ¬∑ W^K_h<br>
                        V_h = X ¬∑ W^V_h<br>
                        where W^Q_h, W^K_h, W^V_h ‚àà ‚Ñù^(${mhsa.embedDim} √ó ${mhsa.headDim})
                    </div>
                    <div class="info-box">
                        Each head has its own projection matrices, allowing it to learn different representations.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 3: Scaled Dot-Product Attention</h4>
                    <p>For each head, we compute attention scores:</p>
                    <div class="formula">
                        scores = (Q ¬∑ K^T) / ‚àöd_k<br>
                        where d_k = ${mhsa.headDim}
                    </div>
                    <p>The scaling factor prevents the dot products from becoming too large.</p>
                </div>

                <div class="math-step">
                    <h4>Step 4: Softmax Normalization</h4>
                    <p>Apply softmax to get attention weights that sum to 1:</p>
                    <div class="formula">
                        Attention(Q, K, V) = softmax(scores / temperature) ¬∑ V<br>
                        temperature = ${mhsa.temperature}
                    </div>
                    <p>Higher temperature makes the distribution more uniform; lower makes it more peaked.</p>
                </div>
            `;

            // Show first head's computations in detail
            const head0 = result.headOutputs[0];
            html += `
                <div class="math-step">
                    <h4>Example: Head 1 Detailed Computation</h4>
                    ${formatMatrix(head0.Q.map(q => q.slice(0, 4)), 'Q (first 4 dims)')}
                    ${formatMatrix(head0.K.map(k => k.slice(0, 4)), 'K (first 4 dims)')}
                    ${formatMatrix(head0.scores, 'Attention Scores (scaled)')}
                    ${formatMatrix(head0.attentionWeights, 'Attention Weights (after softmax)')}
                </div>

                <div class="math-step">
                    <h4>Step 5: Concatenate Heads</h4>
                    <p>Concatenate outputs from all ${mhsa.numHeads} heads:</p>
                    <div class="formula">
                        MultiHead = Concat(head_1, head_2, ..., head_${mhsa.numHeads})<br>
                        where each head_i ‚àà ‚Ñù^(${tokens.length} √ó ${mhsa.headDim})
                    </div>
                    <p>Result: ‚Ñù^(${tokens.length} √ó ${mhsa.numHeads * mhsa.headDim})</p>
                </div>

                <div class="math-step">
                    <h4>Step 6: Output Projection</h4>
                    <p>Final linear transformation:</p>
                    <div class="formula">
                        Output = MultiHead ¬∑ W^O<br>
                        where W^O ‚àà ‚Ñù^(${mhsa.numHeads * mhsa.headDim} √ó ${mhsa.embedDim})
                    </div>
                    ${formatMatrix(result.output.map(o => o.slice(0, 8)), 'Final Output (first 8 dimensions)')}
                </div>

                <div class="info-box">
                    <strong>Key Insights:</strong><br>
                    ‚Ä¢ Each attention head can learn to focus on different relationships<br>
                    ‚Ä¢ The attention weights are position-dependent and learned from data<br>
                    ‚Ä¢ Self-attention allows every token to attend to every other token<br>
                    ‚Ä¢ Scaling by ‚àöd_k stabilizes gradients during training<br>
                    ‚Ä¢ The final output combines information from all attention heads
                </div>
            `;

            container.innerHTML = html;
        }

        function generateQKVVisualization(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('qkvVisualization');
            const head0 = result.headOutputs[0];

            let html = `
                <div class="math-step">
                    <h4>Transformation Pipeline for First Token: "${tokens[0]}"</h4>
                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box token-box">Token<br>"${tokens[0]}"</div>
                            <small>Input word</small>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                            <small>Dense vector</small>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #FF6B6B, #EE5A6F);">
                                Query<br>[${mhsa.headDim}d]
                            </div>
                            <small>What to look for</small>
                        </div>
                    </div>

                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #4ECDC4, #44A08D);">
                                Key<br>[${mhsa.headDim}d]
                            </div>
                            <small>What I offer</small>
                        </div>
                    </div>

                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #95E1D3, #74C7BB);">
                                Value<br>[${mhsa.headDim}d]
                            </div>
                            <small>Actual content</small>
                        </div>
                    </div>
                </div>

                <div class="meaning-box">
                    <h4>üéØ How Meaning Emerges</h4>
                    <p><strong>Step 1: Projection Matrices Learn Semantic Roles</strong></p>
                    <p>The weight matrices W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> are learned during training. They transform the generic embedding into specialized representations:</p>
                    <ul style="margin: 10px 0; padding-left: 20px;">
                        <li><span class="highlight-query">Query (Q)</span> encodes what information this token needs (e.g., "cat" might query for subjects or animals)</li>
                        <li><span class="highlight-key">Key (K)</span> encodes what information this token can provide (e.g., "sat" might provide action information)</li>
                        <li><span class="highlight-value">Value (V)</span> contains the semantic content to be retrieved</li>
                    </ul>
                </div>

                <div class="qkv-grid">
                    <div class="qkv-card qkv-query">
                        <h3>üîç Query Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">Q = Embedding √ó W<sup>Q</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.Q[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row shows what that token is "asking for"</small>
                    </div>

                    <div class="qkv-card qkv-key">
                        <h3>üîë Key Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">K = Embedding √ó W<sup>K</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.K[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row shows what that token "offers"</small>
                    </div>

                    <div class="qkv-card qkv-value">
                        <h3>üíé Value Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">V = Embedding √ó W<sup>V</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.V[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row contains the actual information</small>
                    </div>
                </div>

                <div class="math-step">
                    <h4>üìä Projection Weight Matrices (Head 1)</h4>
                    <p>These matrices transform ${mhsa.embedDim}-dimensional embeddings into ${mhsa.headDim}-dimensional Q/K/V vectors:</p>
                    <canvas id="weightMatrixCanvas" class="interaction-canvas"></canvas>
                </div>
            `;

            container.innerHTML = html;

            // Draw weight matrix visualization
            setTimeout(() => {
                drawWeightMatrixVisualization(mhsa);
            }, 10);
        }

        function drawWeightMatrixVisualization(mhsa) {
            const canvas = document.getElementById('weightMatrixCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const head = mhsa.heads[0];

            canvas.width = 900;
            canvas.height = 250;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            const matrices = [
                { matrix: head.Wq, name: 'W^Q', color: '#FF6B6B', x: 50 },
                { matrix: head.Wk, name: 'W^K', color: '#4ECDC4', x: 350 },
                { matrix: head.Wv, name: 'W^V', color: '#95E1D3', x: 650 }
            ];

            matrices.forEach(({ matrix, name, color, x }) => {
                const rows = Math.min(matrix.length, 16);
                const cols = Math.min(matrix[0].length, 8);
                const cellSize = 8;

                // Draw matrix
                for (let i = 0; i < rows; i++) {
                    for (let j = 0; j < cols; j++) {
                        const val = matrix[i][j];
                        const normalized = (val + 0.5); // Normalize to [0, 1]
                        const intensity = Math.floor(normalized * 255);
                        ctx.fillStyle = `rgba(${color.slice(1, 3)}, ${color.slice(3, 5)}, ${color.slice(5, 7)}, ${normalized})`;
                        ctx.fillRect(x + j * cellSize, 80 + i * cellSize, cellSize - 1, cellSize - 1);
                    }
                }

                // Draw labels
                ctx.fillStyle = '#495057';
                ctx.font = 'bold 16px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(name, x + cols * cellSize / 2, 60);

                ctx.font = '12px Arial';
                ctx.fillText(`${matrix.length} √ó ${matrix[0].length}`, x + cols * cellSize / 2, 220);

                // Draw border
                ctx.strokeStyle = color;
                ctx.lineWidth = 2;
                ctx.strokeRect(x, 80, cols * cellSize, rows * cellSize);
            });

            // Add description
            ctx.fillStyle = '#495057';
            ctx.font = '13px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Each matrix learns to extract different semantic aspects during training', canvas.width / 2, 245);
        }

        function generateAttentionFlowVisualization(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('attentionFlow');
            const head0 = result.headOutputs[0];

            // Pick an example token to focus on
            const focusIdx = Math.floor(tokens.length / 2);
            const focusToken = tokens[focusIdx];

            let html = `
                <div class="meaning-box">
                    <h4>üß† How Attention Extracts Meaning: Example with "${focusToken}"</h4>
                    <p>Let's trace how token "${focusToken}" (position ${focusIdx + 1}) gathers information from other tokens:</p>
                </div>

                <div class="math-step">
                    <h4>Step 1: Compute Attention Scores (Q ¬∑ K<sup>T</sup>)</h4>
                    <p>The <span class="highlight-query">query</span> of "${focusToken}" is compared with the <span class="highlight-key">keys</span> of all tokens via dot product:</p>
                    <div class="flow-diagram" style="flex-wrap: wrap;">
            `;

            tokens.forEach((token, i) => {
                const score = head0.scores[focusIdx][i];
                const weight = head0.attentionWeights[focusIdx][i];
                html += `
                    <div class="flow-step" style="flex: 1; min-width: 120px;">
                        <div class="flow-box" style="background: rgba(102, 126, 234, ${weight});">
                            ${token}
                        </div>
                        <small>Score: ${score.toFixed(3)}<br>Weight: ${weight.toFixed(3)}</small>
                    </div>
                `;
            });

            html += `
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> High dot product (Q¬∑K) means the query and key are aligned in the learned semantic space. 
                        For example, if "cat" learns to query for subjects, and "sat" has a key indicating it's a verb, their dot product will be high.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 2: Softmax Normalization</h4>
                    <p>Raw scores are converted to probabilities that sum to 1:</p>
                    <canvas id="softmaxCanvas" class="interaction-canvas"></canvas>
                </div>

                <div class="math-step">
                    <h4>Step 3: Weighted Aggregation of Values</h4>
                    <p>The attention weights determine how much of each token's <span class="highlight-value">value</span> vector to include:</p>
                    <div class="formula">
                        output[${focusToken}] = ${tokens.map((t, i) => 
                            `${head0.attentionWeights[focusIdx][i].toFixed(2)} √ó V[${t}]`
                        ).join(' + ')}
                    </div>
                    <p style="margin-top: 10px;">This weighted sum creates a context-aware representation of "${focusToken}" based on the entire sequence.</p>
                </div>

                <div class="math-step">
                    <h4>Visualizing the Aggregation</h4>
                    <canvas id="aggregationCanvas" class="interaction-canvas"></canvas>
                </div>

                <div class="meaning-box">
                    <h4>üéì Key Insights on Semantic Understanding</h4>
                    <ol style="margin: 10px 0; padding-left: 20px; line-height: 1.8;">
                        <li><strong>Learned Representations:</strong> During training, the projection matrices learn to encode linguistic patterns. W<sup>Q</sup> might learn to encode "what grammatical role does this word need to find?", W<sup>K</sup> encodes "what role do I play?", and W<sup>V</sup> holds the actual semantic content.</li>
                        
                        <li><strong>Dynamic Contextualization:</strong> Unlike static word embeddings, attention creates context-dependent representations. The word "bank" gets different representations in "river bank" vs "bank account" because it attends to different neighboring words.</li>
                        
                        <li><strong>Multiple Heads = Multiple Perspectives:</strong> With ${mhsa.numHeads} heads, the model can simultaneously attend to different aspects: one head might capture syntax, another semantics, another long-range dependencies.</li>
                        
                        <li><strong>Information Routing:</strong> Attention acts as a soft routing mechanism, allowing information to flow from relevant tokens to where it's needed. High attention = "this token is relevant to understanding me".</li>
                        
                        <li><strong>Permutation Invariance:</strong> Self-attention doesn't care about the order of tokens inherently (that's why positional encodings are added separately). It learns relationships based purely on content.</li>
                    </ol>
                </div>

                <div class="math-step">
                    <h4>üîÑ Comparison: Before and After Attention</h4>
                    <canvas id="comparisonCanvas" class="interaction-canvas"></canvas>
                    <p style="margin-top: 10px; text-align: center;">
                        <strong>Before:</strong> Static embedding based only on the token itself<br>
                        <strong>After:</strong> Contextualized representation incorporating information from the entire sequence
                    </p>
                </div>
            `;

            container.innerHTML = html;

            // Draw visualizations
            setTimeout(() => {
                drawSoftmaxVisualization(tokens, focusIdx, head0);
                drawAggregationVisualization(tokens, focusIdx, head0);
                drawComparisonVisualization(embeddings, result.output, focusIdx, tokens[focusIdx]);
            }, 10);
        }

        function drawSoftmaxVisualization(tokens, focusIdx, head0) {
            const canvas = document.getElementById('softmaxCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 300;

            const scores = head0.scores[focusIdx];
            const weights = head0.attentionWeights[focusIdx];

            const barWidth = 80;
            const gap = 20;
            const startX = (canvas.width - tokens.length * (barWidth + gap)) / 2;
            const baseY = 250;

            // Draw bars for before and after
            tokens.forEach((token, i) => {
                const x = startX + i * (barWidth + gap);
                
                // Raw score (scaled for visualization)
                const scoreHeight = Math.min(Math.abs(scores[i]) * 30, 60);
                ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
                ctx.fillRect(x, baseY - scoreHeight - 80, barWidth / 2 - 5, scoreHeight);

                // Softmax weight
                const weightHeight = weights[i] * 120;
                ctx.fillStyle = 'rgba(102, 126, 234, 0.8)';
                ctx.fillRect(x + barWidth / 2, baseY - weightHeight - 80, barWidth / 2 - 5, weightHeight);

                // Labels
                ctx.fillStyle = '#495057';
                ctx.font = '12px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(token, x + barWidth / 2, baseY - 60);
                ctx.font = '10px Arial';
                ctx.fillText(`${scores[i].toFixed(2)}`, x + barWidth / 4, baseY - scoreHeight - 85);
                ctx.fillText(`${weights[i].toFixed(2)}`, x + 3 * barWidth / 4, baseY - weightHeight - 85);
            });

            // Legend
            ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
            ctx.fillRect(20, 20, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Raw Scores (Q¬∑K)', 40, 32);

            ctx.fillStyle = 'rgba(102, 126, 234, 0.8)';
            ctx.fillRect(20, 40, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.fillText('After Softmax (Attention Weights)', 40, 52);
        }

        function drawAggregationVisualization(tokens, focusIdx, head0) {
            const canvas = document.getElementById('aggregationCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;

            const weights = head0.attentionWeights[focusIdx];
            const values = head0.V;

            // Draw value vectors as bars
            const numDims = 8; // Show first 8 dimensions
            const barWidth = 60;
            const gap = 10;
            const dimHeight = 30;

            let startY = 50;

            tokens.forEach((token, tokenIdx) => {
                const weight = weights[tokenIdx];
                const x = 50;

                // Token label
                ctx.fillStyle = '#495057';
                ctx.font = '12px Arial';
                ctx.textAlign = 'right';
                ctx.fillText(`${token} (${weight.toFixed(2)}√ó)`, x - 10, startY + 15);

                // Draw value vector bars
                for (let dim = 0; dim < numDims; dim++) {
                    const val = values[tokenIdx][dim];
                    const weightedVal = val * weight;
                    const barLen = Math.abs(val) * 40;
                    const weightedBarLen = Math.abs(weightedVal) * 40;

                    // Original value (faint)
                    ctx.fillStyle = 'rgba(149, 225, 211, 0.3)';
                    ctx.fillRect(x + dim * (barWidth / numDims), startY, barWidth / numDims - 2, 10);

                    // Weighted value
                    const hue = weightedVal > 0 ? 150 : 0;
                    ctx.fillStyle = `hsla(${hue}, 70%, 50%, ${Math.abs(weight)})`;
                    ctx.fillRect(x + dim * (barWidth / numDims), startY, (barWidth / numDims - 2) * Math.min(Math.abs(weightedVal) * 5, 1), 10);
                }

                startY += 20;
            });

            // Title
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Each row: Value vector √ó Attention weight', 50, 30);

            ctx.font = '11px Arial';
            ctx.fillText('Higher attention weight = more contribution to final output', 50, startY + 20);
        }

        function drawComparisonVisualization(embeddings, outputs, focusIdx, token) {
            const canvas = document.getElementById('comparisonCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 300;

            const numDims = Math.min(embeddings[focusIdx].length, 32);
            const barWidth = 500 / numDims;
            const startX = 50;

            // Draw input embedding
            let y = 80;
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 13px Arial';
            ctx.textAlign = 'left';
            ctx.fillText(`Input Embedding of "${token}"`, startX, y - 10);

            for (let i = 0; i < numDims; i++) {
                const val = embeddings[focusIdx][i];
                const height = Math.abs(val) * 50;
                ctx.fillStyle = val > 0 ? 'rgba(102, 126, 234, 0.6)' : 'rgba(234, 102, 126, 0.6)';
                ctx.fillRect(startX + i * barWidth, y + 50 - height, barWidth - 1, height);
            }

            // Draw output
            y = 180;
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 13px Arial';
            ctx.fillText(`Contextualized Output of "${token}"`, startX, y - 10);

            for (let i = 0; i < numDims; i++) {
                const val = outputs[focusIdx][i];
                const height = Math.abs(val) * 50;
                ctx.fillStyle = val > 0 ? 'rgba(118, 75, 162, 0.8)' : 'rgba(162, 75, 118, 0.8)';
                ctx.fillRect(startX + i * barWidth, y + 50 - height, barWidth - 1, height);
            }

            // Arrow
            ctx.strokeStyle = '#667eea';
            ctx.lineWidth = 3;
            ctx.beginPath();
            ctx.moveTo(20, 110);
            ctx.lineTo(20, 190);
            ctx.stroke();

            // Arrowhead
            ctx.beginPath();
            ctx.moveTo(15, 180);
            ctx.lineTo(20, 190);
            ctx.lineTo(25, 180);
            ctx.fillStyle = '#667eea';
            ctx.fill();

            ctx.fillStyle = '#667eea';
            ctx.font = 'bold 12px Arial';
            ctx.save();
            ctx.translate(10, 150);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('ATTENTION', 0, 0);
            ctx.restore();
        }

        function drawStepSoftmaxCanvas(canvas, tokens, focusIdx, head0) {
            const ctx = canvas.getContext('2d');
            const scores = head0.scores[focusIdx];
            const weights = head0.attentionWeights[focusIdx];

            const barWidth = Math.min(80, 500 / tokens.length);
            const gap = 10;
            const startX = (canvas.width - tokens.length * (barWidth + gap)) / 2;
            const baseY = 250;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw bars
            tokens.forEach((token, i) => {
                const x = startX + i * (barWidth + gap);
                
                // Scaled score (before softmax)
                const scoreHeight = Math.min(Math.abs(scores[i]) * 40, 80);
                ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
                ctx.fillRect(x, baseY - scoreHeight - 80, barWidth / 2 - 2, scoreHeight);

                // Softmax weight
                const weightHeight = weights[i] * 120;
                ctx.fillStyle = 'rgba(102, 126, 234, 0.9)';
                ctx.fillRect(x + barWidth / 2, baseY - weightHeight - 80, barWidth / 2 - 2, weightHeight);

                // Labels
                ctx.fillStyle = '#495057';
                ctx.font = '11px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(token, x + barWidth / 2, baseY - 60);
                
                ctx.font = '9px Arial';
                ctx.fillText(scores[i].toFixed(2), x + barWidth / 4, baseY - scoreHeight - 85);
                ctx.fillText((weights[i] * 100).toFixed(0) + '%', x + 3 * barWidth / 4, baseY - weightHeight - 85);
            });

            // Legend
            ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
            ctx.fillRect(20, 20, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Scaled Scores', 40, 32);

            ctx.fillStyle = 'rgba(102, 126, 234, 0.9)';
            ctx.fillRect(20, 40, 15, 15);
            ctx.fillText('Attention Weights (%)', 40, 52);
        }

        function prepareSequenceData(tokens, embeddings, mhsa, result) {
            sequenceData = {
                tokens,
                embeddings,
                mhsa,
                result,
                focusToken: Math.floor(tokens.length / 2)
            };
        }

        function initializeStepNavigator() {
            const steps = [
                'Input Tokens',
                'Token Embeddings',
                'Linear Projections',
                'Query-Key Dot Product',
                'Scaling',
                'Softmax',
                'Attention √ó Values',
                'Concatenate Heads',
                'Final Output'
            ];

            const navGrid = document.getElementById('stepNavGrid');
            navGrid.innerHTML = steps.map((name, idx) => `
                <div class="nav-step ${idx === 0 ? 'active' : ''}" onclick="goToStep(${idx})">
                    <div class="nav-step-num">Step ${idx + 1}</div>
                    <div class="nav-step-name">${name}</div>
                </div>
            `).join('');
        }

        function generateStepContent(step, data) {
            const { tokens, embeddings, mhsa, result, focusToken } = data;
            const head0 = result.headOutputs[0];

            switch(step) {
                case 0: return generateStep0_InputTokens(tokens);
                case 1: return generateStep1_Embeddings(tokens, embeddings, mhsa);
                case 2: return generateStep2_Projections(tokens, embeddings, mhsa, head0);
                case 3: return generateStep3_DotProduct(tokens, head0, focusToken);
                case 4: return generateStep4_Scaling(tokens, head0, mhsa, focusToken);
                case 5: return generateStep5_Softmax(tokens, head0, focusToken);
                case 6: return generateStep6_AttentionValues(tokens, head0, focusToken);
                case 7: return generateStep7_Concatenate(tokens, result, mhsa);
                case 8: return generateStep8_FinalOutput(tokens, embeddings, result);
                default: return '';
            }
        }

        function generateStep0_InputTokens(tokens) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">1</span>
                        Input Tokens
                    </div>
                    <div class="step-description">
                        We start with a sequence of tokens (words or subwords) from our input text.
                        Each token will be processed independently but will interact through attention.
                    </div>

                    <div class="computation-visual">
                        <h4 style="margin-bottom: 15px;">Input Sequence</h4>
                        <div class="data-flow">
                            ${tokens.map((token, idx) => `
                                <div class="data-box active">
                                    <div class="data-label">Token ${idx + 1}</div>
                                    <div class="data-value">"${token}"</div>
                                </div>
                            `).join('')}
                        </div>
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> We have ${tokens.length} tokens in our sequence. 
                        Each token starts as a discrete symbol that needs to be converted to a continuous vector representation.
                    </div>

                    <div class="formula-box">
                        Sequence Length: n = ${tokens.length}<br>
                        Tokens: [${tokens.map(t => `"${t}"`).join(', ')}]
                    </div>
                </div>
            `;
        }

        function generateStep1_Embeddings(tokens, embeddings, mhsa) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">2</span>
                        Token Embeddings
                    </div>
                    <div class="step-description">
                        Each token is converted into a dense vector of dimension d<sub>model</sub> = ${mhsa.embedDim}.
                        This embedding captures semantic information about the token.
                    </div>

                    <div class="computation-visual">
                        <h4 style="margin-bottom: 15px;">Embedding Transformation</h4>
                        ${tokens.map((token, idx) => `
                            <div class="token-embedding-viz">
                                <div class="token-box">${token}</div>
                                <div class="projection-arrow">‚Üí</div>
                                <div class="embedding-viz">
                                    ${embeddings[idx].slice(0, 20).map(val => 
                                        `<div class="embedding-bar" style="height: ${Math.abs(val) * 80 + 20}px;"></div>`
                                    ).join('')}
                                </div>
                                <small style="margin-left: 10px;">[${mhsa.embedDim}-dimensional vector]</small>
                            </div>
                        `).join('')}
                    </div>

                    <div class="visual-equation">
                        <div class="eq-component">Token</div>
                        <div class="eq-operator">‚Üí</div>
                        <div class="eq-component">Embedding Matrix</div>
                        <div class="eq-operator">‚Üí</div>
                        <div class="eq-component">Vector ‚àà ‚Ñù<sup>${mhsa.embedDim}</sup></div>
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> Embeddings are learned representations where semantically similar words 
                        have similar vectors. This ${mhsa.embedDim}-dimensional space captures linguistic properties.
                    </div>

                    <div class="matrix-viz-container">
                        ${tokens.slice(0, 3).map((token, idx) => `
                            <div class="matrix-card highlight">
                                <div class="matrix-label">"${token}" embedding (first 8 dims)</div>
                                <div class="mini-matrix">
                                    [${embeddings[idx].slice(0, 8).map(v => v.toFixed(3)).join(', ')}...]
                                </div>
                            </div>
                        `).join('')}
                    </div>

                    <div class="formula-box">
                        X ‚àà ‚Ñù<sup>${tokens.length} √ó ${mhsa.embedDim}</sup><br>
                        where each row is a token's embedding vector
                    </div>
                </div>
            `;
        }

        function generateStep2_Projections(tokens, embeddings, mhsa, head0) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">3</span>
                        Linear Projections to Q, K, V
                    </div>
                    <div class="step-description">
                        Each embedding is transformed into three different representations using learned weight matrices:
                        <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>.
                    </div>

                    <div class="computation-visual">
                        <h4 style="margin-bottom: 15px;">Projection Process (Head 1)</h4>
                        <div class="visual-equation">
                            <div class="eq-component" style="border-color: #FF6B6B;">
                                Q = X ¬∑ W<sup>Q</sup><br>
                                <small>[${tokens.length} √ó ${mhsa.headDim}]</small>
                            </div>
                            <div class="eq-component" style="border-color: #4ECDC4;">
                                K = X ¬∑ W<sup>K</sup><br>
                                <small>[${tokens.length} √ó ${mhsa.headDim}]</small>
                            </div>
                            <div class="eq-component" style="border-color: #95E1D3;">
                                V = X ¬∑ W<sup>V</sup><br>
                                <small>[${tokens.length} √ó ${mhsa.headDim}]</small>
                            </div>
                        </div>

                        <div style="margin: 30px 0;">
                            <h4>Example: Token "${tokens[0]}"</h4>
                            <div class="data-flow">
                                <div class="data-box active">
                                    <div class="data-label">Embedding</div>
                                    <div class="data-value">[${mhsa.embedDim}d]</div>
                                </div>
                                <div class="operation-symbol">√ó</div>
                                <div class="data-box" style="border-color: #FF6B6B;">
                                    <div class="data-label">W<sup>Q</sup></div>
                                    <div class="data-value">[${mhsa.embedDim}√ó${mhsa.headDim}]</div>
                                </div>
                                <div class="operation-symbol">=</div>
                                <div class="data-box active" style="background: linear-gradient(135deg, #FF6B6B, #EE5A6F); color: white;">
                                    <div class="data-label">Query</div>
                                    <div class="data-value">[${mhsa.headDim}d]</div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="qkv-grid">
                        <div class="qkv-card qkv-query">
                            <h3>üîç Queries</h3>
                            <div class="mini-matrix">
                                ${tokens.slice(0, 3).map((t, i) => 
                                    `${t}: [${head0.Q[i].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]`
                                ).join('<br>')}
                            </div>
                        </div>
                        <div class="qkv-card qkv-key">
                            <h3>üîë Keys</h3>
                            <div class="mini-matrix">
                                ${tokens.slice(0, 3).map((t, i) => 
                                    `${t}: [${head0.K[i].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]`
                                ).join('<br>')}
                            </div>
                        </div>
                        <div class="qkv-card qkv-value">
                            <h3>üíé Values</h3>
                            <div class="mini-matrix">
                                ${tokens.slice(0, 3).map((t, i) => 
                                    `${t}: [${head0.V[i].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]`
                                ).join('<br>')}
                            </div>
                        </div>
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> The weight matrices W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> 
                        are learned during training. They transform the embedding from d<sub>model</sub>=${mhsa.embedDim} 
                        to d<sub>k</sub>=${mhsa.headDim} dimensions per head.
                    </div>
                </div>
            `;
        }

        function generateStep3_DotProduct(tokens, head0, focusToken) {
            const focusTokenName = tokens[focusToken];
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">4</span>
                        Compute Query-Key Dot Product (Q ¬∑ K<sup>T</sup>)
                    </div>
                    <div class="step-description">
                        For each query vector, we compute its dot product with all key vectors.
                        This measures the similarity between queries and keys.
                        <br><strong>Focus: Token "${focusTokenName}"</strong>
                    </div>

                    <div class="computation-visual">
                        <h4>Attention Scores for "${focusTokenName}"</h4>
                        <div class="attention-computation-viz">
                            <div class="token-row">
                                <strong style="min-width: 100px;">Query:</strong>
                                <div class="token-cell highlight">
                                    ${focusTokenName}<br>
                                    Q[${head0.Q[focusToken].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]
                                </div>
                            </div>

                            <div style="text-align: center; margin: 20px 0; font-size: 24px; color: #667eea;">
                                ‚äó (dot product with each key)
                            </div>

                            ${tokens.map((token, idx) => {
                                const dotProduct = head0.Q[focusToken].reduce((sum, q, i) => 
                                    sum + q * head0.K[idx][i], 0
                                );
                                const isMax = idx === focusToken || 
                                    Math.abs(dotProduct) === Math.max(...tokens.map((_, i) => 
                                        Math.abs(head0.Q[focusToken].reduce((s, q, j) => s + q * head0.K[i][j], 0))
                                    ));
                                return `
                                    <div class="token-row">
                                        <strong style="min-width: 100px;">Key ${idx + 1}:</strong>
                                        <div class="token-cell ${isMax ? 'highlight' : ''}">
                                            ${token}
                                        </div>
                                        <div class="operation-symbol" style="font-size: 20px;">‚Üí</div>
                                        <div class="token-cell ${isMax ? 'highlight' : ''}" style="min-width: 100px;">
                                            Score: ${dotProduct.toFixed(3)}
                                        </div>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                    </div>

                    <div class="formula-box">
                        score<sub>ij</sub> = Q<sub>i</sub> ¬∑ K<sub>j</sub><sup>T</sup> = Œ£<sub>k</sub> Q<sub>i,k</sub> √ó K<sub>j,k</sub>
                        <br><br>
                        For token "${focusTokenName}" (i=${focusToken}), we compute its similarity with all ${tokens.length} keys.
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> High dot product means the query and key are aligned in the learned space.
                        This indicates relevance: "${focusTokenName}" should pay attention to tokens with high scores.
                    </div>
                </div>
            `;
        }

        function generateStep4_Scaling(tokens, head0, mhsa, focusToken) {
            const focusTokenName = tokens[focusToken];
            const scale = Math.sqrt(mhsa.headDim);
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">5</span>
                        Scale by ‚àöd<sub>k</sub>
                    </div>
                    <div class="step-description">
                        Divide attention scores by ‚àöd<sub>k</sub> = ‚àö${mhsa.headDim} = ${scale.toFixed(2)}.
                        This prevents the dot products from becoming too large, which would make gradients very small.
                    </div>

                    <div class="computation-visual">
                        <h4>Scaling Transformation</h4>
                        <div class="visual-equation">
                            <div class="eq-component">Raw Scores</div>
                            <div class="eq-operator">√∑</div>
                            <div class="eq-component">‚àö${mhsa.headDim} = ${scale.toFixed(2)}</div>
                            <div class="eq-operator">=</div>
                            <div class="eq-component">Scaled Scores</div>
                        </div>

                        <div style="margin: 30px 0;">
                            <h4>Before and After Scaling for "${focusTokenName}"</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div>
                                    <strong>Before Scaling:</strong>
                                    ${tokens.map((token, idx) => {
                                        const rawScore = head0.Q[focusToken].reduce((sum, q, i) => 
                                            sum + q * head0.K[idx][i], 0
                                        );
                                        return `
                                            <div class="token-row">
                                                <div class="token-cell">
                                                    ${token}: ${rawScore.toFixed(3)}
                                                </div>
                                            </div>
                                        `;
                                    }).join('')}
                                </div>
                                <div>
                                    <strong>After Scaling:</strong>
                                    ${tokens.map((token, idx) => `
                                        <div class="token-row">
                                            <div class="token-cell highlight">
                                                ${token}: ${head0.scores[focusToken][idx].toFixed(3)}
                                            </div>
                                        </div>
                                    `).join('')}
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="formula-box">
                        scaled_score<sub>ij</sub> = score<sub>ij</sub> / ‚àöd<sub>k</sub>
                        <br><br>
                        This scaling factor keeps the variance of scores stable as d<sub>k</sub> increases.
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> Without scaling, large d<sub>k</sub> would lead to very large dot products,
                        pushing softmax into regions with extremely small gradients (vanishing gradient problem).
                    </div>
                </div>
            `;
        }

        function generateStep5_Softmax(tokens, head0, focusToken) {
            const focusTokenName = tokens[focusToken];
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">6</span>
                        Apply Softmax Normalization
                    </div>
                    <div class="step-description">
                        Convert scaled scores to probability distribution using softmax.
                        Now the attention weights sum to 1 and represent "how much to attend to each token."
                    </div>

                    <div class="computation-visual">
                        <h4>Softmax Transformation for "${focusTokenName}"</h4>
                        <canvas id="stepSoftmaxCanvas" width="600" height="300" style="display: block; margin: 20px auto;"></canvas>
                    </div>

                    <div class="visual-equation">
                        <div class="eq-component">Scaled Scores</div>
                        <div class="eq-operator">‚Üí</div>
                        <div class="eq-component">exp(x)</div>
                        <div class="eq-operator">‚Üí</div>
                        <div class="eq-component">Normalize</div>
                        <div class="eq-operator">=</div>
                        <div class="eq-component">Probabilities</div>
                    </div>

                    <div style="margin: 30px 0;">
                        <h4>Attention Weights Distribution</h4>
                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(120px, 1fr)); gap: 10px;">
                            ${tokens.map((token, idx) => {
                                const weight = head0.attentionWeights[focusToken][idx];
                                return `
                                    <div class="data-box ${weight > 0.15 ? 'active' : ''}">
                                        <div class="data-label">${token}</div>
                                        <div class="data-value">${(weight * 100).toFixed(1)}%</div>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                        <div style="text-align: center; margin-top: 15px; font-weight: 600;">
                            Sum = ${head0.attentionWeights[focusToken].reduce((a, b) => a + b, 0).toFixed(3)} ‚âà 1.0 ‚úì
                        </div>
                    </div>

                    <div class="formula-box">
                        attention<sub>ij</sub> = exp(scaled_score<sub>ij</sub>) / Œ£<sub>k</sub> exp(scaled_score<sub>ik</sub>)
                        <br><br>
                        where Œ£<sub>j</sub> attention<sub>ij</sub> = 1
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> Softmax creates a probability distribution. Tokens with higher scores
                        get higher attention weights, but all tokens contribute at least a little bit.
                    </div>
                </div>
            `;
        }

        function generateStep6_AttentionValues(tokens, head0, focusToken) {
            const focusTokenName = tokens[focusToken];
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">7</span>
                        Multiply Attention √ó Values
                    </div>
                    <div class="step-description">
                        Use attention weights to compute a weighted sum of value vectors.
                        This creates a context-aware representation for "${focusTokenName}".
                    </div>

                    <div class="computation-visual">
                        <h4>Weighted Aggregation</h4>
                        <div class="formula-box">
                            output[${focusTokenName}] = ${tokens.map((t, i) => 
                                `${head0.attentionWeights[focusToken][i].toFixed(3)} √ó V[${t}]`
                            ).join(' +<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ')}
                        </div>

                        <div style="margin: 30px 0;">
                            <h4>Visual Breakdown</h4>
                            ${tokens.map((token, idx) => {
                                const weight = head0.attentionWeights[focusToken][idx];
                                return `
                                    <div style="display: flex; align-items: center; gap: 15px; margin: 15px 0; 
                                                padding: 15px; background: ${weight > 0.15 ? '#e7f3ff' : '#f8f9fa'}; 
                                                border-radius: 8px; border-left: 4px solid ${weight > 0.15 ? '#667eea' : '#dee2e6'};">
                                        <div style="min-width: 80px; font-weight: 700;">${token}</div>
                                        <div style="min-width: 80px;">
                                            Weight: <strong>${(weight * 100).toFixed(1)}%</strong>
                                        </div>
                                        <div style="flex: 1;">
                                            <div style="background: #667eea; height: 25px; width: ${weight * 100}%; 
                                                        border-radius: 4px; transition: width 0.3s;"></div>
                                        </div>
                                        <div style="min-width: 200px; font-family: monospace; font-size: 11px;">
                                            V: [${head0.V[idx].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]
                                        </div>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                    </div>

                    <div class="result-box">
                        <h4 style="margin-bottom: 10px;">Output for "${focusTokenName}"</h4>
                        <div style="font-family: monospace;">
                            [${head0.output[focusToken].slice(0, 8).map(v => v.toFixed(3)).join(', ')}...]
                        </div>
                        <div style="margin-top: 10px; font-size: 14px;">
                            This is a ${head0.output[focusToken].length}-dimensional contextualized representation
                        </div>
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> Tokens with higher attention weights contribute more to the output.
                        This allows "${focusTokenName}" to incorporate relevant information from the entire sequence.
                    </div>
                </div>
            `;
        }

        function generateStep7_Concatenate(tokens, result, mhsa) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">8</span>
                        Concatenate Multiple Heads
                    </div>
                    <div class="step-description">
                        Each of the ${mhsa.numHeads} heads has produced its own output.
                        We concatenate these outputs along the feature dimension.
                    </div>

                    <div class="computation-visual">
                        <h4>Multi-Head Concatenation</h4>
                        <div class="data-flow">
                            ${Array.from({length: mhsa.numHeads}, (_, h) => `
                                <div class="data-box active">
                                    <div class="data-label">Head ${h + 1}</div>
                                    <div class="data-value">[${tokens.length}√ó${mhsa.headDim}]</div>
                                </div>
                            `).join('<div class="operation-symbol" style="font-size: 24px;">‚äï</div>')}
                        </div>

                        <div style="text-align: center; margin: 30px 0;">
                            <div class="operation-symbol">‚Üì</div>
                            <div class="data-box active" style="display: inline-block; margin-top: 20px; font-size: 18px;">
                                Concatenated Output<br>
                                <div class="data-value">[${tokens.length} √ó ${mhsa.numHeads * mhsa.headDim}]</div>
                            </div>
                        </div>

                        <div style="margin: 30px 0;">
                            <h4>Why Multiple Heads?</h4>
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px;">
                                <div style="background: white; padding: 15px; border-radius: 8px; border-left: 4px solid #FF6B6B;">
                                    <strong>Head 1 might focus on:</strong><br>
                                    Syntactic relationships<br>
                                    (subject-verb agreement)
                                </div>
                                <div style="background: white; padding: 15px; border-radius: 8px; border-left: 4px solid #4ECDC4;">
                                    <strong>Head 2 might focus on:</strong><br>
                                    Semantic relationships<br>
                                    (word meanings)
                                </div>
                                <div style="background: white; padding: 15px; border-radius: 8px; border-left: 4px solid #95E1D3;">
                                    <strong>Head 3 might focus on:</strong><br>
                                    Long-range dependencies<br>
                                    (distant words)
                                </div>
                                <div style="background: white; padding: 15px; border-radius: 8px; border-left: 4px solid #F7DC6F;">
                                    <strong>Head 4 might focus on:</strong><br>
                                    Local context<br>
                                    (adjacent words)
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="formula-box">
                        MultiHead = Concat(head<sub>1</sub>, head<sub>2</sub>, ..., head<sub>${mhsa.numHeads}</sub>)
                        <br><br>
                        Each head<sub>i</sub> ‚àà ‚Ñù<sup>${tokens.length} √ó ${mhsa.headDim}</sup><br>
                        MultiHead ‚àà ‚Ñù<sup>${tokens.length} √ó ${mhsa.numHeads * mhsa.headDim}</sup>
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> Multiple heads allow the model to attend to different aspects simultaneously.
                        Each head can specialize in capturing different types of relationships in the data.
                    </div>
                </div>
            `;
        }

        function generateStep8_FinalOutput(tokens, embeddings, result) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">9</span>
                        Final Output Projection
                    </div>
                    <div class="step-description">
                        Apply a final linear transformation W<sup>O</sup> to project the concatenated heads
                        back to the original embedding dimension.
                    </div>

                    <div class="computation-visual">
                        <h4>Output Projection</h4>
                        <div class="visual-equation">
                            <div class="eq-component">
                                Concatenated<br>
                                [${tokens.length} √ó ${result.headOutputs.length * result.headOutputs[0].output[0].length}]
                            </div>
                            <div class="eq-operator">√ó</div>
                            <div class="eq-component">
                                W<sup>O</sup><br>
                                [${result.headOutputs.length * result.headOutputs[0].output[0].length} √ó ${embeddings[0].length}]
                            </div>
                            <div class="eq-operator">=</div>
                            <div class="eq-component" style="background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);">
                                Final Output<br>
                                [${tokens.length} √ó ${embeddings[0].length}]
                            </div>
                        </div>

                        <div style="margin: 30px 0;">
                            <h4>Transformation Comparison</h4>
                            ${tokens.slice(0, 3).map((token, idx) => `
                                <div style="margin: 20px 0; padding: 20px; background: white; border-radius: 8px; border: 2px solid #667eea;">
                                    <div style="font-weight: 700; font-size: 18px; margin-bottom: 15px; color: #667eea;">
                                        Token: "${token}"
                                    </div>
                                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                        <div>
                                            <div style="font-weight: 600; margin-bottom: 8px;">Input Embedding:</div>
                                            <div style="font-family: monospace; font-size: 11px; background: #f8f9fa; padding: 10px; border-radius: 4px;">
                                                [${embeddings[idx].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]
                                            </div>
                                            <div style="margin-top: 8px; font-size: 12px; color: #6c757d;">
                                                Static representation
                                            </div>
                                        </div>
                                        <div>
                                            <div style="font-weight: 600; margin-bottom: 8px;">After Attention:</div>
                                            <div style="font-family: monospace; font-size: 11px; background: linear-gradient(135deg, #e7f3ff, #f0e7ff); padding: 10px; border-radius: 4px;">
                                                [${result.output[idx].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]
                                            </div>
                                            <div style="margin-top: 8px; font-size: 12px; color: #667eea; font-weight: 600;">
                                                Contextualized representation ‚ú®
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            `).join('')}
                        </div>
                    </div>

                    <div class="result-box">
                        <h4 style="margin-bottom: 10px;">üéâ Complete! Multi-Head Self-Attention Output</h4>
                        <div style="font-size: 16px; line-height: 1.8;">
                            ‚úì Each token now has a context-aware representation<br>
                            ‚úì Tokens have incorporated information from the entire sequence<br>
                            ‚úì Multiple attention heads captured different relationship patterns<br>
                            ‚úì Output shape matches input shape: [${tokens.length} √ó ${embeddings[0].length}]
                        </div>
                    </div>

                    <div class="info-box">
                        <strong>üí° Key Point:</strong> The final output maintains the same shape as the input,
                        but now each token's representation is enriched with contextual information from all other tokens.
                        This contextualized output can be fed into subsequent layers of the transformer.
                    </div>

                    <div class="formula-box">
                        Output = MultiHead ¬∑ W<sup>O</sup><br>
                        <br>
                        This completes one full multi-head self-attention layer!<br>
                        In a real transformer, this would be followed by:<br>
                        ‚Ä¢ Residual connection: output + input<br>
                        ‚Ä¢ Layer normalization<br>
                        ‚Ä¢ Feed-forward network<br>
                        ‚Ä¢ More transformer layers...
                    </div>
                </div>
            `;
        }

        // Run on page load
        window.onload = function() {
            runAttention();
        };
    </script>
</body>
</html>
