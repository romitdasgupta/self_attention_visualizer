<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Self-Attention Visualizer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .controls {
            padding: 25px;
            background: #f8f9fa;
            border-bottom: 2px solid #dee2e6;
        }

        .control-group {
            margin-bottom: 20px;
        }

        label {
            display: block;
            font-weight: 600;
            margin-bottom: 8px;
            color: #495057;
        }

        input[type="text"],
        input[type="number"],
        select {
            width: 100%;
            padding: 10px;
            border: 2px solid #dee2e6;
            border-radius: 6px;
            font-size: 14px;
            transition: border-color 0.3s;
        }

        input[type="text"]:focus,
        input[type="number"]:focus,
        select:focus {
            outline: none;
            border-color: #667eea;
        }

        .control-row {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }

        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        button:active {
            transform: translateY(0);
        }

        .content {
            padding: 25px;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }

        .section-title {
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #495057;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background: #667eea;
            border-radius: 50%;
        }

        .visualization-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 15px;
        }

        .viz-card {
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .viz-card h3 {
            margin-bottom: 12px;
            color: #495057;
            font-size: 1.1em;
        }

        canvas {
            display: block;
            width: 100%;
            border-radius: 4px;
            border: 1px solid #dee2e6;
        }

        .matrix-display {
            font-family: 'Courier New', monospace;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 12px;
            line-height: 1.6;
        }

        .matrix-row {
            white-space: nowrap;
        }

        .matrix-value {
            display: inline-block;
            min-width: 60px;
            text-align: right;
            padding: 2px 4px;
        }

        .token-display {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin: 15px 0;
        }

        .token {
            background: white;
            padding: 8px 16px;
            border-radius: 20px;
            border: 2px solid #667eea;
            font-weight: 600;
            color: #667eea;
        }

        .math-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 6px;
            border-left: 3px solid #667eea;
        }

        .math-step h4 {
            color: #667eea;
            margin-bottom: 8px;
        }

        .formula {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            margin: 8px 0;
            overflow-x: auto;
        }

        .info-box {
            background: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: #1976D2;
        }

        .attention-grid {
            display: grid;
            gap: 2px;
            margin: 15px 0;
        }

        .attention-cell {
            padding: 8px;
            text-align: center;
            font-size: 11px;
            border-radius: 2px;
            color: white;
            font-weight: 600;
        }

        .head-container {
            margin-bottom: 20px;
            padding: 15px;
            background: white;
            border-radius: 8px;
            border: 2px solid #dee2e6;
        }

        .head-title {
            font-weight: 700;
            color: #667eea;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
            border-bottom: 2px solid #dee2e6;
        }

        .tab {
            padding: 10px 20px;
            background: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            cursor: pointer;
            font-weight: 600;
            color: #6c757d;
            transition: all 0.3s;
        }

        .tab.active {
            color: #667eea;
            border-bottom-color: #667eea;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Multi-Head Self-Attention Visualizer</h1>
            <p class="subtitle">Understanding Transformer Attention from First Principles</p>
        </header>

        <div class="controls">
            <div class="control-group">
                <label for="inputText">Input Sequence (space-separated tokens):</label>
                <input type="text" id="inputText" value="The cat sat on the mat" placeholder="Enter your text...">
            </div>

            <div class="control-row">
                <div class="control-group">
                    <label for="embedDim">Embedding Dimension:</label>
                    <input type="number" id="embedDim" value="64" min="8" max="512" step="8">
                </div>

                <div class="control-group">
                    <label for="numHeads">Number of Heads:</label>
                    <input type="number" id="numHeads" value="4" min="1" max="8">
                </div>

                <div class="control-group">
                    <label for="temperature">Temperature (for attention):</label>
                    <input type="number" id="temperature" value="1.0" min="0.1" max="2.0" step="0.1">
                </div>
            </div>

            <div class="control-group" style="margin-top: 15px;">
                <button onclick="runAttention()">Compute Attention</button>
            </div>
        </div>

        <div class="content">
            <div class="section">
                <div class="section-title">Input Tokens</div>
                <div id="tokenDisplay" class="token-display"></div>
                <div class="info-box">
                    <strong>What's happening:</strong> Each token is converted to a dense vector representation (embedding) of dimension d_model.
                </div>
            </div>

            <div class="section">
                <div class="section-title">Architecture Overview</div>
                <div id="architectureInfo"></div>
            </div>

            <div class="tabs">
                <button class="tab active" onclick="switchTab('overview')">Overview</button>
                <button class="tab" onclick="switchTab('heads')">Individual Heads</button>
                <button class="tab" onclick="switchTab('mathematics')">Mathematics</button>
            </div>

            <div id="overview" class="tab-content active">
                <div class="section">
                    <div class="section-title">Combined Attention Pattern</div>
                    <div class="viz-card">
                        <h3>Average Attention Weights Across All Heads</h3>
                        <canvas id="combinedAttentionCanvas"></canvas>
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> This shows the average attention pattern across all heads. Each cell (i,j) shows how much token i attends to token j.
                    </div>
                </div>
            </div>

            <div id="heads" class="tab-content">
                <div class="section">
                    <div class="section-title">Individual Head Attention Patterns</div>
                    <div id="headsContainer"></div>
                    <div class="info-box">
                        <strong>What's happening:</strong> Each head learns to attend to different aspects of the input. Some might focus on local patterns, others on long-range dependencies.
                    </div>
                </div>
            </div>

            <div id="mathematics" class="tab-content">
                <div class="section">
                    <div class="section-title">Step-by-Step Mathematical Breakdown</div>
                    <div id="mathBreakdown"></div>
                </div>
            </div>
        </div>
    </div>

    <script>
        let currentAttention = null;

        function switchTab(tabName) {
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));

            event.target.classList.add('active');
            document.getElementById(tabName).classList.add('active');
        }

        class MultiHeadSelfAttention {
            constructor(embedDim, numHeads, temperature = 1.0) {
                this.embedDim = embedDim;
                this.numHeads = numHeads;
                this.headDim = Math.floor(embedDim / numHeads);
                this.temperature = temperature;

                // Initialize weight matrices for each head
                this.initializeWeights();
            }

            initializeWeights() {
                this.heads = [];
                for (let h = 0; h < this.numHeads; h++) {
                    this.heads.push({
                        Wq: this.randomMatrix(this.embedDim, this.headDim),
                        Wk: this.randomMatrix(this.embedDim, this.headDim),
                        Wv: this.randomMatrix(this.embedDim, this.headDim)
                    });
                }
                // Output projection
                this.Wo = this.randomMatrix(this.numHeads * this.headDim, this.embedDim);
            }

            randomMatrix(rows, cols) {
                const matrix = [];
                const scale = Math.sqrt(2.0 / (rows + cols)); // Xavier initialization
                for (let i = 0; i < rows; i++) {
                    matrix[i] = [];
                    for (let j = 0; j < cols; j++) {
                        matrix[i][j] = (Math.random() - 0.5) * 2 * scale;
                    }
                }
                return matrix;
            }

            matmul(A, B) {
                const rowsA = A.length;
                const colsA = A[0].length;
                const colsB = B[0].length;
                const C = [];

                for (let i = 0; i < rowsA; i++) {
                    C[i] = [];
                    for (let j = 0; j < colsB; j++) {
                        let sum = 0;
                        for (let k = 0; k < colsA; k++) {
                            sum += A[i][k] * B[k][j];
                        }
                        C[i][j] = sum;
                    }
                }
                return C;
            }

            transpose(A) {
                const rows = A.length;
                const cols = A[0].length;
                const T = [];
                for (let j = 0; j < cols; j++) {
                    T[j] = [];
                    for (let i = 0; i < rows; i++) {
                        T[j][i] = A[i][j];
                    }
                }
                return T;
            }

            softmax(scores) {
                const result = [];
                for (let i = 0; i < scores.length; i++) {
                    const row = scores[i];
                    const maxScore = Math.max(...row);
                    const expScores = row.map(s => Math.exp((s - maxScore) / this.temperature));
                    const sumExp = expScores.reduce((a, b) => a + b, 0);
                    result[i] = expScores.map(e => e / sumExp);
                }
                return result;
            }

            scaledDotProductAttention(Q, K, V) {
                // Compute attention scores: Q @ K^T
                const KT = this.transpose(K);
                const scores = this.matmul(Q, KT);

                // Scale by sqrt(d_k)
                const scale = Math.sqrt(this.headDim);
                const scaledScores = scores.map(row =>
                    row.map(val => val / scale)
                );

                // Apply softmax
                const attentionWeights = this.softmax(scaledScores);

                // Apply attention to values: attention @ V
                const output = this.matmul(attentionWeights, V);

                return {
                    output,
                    attentionWeights,
                    scores: scaledScores
                };
            }

            forward(embeddings) {
                const seqLen = embeddings.length;
                const headOutputs = [];
                const headAttentions = [];

                // Process each head
                for (let h = 0; h < this.numHeads; h++) {
                    const head = this.heads[h];

                    // Project to Q, K, V
                    const Q = this.matmul(embeddings, head.Wq);
                    const K = this.matmul(embeddings, head.Wk);
                    const V = this.matmul(embeddings, head.Wv);

                    // Compute attention
                    const { output, attentionWeights, scores } =
                        this.scaledDotProductAttention(Q, K, V);

                    headOutputs.push({
                        Q, K, V, output, attentionWeights, scores
                    });
                    headAttentions.push(attentionWeights);
                }

                // Concatenate heads
                const concatenated = [];
                for (let i = 0; i < seqLen; i++) {
                    concatenated[i] = [];
                    for (let h = 0; h < this.numHeads; h++) {
                        concatenated[i].push(...headOutputs[h].output[i]);
                    }
                }

                // Final linear projection
                const finalOutput = this.matmul(concatenated, this.Wo);

                return {
                    output: finalOutput,
                    headOutputs,
                    headAttentions
                };
            }
        }

        function generateEmbeddings(tokens, embedDim) {
            const embeddings = [];
            for (let i = 0; i < tokens.length; i++) {
                const embedding = [];
                // Generate deterministic embeddings based on token hash
                const hash = tokens[i].split('').reduce((acc, char) =>
                    acc + char.charCodeAt(0), 0);
                for (let j = 0; j < embedDim; j++) {
                    const angle = (hash * 0.1 + j * 0.5) % (2 * Math.PI);
                    embedding[j] = Math.sin(angle) * 0.5;
                }
                embeddings.push(embedding);
            }
            return embeddings;
        }

        function drawAttentionHeatmap(canvas, attentionWeights, tokens) {
            const ctx = canvas.getContext('2d');
            const size = attentionWeights.length;
            const cellSize = Math.min(400 / size, 50);
            const margin = 80;

            canvas.width = size * cellSize + margin * 2;
            canvas.height = size * cellSize + margin * 2;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw heatmap
            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size; j++) {
                    const weight = attentionWeights[i][j];
                    const intensity = Math.floor(weight * 255);
                    ctx.fillStyle = `rgb(${255-intensity}, ${255-intensity/2}, 255)`;
                    ctx.fillRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw border
                    ctx.strokeStyle = '#dee2e6';
                    ctx.strokeRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw value
                    ctx.fillStyle = weight > 0.5 ? 'white' : 'black';
                    ctx.font = `${Math.min(cellSize/3, 12)}px Arial`;
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    ctx.fillText(
                        weight.toFixed(2),
                        margin + j * cellSize + cellSize/2,
                        margin + i * cellSize + cellSize/2
                    );
                }
            }

            // Draw labels
            ctx.fillStyle = '#495057';
            ctx.font = '14px Arial';
            ctx.textAlign = 'right';
            for (let i = 0; i < size; i++) {
                ctx.fillText(tokens[i], margin - 10, margin + i * cellSize + cellSize/2);
            }

            ctx.textAlign = 'center';
            for (let j = 0; j < size; j++) {
                ctx.save();
                ctx.translate(margin + j * cellSize + cellSize/2, margin - 10);
                ctx.rotate(-Math.PI / 4);
                ctx.fillText(tokens[j], 0, 0);
                ctx.restore();
            }

            // Draw axis labels
            ctx.font = 'bold 16px Arial';
            ctx.fillStyle = '#667eea';
            ctx.textAlign = 'center';
            ctx.fillText('Keys (attending to)', canvas.width / 2, 30);
            ctx.save();
            ctx.translate(30, canvas.height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Queries (attending from)', 0, 0);
            ctx.restore();
        }

        function formatMatrix(matrix, label) {
            let html = `<h4>${label}</h4><div class="matrix-display">`;
            html += matrix.map((row, i) =>
                `<div class="matrix-row">[${row.map(v =>
                    `<span class="matrix-value">${v.toFixed(4)}</span>`
                ).join(' ')}]</div>`
            ).join('');
            html += '</div>';
            return html;
        }

        function runAttention() {
            const inputText = document.getElementById('inputText').value;
            const embedDim = parseInt(document.getElementById('embedDim').value);
            const numHeads = parseInt(document.getElementById('numHeads').value);
            const temperature = parseFloat(document.getElementById('temperature').value);

            const tokens = inputText.trim().split(/\s+/);

            // Display tokens
            document.getElementById('tokenDisplay').innerHTML = tokens.map(
                (t, i) => `<div class="token">${i+1}. ${t}</div>`
            ).join('');

            // Generate embeddings
            const embeddings = generateEmbeddings(tokens, embedDim);

            // Create and run attention
            const mhsa = new MultiHeadSelfAttention(embedDim, numHeads, temperature);
            const result = mhsa.forward(embeddings);
            currentAttention = result;

            // Display architecture info
            document.getElementById('architectureInfo').innerHTML = `
                <div class="info-box">
                    <strong>Configuration:</strong><br>
                    • Sequence Length: ${tokens.length} tokens<br>
                    • Embedding Dimension (d_model): ${embedDim}<br>
                    • Number of Heads: ${numHeads}<br>
                    • Head Dimension (d_k = d_model/num_heads): ${mhsa.headDim}<br>
                    • Temperature: ${temperature}
                </div>
            `;

            // Draw combined attention
            const avgAttention = averageAttentions(result.headAttentions);
            const combinedCanvas = document.getElementById('combinedAttentionCanvas');
            drawAttentionHeatmap(combinedCanvas, avgAttention, tokens);

            // Draw individual heads
            const headsContainer = document.getElementById('headsContainer');
            headsContainer.innerHTML = '';
            for (let h = 0; h < numHeads; h++) {
                const headDiv = document.createElement('div');
                headDiv.className = 'head-container';
                headDiv.innerHTML = `
                    <div class="head-title">Head ${h + 1}</div>
                    <canvas id="headCanvas${h}"></canvas>
                `;
                headsContainer.appendChild(headDiv);

                setTimeout(() => {
                    const canvas = document.getElementById(`headCanvas${h}`);
                    drawAttentionHeatmap(canvas, result.headAttentions[h], tokens);
                }, 10);
            }

            // Generate mathematical breakdown
            generateMathBreakdown(tokens, embeddings, mhsa, result);
        }

        function averageAttentions(attentions) {
            const numHeads = attentions.length;
            const seqLen = attentions[0].length;
            const avg = [];

            for (let i = 0; i < seqLen; i++) {
                avg[i] = [];
                for (let j = 0; j < seqLen; j++) {
                    let sum = 0;
                    for (let h = 0; h < numHeads; h++) {
                        sum += attentions[h][i][j];
                    }
                    avg[i][j] = sum / numHeads;
                }
            }
            return avg;
        }

        function generateMathBreakdown(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('mathBreakdown');

            let html = `
                <div class="math-step">
                    <h4>Step 1: Input Embeddings</h4>
                    <p>Each token is represented as a ${mhsa.embedDim}-dimensional vector.</p>
                    <div class="formula">X ∈ ℝ^(${tokens.length} × ${mhsa.embedDim})</div>
                    ${formatMatrix(embeddings.map(e => e.slice(0, 8)), 'Embeddings (first 8 dimensions)')}
                </div>

                <div class="math-step">
                    <h4>Step 2: Linear Projections</h4>
                    <p>For each head h, we project the input into Query, Key, and Value spaces:</p>
                    <div class="formula">
                        Q_h = X · W^Q_h<br>
                        K_h = X · W^K_h<br>
                        V_h = X · W^V_h<br>
                        where W^Q_h, W^K_h, W^V_h ∈ ℝ^(${mhsa.embedDim} × ${mhsa.headDim})
                    </div>
                    <div class="info-box">
                        Each head has its own projection matrices, allowing it to learn different representations.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 3: Scaled Dot-Product Attention</h4>
                    <p>For each head, we compute attention scores:</p>
                    <div class="formula">
                        scores = (Q · K^T) / √d_k<br>
                        where d_k = ${mhsa.headDim}
                    </div>
                    <p>The scaling factor prevents the dot products from becoming too large.</p>
                </div>

                <div class="math-step">
                    <h4>Step 4: Softmax Normalization</h4>
                    <p>Apply softmax to get attention weights that sum to 1:</p>
                    <div class="formula">
                        Attention(Q, K, V) = softmax(scores / temperature) · V<br>
                        temperature = ${mhsa.temperature}
                    </div>
                    <p>Higher temperature makes the distribution more uniform; lower makes it more peaked.</p>
                </div>
            `;

            // Show first head's computations in detail
            const head0 = result.headOutputs[0];
            html += `
                <div class="math-step">
                    <h4>Example: Head 1 Detailed Computation</h4>
                    ${formatMatrix(head0.Q.map(q => q.slice(0, 4)), 'Q (first 4 dims)')}
                    ${formatMatrix(head0.K.map(k => k.slice(0, 4)), 'K (first 4 dims)')}
                    ${formatMatrix(head0.scores, 'Attention Scores (scaled)')}
                    ${formatMatrix(head0.attentionWeights, 'Attention Weights (after softmax)')}
                </div>

                <div class="math-step">
                    <h4>Step 5: Concatenate Heads</h4>
                    <p>Concatenate outputs from all ${mhsa.numHeads} heads:</p>
                    <div class="formula">
                        MultiHead = Concat(head_1, head_2, ..., head_${mhsa.numHeads})<br>
                        where each head_i ∈ ℝ^(${tokens.length} × ${mhsa.headDim})
                    </div>
                    <p>Result: ℝ^(${tokens.length} × ${mhsa.numHeads * mhsa.headDim})</p>
                </div>

                <div class="math-step">
                    <h4>Step 6: Output Projection</h4>
                    <p>Final linear transformation:</p>
                    <div class="formula">
                        Output = MultiHead · W^O<br>
                        where W^O ∈ ℝ^(${mhsa.numHeads * mhsa.headDim} × ${mhsa.embedDim})
                    </div>
                    ${formatMatrix(result.output.map(o => o.slice(0, 8)), 'Final Output (first 8 dimensions)')}
                </div>

                <div class="info-box">
                    <strong>Key Insights:</strong><br>
                    • Each attention head can learn to focus on different relationships<br>
                    • The attention weights are position-dependent and learned from data<br>
                    • Self-attention allows every token to attend to every other token<br>
                    • Scaling by √d_k stabilizes gradients during training<br>
                    • The final output combines information from all attention heads
                </div>
            `;

            container.innerHTML = html;
        }

        // Run on page load
        window.onload = function() {
            runAttention();
        };
    </script>
</body>
</html>
