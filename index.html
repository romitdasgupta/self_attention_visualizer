<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Self-Attention Visualizer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .controls {
            padding: 25px;
            background: #f8f9fa;
            border-bottom: 2px solid #dee2e6;
        }

        .control-group {
            margin-bottom: 20px;
        }

        label {
            display: block;
            font-weight: 600;
            margin-bottom: 8px;
            color: #495057;
        }

        input[type="text"],
        input[type="number"],
        select {
            width: 100%;
            padding: 10px;
            border: 2px solid #dee2e6;
            border-radius: 6px;
            font-size: 14px;
            transition: border-color 0.3s;
        }

        input[type="text"]:focus,
        input[type="number"]:focus,
        select:focus {
            outline: none;
            border-color: #667eea;
        }

        .control-row {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }

        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        button:active {
            transform: translateY(0);
        }

        .content {
            padding: 25px;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }

        .section-title {
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #495057;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background: #667eea;
            border-radius: 50%;
        }

        .visualization-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 15px;
        }

        .viz-card {
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .viz-card h3 {
            margin-bottom: 12px;
            color: #495057;
            font-size: 1.1em;
        }

        canvas {
            display: block;
            width: 100%;
            border-radius: 4px;
            border: 1px solid #dee2e6;
        }

        .matrix-display {
            font-family: 'Courier New', monospace;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 12px;
            line-height: 1.6;
        }

        .matrix-row {
            white-space: nowrap;
        }

        .matrix-value {
            display: inline-block;
            min-width: 60px;
            text-align: right;
            padding: 2px 4px;
        }

        .token-display {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin: 15px 0;
        }

        .token {
            background: white;
            padding: 8px 16px;
            border-radius: 20px;
            border: 2px solid #667eea;
            font-weight: 600;
            color: #667eea;
        }

        .math-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 6px;
            border-left: 3px solid #667eea;
        }

        .math-step h4 {
            color: #667eea;
            margin-bottom: 8px;
        }

        .formula {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            margin: 8px 0;
            overflow-x: auto;
        }

        .info-box {
            background: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: #1976D2;
        }

        .attention-grid {
            display: grid;
            gap: 2px;
            margin: 15px 0;
        }

        .attention-cell {
            padding: 8px;
            text-align: center;
            font-size: 11px;
            border-radius: 2px;
            color: white;
            font-weight: 600;
        }

        .head-container {
            margin-bottom: 20px;
            padding: 15px;
            background: white;
            border-radius: 8px;
            border: 2px solid #dee2e6;
        }

        .head-title {
            font-weight: 700;
            color: #667eea;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
            border-bottom: 2px solid #dee2e6;
        }

        .tab {
            padding: 10px 20px;
            background: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            cursor: pointer;
            font-weight: 600;
            color: #6c757d;
            transition: all 0.3s;
        }

        .tab.active {
            color: #667eea;
            border-bottom-color: #667eea;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .flow-diagram {
            display: flex;
            align-items: center;
            justify-content: space-around;
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
            overflow-x: auto;
        }

        .flow-step {
            text-align: center;
            padding: 15px;
            min-width: 120px;
        }

        .flow-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            font-weight: 600;
            margin-bottom: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .flow-arrow {
            font-size: 2em;
            color: #667eea;
            margin: 0 10px;
        }

        .qkv-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .qkv-card {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .qkv-card h3 {
            margin-bottom: 15px;
            color: #495057;
        }

        .qkv-query {
            border-left: 4px solid #FF6B6B;
        }

        .qkv-key {
            border-left: 4px solid #4ECDC4;
        }

        .qkv-value {
            border-left: 4px solid #95E1D3;
        }

        .vector-display {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            font-size: 11px;
            margin: 8px 0;
            overflow-x: auto;
            max-height: 150px;
            overflow-y: auto;
        }

        .vector-row {
            margin: 4px 0;
        }

        .interaction-canvas {
            width: 100%;
            max-width: 600px;
            margin: 20px auto;
            display: block;
        }

        .meaning-box {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
        }

        .meaning-box h4 {
            margin-bottom: 10px;
        }

        .highlight-query {
            background: rgba(255, 107, 107, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .highlight-key {
            background: rgba(78, 205, 196, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .highlight-value {
            background: rgba(149, 225, 211, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .token-embedding-viz {
            display: flex;
            align-items: center;
            margin: 15px 0;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
        }

        .token-box {
            background: #667eea;
            color: white;
            padding: 10px 15px;
            border-radius: 6px;
            font-weight: 600;
            min-width: 80px;
            text-align: center;
        }

        .embedding-viz {
            display: flex;
            gap: 2px;
            margin: 0 15px;
        }

        .embedding-bar {
            width: 8px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 2px;
        }

        .projection-arrow {
            margin: 0 10px;
            font-size: 1.5em;
            color: #667eea;
        }

        /* Sequence Visualization Styles */
        .sequence-controls {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
        }

        .seq-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .seq-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        .seq-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .seq-btn-secondary {
            background: #6c757d;
            color: white;
            padding: 10px 25px;
            border: none;
            border-radius: 6px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
        }

        .seq-btn-secondary:hover {
            background: #5a6268;
        }

        .step-indicator {
            font-size: 18px;
            font-weight: 700;
            color: #667eea;
            min-width: 150px;
            text-align: center;
        }

        .step-progress {
            width: 100%;
            height: 8px;
            background: #e9ecef;
            border-radius: 4px;
            overflow: hidden;
            margin: 10px 0 30px 0;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.4s ease;
            width: 11.11%;
        }

        .sequence-visualization {
            background: white;
            border-radius: 8px;
            padding: 30px;
            min-height: 500px;
            margin: 20px 0;
        }

        .step-content {
            animation: fadeIn 0.5s ease-in;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .step-title {
            font-size: 24px;
            font-weight: 700;
            color: #667eea;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .step-number {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 18px;
            font-weight: 700;
        }

        .step-description {
            font-size: 16px;
            line-height: 1.8;
            color: #495057;
            margin-bottom: 20px;
        }

        .computation-visual {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .data-flow {
            display: flex;
            align-items: center;
            justify-content: space-around;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .data-box {
            background: white;
            border: 3px solid #667eea;
            border-radius: 8px;
            padding: 15px 20px;
            min-width: 120px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            transition: all 0.3s;
        }

        .data-box.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(102, 126, 234, 0.4);
        }

        .data-label {
            font-size: 12px;
            font-weight: 600;
            text-transform: uppercase;
            opacity: 0.7;
            margin-bottom: 5px;
        }

        .data-value {
            font-size: 18px;
            font-weight: 700;
        }

        .operation-symbol {
            font-size: 36px;
            color: #667eea;
            font-weight: 700;
        }

        .matrix-viz-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .matrix-card {
            background: white;
            border-radius: 8px;
            padding: 15px;
            border: 2px solid #dee2e6;
        }

        .matrix-card.highlight {
            border-color: #667eea;
            box-shadow: 0 0 20px rgba(102, 126, 234, 0.3);
        }

        .matrix-label {
            font-weight: 700;
            color: #495057;
            margin-bottom: 10px;
            font-size: 14px;
        }

        .mini-matrix {
            font-family: 'Courier New', monospace;
            font-size: 10px;
            background: #f8f9fa;
            padding: 8px;
            border-radius: 4px;
            overflow-x: auto;
        }

        .step-navigator {
            margin-top: 30px;
        }

        .nav-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 10px;
        }

        .nav-step {
            background: #f8f9fa;
            border: 2px solid #dee2e6;
            padding: 12px;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.3s;
            text-align: center;
        }

        .nav-step:hover {
            border-color: #667eea;
            background: #e7f3ff;
        }

        .nav-step.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: #667eea;
        }

        .nav-step-num {
            font-weight: 700;
            font-size: 12px;
            margin-bottom: 5px;
        }

        .nav-step-name {
            font-size: 11px;
        }

        .attention-computation-viz {
            margin: 20px 0;
        }

        .token-row {
            display: flex;
            gap: 10px;
            margin: 10px 0;
            align-items: center;
        }

        .token-cell {
            flex: 1;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 4px;
            text-align: center;
            font-size: 12px;
            border: 2px solid transparent;
            transition: all 0.3s;
        }

        .token-cell.highlight {
            border-color: #667eea;
            background: #e7f3ff;
            transform: scale(1.05);
        }

        .formula-box {
            background: #fffbf0;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }

        .visual-equation {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            margin: 20px 0;
            font-size: 18px;
            font-weight: 600;
        }

        .eq-component {
            padding: 15px 25px;
            background: white;
            border: 2px solid #667eea;
            border-radius: 8px;
        }

        .eq-operator {
            color: #667eea;
            font-size: 28px;
        }

        .result-box {
            background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
            color: #1a1a1a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            font-weight: 600;
            text-align: center;
        }

        /* Enhanced dimension display styles */
        .dimension-tracker {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #fff;
            padding: 20px;
            border-radius: 12px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }

        .dimension-tracker h4 {
            color: #00d4ff;
            margin-bottom: 15px;
            font-size: 14px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .dim-row {
            display: flex;
            align-items: center;
            gap: 15px;
            margin: 12px 0;
            flex-wrap: wrap;
        }

        .dim-badge {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            background: rgba(255, 255, 255, 0.1);
            padding: 8px 14px;
            border-radius: 8px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            font-size: 13px;
        }

        .dim-badge.query { border-color: #FF6B6B; background: rgba(255, 107, 107, 0.15); }
        .dim-badge.key { border-color: #4ECDC4; background: rgba(78, 205, 196, 0.15); }
        .dim-badge.value { border-color: #95E1D3; background: rgba(149, 225, 211, 0.15); }
        .dim-badge.scores { border-color: #ffc107; background: rgba(255, 193, 7, 0.15); }
        .dim-badge.attention { border-color: #667eea; background: rgba(102, 126, 234, 0.15); }
        .dim-badge.output { border-color: #84fab0; background: rgba(132, 250, 176, 0.15); }

        .dim-label {
            color: #aaa;
            font-size: 11px;
        }

        .dim-shape {
            color: #00d4ff;
            font-weight: 700;
            font-size: 14px;
        }

        .dim-operator {
            color: #ffc107;
            font-size: 18px;
            font-weight: 700;
        }

        /* Matrix visualization box */
        .matrix-visual-box {
            background: #fff;
            border: 3px solid #667eea;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            position: relative;
        }

        .matrix-visual-box::before {
            content: attr(data-label);
            position: absolute;
            top: -12px;
            left: 20px;
            background: #667eea;
            color: white;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: 600;
        }

        .matrix-shape-display {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 30px;
            margin: 25px 0;
            flex-wrap: wrap;
        }

        .matrix-block {
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 15px;
        }

        .matrix-rect {
            border: 3px solid;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            color: #333;
            position: relative;
            background: linear-gradient(135deg, rgba(255,255,255,0.9), rgba(248,249,250,0.9));
        }

        .matrix-rect.small { width: 60px; height: 80px; font-size: 11px; }
        .matrix-rect.medium { width: 80px; height: 100px; font-size: 12px; }
        .matrix-rect.large { width: 100px; height: 120px; font-size: 13px; }
        .matrix-rect.wide { width: 140px; height: 80px; font-size: 12px; }
        .matrix-rect.tall { width: 60px; height: 120px; font-size: 11px; }
        .matrix-rect.square { width: 90px; height: 90px; font-size: 12px; }

        .matrix-rect.query { border-color: #FF6B6B; background: linear-gradient(135deg, rgba(255,107,107,0.1), rgba(255,107,107,0.2)); }
        .matrix-rect.key { border-color: #4ECDC4; background: linear-gradient(135deg, rgba(78,205,196,0.1), rgba(78,205,196,0.2)); }
        .matrix-rect.value { border-color: #95E1D3; background: linear-gradient(135deg, rgba(149,225,211,0.1), rgba(149,225,211,0.2)); }
        .matrix-rect.scores { border-color: #ffc107; background: linear-gradient(135deg, rgba(255,193,7,0.1), rgba(255,193,7,0.2)); }
        .matrix-rect.attention { border-color: #667eea; background: linear-gradient(135deg, rgba(102,126,234,0.1), rgba(102,126,234,0.2)); }
        .matrix-rect.embedding { border-color: #764ba2; background: linear-gradient(135deg, rgba(118,75,162,0.1), rgba(118,75,162,0.2)); }
        .matrix-rect.output { border-color: #84fab0; background: linear-gradient(135deg, rgba(132,250,176,0.1), rgba(132,250,176,0.2)); }

        .matrix-dim-label {
            position: absolute;
            font-size: 10px;
            font-weight: 600;
            color: #666;
        }

        .matrix-dim-label.top { top: -18px; left: 50%; transform: translateX(-50%); }
        .matrix-dim-label.left { left: -25px; top: 50%; transform: translateY(-50%) rotate(-90deg); }
        .matrix-dim-label.bottom { bottom: -18px; left: 50%; transform: translateX(-50%); }
        .matrix-dim-label.right { right: -25px; top: 50%; transform: translateY(-50%) rotate(90deg); }

        .matrix-name {
            margin-top: 10px;
            font-weight: 700;
            font-size: 14px;
            color: #495057;
        }

        .matrix-dims {
            font-size: 11px;
            color: #6c757d;
            margin-top: 4px;
        }

        .op-symbol {
            font-size: 32px;
            font-weight: 700;
            color: #667eea;
        }

        .equals-arrow {
            display: flex;
            flex-direction: column;
            align-items: center;
            color: #667eea;
        }

        .equals-arrow .arrow { font-size: 28px; }
        .equals-arrow .text { font-size: 11px; font-weight: 600; }

        /* Step summary card */
        .step-summary {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-left: 5px solid #667eea;
            padding: 20px;
            border-radius: 0 12px 12px 0;
            margin: 25px 0;
        }

        .step-summary h4 {
            color: #667eea;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .step-summary ul {
            margin: 0;
            padding-left: 20px;
            line-height: 1.8;
        }

        .step-summary li {
            color: #495057;
        }

        /* Enhanced computation visual */
        .computation-flow {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 20px;
            padding: 30px;
            background: linear-gradient(135deg, #f8f9fa, #fff);
            border-radius: 12px;
            margin: 20px 0;
            overflow-x: auto;
            flex-wrap: wrap;
        }

        .flow-item {
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
        }

        .flow-matrix {
            width: 70px;
            height: 90px;
            border: 3px solid #667eea;
            border-radius: 8px;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            background: white;
            font-size: 11px;
            font-weight: 600;
        }

        .flow-matrix.highlighted {
            box-shadow: 0 0 20px rgba(102, 126, 234, 0.4);
            transform: scale(1.05);
        }

        .flow-matrix .name { color: #667eea; margin-bottom: 5px; }
        .flow-matrix .dims { color: #6c757d; font-size: 10px; }

        .flow-op {
            font-size: 24px;
            color: #667eea;
            font-weight: 700;
        }

        /* Score grid visualization */
        .score-grid-viz {
            display: inline-grid;
            gap: 3px;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
            margin: 15px auto;
        }

        .score-cell {
            width: 50px;
            height: 40px;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            border-radius: 4px;
            font-size: 10px;
            font-weight: 600;
            transition: all 0.3s;
        }

        .score-cell .token-name {
            font-size: 8px;
            opacity: 0.7;
            margin-bottom: 2px;
        }

        /* Interactive highlight */
        .highlight-row {
            animation: highlightPulse 2s ease-in-out infinite;
        }

        @keyframes highlightPulse {
            0%, 100% { box-shadow: 0 0 5px rgba(102, 126, 234, 0.3); }
            50% { box-shadow: 0 0 20px rgba(102, 126, 234, 0.6); }
        }

        /* What's happening box */
        .whats-happening {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 12px;
            margin: 25px 0;
        }

        .whats-happening h4 {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-bottom: 12px;
            font-size: 16px;
        }

        .whats-happening p {
            opacity: 0.95;
            line-height: 1.7;
            margin: 0;
        }

        /* Transformer Architecture Visualization Styles */
        .transformer-architecture {
            background: linear-gradient(135deg, #0f0f1a 0%, #1a1a2e 50%, #16213e 100%);
            border-radius: 16px;
            padding: 30px;
            margin: 20px 0;
            position: relative;
            overflow: hidden;
        }

        .transformer-architecture::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: 
                radial-gradient(circle at 20% 50%, rgba(102, 126, 234, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 80% 50%, rgba(118, 75, 162, 0.1) 0%, transparent 50%);
            pointer-events: none;
        }

        .arch-title {
            color: #00d4ff;
            font-size: 1.4em;
            font-weight: 700;
            text-align: center;
            margin-bottom: 25px;
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        .transformer-stack {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
            position: relative;
        }

        .transformer-layer {
            width: 100%;
            max-width: 800px;
            position: relative;
        }

        .layer-wrapper {
            background: rgba(255, 255, 255, 0.03);
            border: 2px solid rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 20px;
            margin: 8px 0;
            position: relative;
            transition: all 0.3s ease;
        }

        .layer-wrapper:hover {
            border-color: rgba(102, 126, 234, 0.5);
            background: rgba(255, 255, 255, 0.05);
        }

        .layer-wrapper.highlighted {
            border-color: #00d4ff;
            box-shadow: 0 0 30px rgba(0, 212, 255, 0.3), inset 0 0 30px rgba(0, 212, 255, 0.05);
        }

        .layer-label {
            position: absolute;
            top: -12px;
            left: 20px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 4px 14px;
            border-radius: 20px;
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 1px;
            text-transform: uppercase;
        }

        .layer-content {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
        }

        .arch-component {
            background: linear-gradient(135deg, rgba(255,255,255,0.1), rgba(255,255,255,0.05));
            border: 2px solid rgba(255, 255, 255, 0.2);
            border-radius: 10px;
            padding: 15px 20px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            min-width: 120px;
        }

        .arch-component:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.3);
        }

        .arch-component.mha {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.3), rgba(118, 75, 162, 0.3));
            border-color: #667eea;
        }

        .arch-component.mha:hover, .arch-component.mha.active {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.5), rgba(118, 75, 162, 0.5));
            box-shadow: 0 0 30px rgba(102, 126, 234, 0.4);
        }

        .arch-component.ffn {
            background: linear-gradient(135deg, rgba(78, 205, 196, 0.2), rgba(68, 160, 141, 0.2));
            border-color: #4ECDC4;
        }

        .arch-component.norm {
            background: linear-gradient(135deg, rgba(255, 193, 7, 0.2), rgba(255, 152, 0, 0.2));
            border-color: #ffc107;
            min-width: 80px;
            padding: 8px 15px;
        }

        .arch-component.residual {
            background: transparent;
            border: 2px dashed rgba(132, 250, 176, 0.5);
            min-width: auto;
            padding: 8px;
        }

        .component-icon {
            font-size: 24px;
            margin-bottom: 8px;
        }

        .component-name {
            color: #fff;
            font-weight: 600;
            font-size: 13px;
            margin-bottom: 4px;
        }

        .component-detail {
            color: rgba(255, 255, 255, 0.6);
            font-size: 10px;
        }

        .arch-arrow {
            color: rgba(255, 255, 255, 0.4);
            font-size: 24px;
            font-weight: 300;
        }

        .arch-arrow.vertical {
            display: block;
            text-align: center;
            margin: 5px 0;
        }

        /* Multi-Head Detail View */
        .mha-detail-container {
            margin-top: 25px;
            padding-top: 25px;
            border-top: 1px solid rgba(255, 255, 255, 0.1);
        }

        .mha-detail-title {
            color: #00d4ff;
            font-size: 1.1em;
            font-weight: 600;
            text-align: center;
            margin-bottom: 20px;
        }

        .heads-parallel-view {
            display: flex;
            justify-content: center;
            gap: 12px;
            flex-wrap: wrap;
            margin: 20px 0;
        }

        .head-unit {
            background: linear-gradient(180deg, rgba(102, 126, 234, 0.2) 0%, rgba(118, 75, 162, 0.2) 100%);
            border: 2px solid rgba(102, 126, 234, 0.4);
            border-radius: 10px;
            padding: 15px;
            min-width: 100px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
        }

        .head-unit:hover, .head-unit.active {
            border-color: #00d4ff;
            background: linear-gradient(180deg, rgba(102, 126, 234, 0.4) 0%, rgba(118, 75, 162, 0.4) 100%);
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 212, 255, 0.2);
        }

        .head-unit::after {
            content: '';
            position: absolute;
            bottom: -15px;
            left: 50%;
            width: 2px;
            height: 15px;
            background: linear-gradient(to bottom, rgba(102, 126, 234, 0.5), transparent);
        }

        .head-number {
            color: #00d4ff;
            font-size: 12px;
            font-weight: 700;
            margin-bottom: 8px;
        }

        .head-qkv {
            display: flex;
            gap: 4px;
            justify-content: center;
            margin-bottom: 8px;
        }

        .head-qkv-item {
            width: 22px;
            height: 22px;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 10px;
            font-weight: 700;
            color: white;
        }

        .head-qkv-item.q { background: #FF6B6B; }
        .head-qkv-item.k { background: #4ECDC4; }
        .head-qkv-item.v { background: #95E1D3; }

        .head-dim {
            color: rgba(255, 255, 255, 0.5);
            font-size: 9px;
        }

        .concat-bar {
            background: linear-gradient(90deg, 
                #FF6B6B 0%, #FF6B6B 20%, 
                #4ECDC4 20%, #4ECDC4 40%, 
                #95E1D3 40%, #95E1D3 60%,
                #F7DC6F 60%, #F7DC6F 80%,
                #DDA0DD 80%, #DDA0DD 100%);
            height: 20px;
            border-radius: 10px;
            max-width: 500px;
            margin: 20px auto;
            position: relative;
            overflow: hidden;
        }

        .concat-label {
            color: rgba(255, 255, 255, 0.7);
            font-size: 11px;
            text-align: center;
            margin-top: 8px;
        }

        /* Data Flow Visualization */
        .data-flow-viz {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            padding: 20px;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
        }

        .flow-node {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 8px;
        }

        .flow-node-box {
            background: rgba(255, 255, 255, 0.1);
            border: 2px solid rgba(255, 255, 255, 0.3);
            border-radius: 8px;
            padding: 12px 18px;
            color: white;
            font-weight: 600;
            font-size: 12px;
            white-space: nowrap;
        }

        .flow-node-box.input { border-color: #a8edea; color: #a8edea; }
        .flow-node-box.qkv-proj { border-color: #667eea; color: #667eea; }
        .flow-node-box.attention { border-color: #ffc107; color: #ffc107; }
        .flow-node-box.output { border-color: #84fab0; color: #84fab0; }

        .flow-connector {
            color: rgba(255, 255, 255, 0.4);
            font-size: 18px;
        }

        .flow-dim {
            color: rgba(255, 255, 255, 0.5);
            font-size: 10px;
        }

        /* Architecture Info Panel */
        .arch-info-panel {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 10px;
            padding: 20px;
            margin-top: 20px;
            border-left: 4px solid #00d4ff;
        }

        .arch-info-panel h4 {
            color: #00d4ff;
            margin-bottom: 12px;
            font-size: 14px;
        }

        .arch-info-panel p {
            color: rgba(255, 255, 255, 0.8);
            font-size: 13px;
            line-height: 1.7;
            margin: 0;
        }

        .arch-stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 12px;
            margin-top: 20px;
        }

        .arch-stat {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 8px;
            padding: 12px;
            text-align: center;
        }

        .arch-stat-value {
            color: #00d4ff;
            font-size: 20px;
            font-weight: 700;
        }

        .arch-stat-label {
            color: rgba(255, 255, 255, 0.6);
            font-size: 11px;
            margin-top: 4px;
        }

        /* Layer Nx indicator */
        .layer-repeat-indicator {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            color: rgba(255, 255, 255, 0.5);
            font-size: 12px;
            margin: 15px 0;
            padding: 10px;
            border: 1px dashed rgba(255, 255, 255, 0.2);
            border-radius: 8px;
        }

        .layer-repeat-indicator strong {
            color: #00d4ff;
        }

        /* Connection lines animation */
        .connection-pulse {
            animation: pulseFlow 2s ease-in-out infinite;
        }

        @keyframes pulseFlow {
            0%, 100% { opacity: 0.4; }
            50% { opacity: 1; }
        }

        /* Encoder-Decoder View Toggle */
        .arch-view-toggle {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-bottom: 20px;
        }

        .arch-toggle-btn {
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            color: rgba(255, 255, 255, 0.6);
            padding: 8px 20px;
            border-radius: 20px;
            cursor: pointer;
            font-size: 12px;
            font-weight: 600;
            transition: all 0.3s;
        }

        .arch-toggle-btn:hover {
            background: rgba(255, 255, 255, 0.15);
            color: white;
        }

        .arch-toggle-btn.active {
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-color: #667eea;
            color: white;
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .heads-parallel-view {
                gap: 8px;
            }
            .head-unit {
                min-width: 80px;
                padding: 10px;
            }
            .layer-content {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Multi-Head Self-Attention Visualizer</h1>
            <p class="subtitle">Understanding Transformer Attention from First Principles</p>
        </header>

        <div class="controls">
            <div class="control-group">
                <label for="inputText">Input Sequence (space-separated tokens):</label>
                <input type="text" id="inputText" value="The cat sat on the mat" placeholder="Enter your text...">
            </div>

            <div class="control-row">
                <div class="control-group">
                    <label for="embedDim">Embedding Dimension:</label>
                    <input type="number" id="embedDim" value="64" min="8" max="512" step="8">
                </div>

                <div class="control-group">
                    <label for="numHeads">Number of Heads:</label>
                    <input type="number" id="numHeads" value="4" min="1" max="8">
                </div>

                <div class="control-group">
                    <label for="temperature">Temperature (for attention):</label>
                    <input type="number" id="temperature" value="1.0" min="0.1" max="2.0" step="0.1">
                </div>
            </div>

            <div class="control-group" style="margin-top: 15px;">
                <button onclick="runAttention()">Compute Attention</button>
            </div>
        </div>

        <div class="content">
            <div class="section">
                <div class="section-title">Input Tokens</div>
                <div id="tokenDisplay" class="token-display"></div>
                <div class="info-box">
                    <strong>What's happening:</strong> Each token is converted to a dense vector representation (embedding) of dimension d_model.
                </div>
            </div>

            <div class="section">
                <div class="section-title">Architecture Overview</div>
                <div id="architectureInfo"></div>
            </div>

            <div class="section">
                <div class="section-title">Transformer Architecture: Where Multi-Head Attention Lives</div>
                <div class="transformer-architecture">
                    <div class="arch-title">ðŸ”® Transformer Encoder Block</div>
                    
                    <div class="arch-view-toggle">
                        <button class="arch-toggle-btn active" onclick="setArchView('encoder')">Encoder</button>
                        <button class="arch-toggle-btn" onclick="setArchView('decoder')">Decoder</button>
                        <button class="arch-toggle-btn" onclick="setArchView('full')">Full Model</button>
                    </div>

                    <div id="architectureView" class="transformer-stack">
                        <!-- Dynamic architecture visualization will be inserted here -->
                    </div>

                    <div class="mha-detail-container" id="mhaDetailView">
                        <div class="mha-detail-title">ðŸŽ¯ Multi-Head Attention: Parallel Processing</div>
                        
                        <div class="data-flow-viz">
                            <div class="flow-node">
                                <div class="flow-node-box input">Input X</div>
                                <div class="flow-dim" id="flowInputDim">[n Ã— d_model]</div>
                            </div>
                            <div class="flow-connector connection-pulse">â†’</div>
                            <div class="flow-node">
                                <div class="flow-node-box qkv-proj">Linear Projections</div>
                                <div class="flow-dim">W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup></div>
                            </div>
                            <div class="flow-connector connection-pulse">â†’</div>
                            <div class="flow-node">
                                <div class="flow-node-box attention">Parallel Heads</div>
                                <div class="flow-dim" id="flowHeadsDim">[h Ã— n Ã— d_k]</div>
                            </div>
                            <div class="flow-connector connection-pulse">â†’</div>
                            <div class="flow-node">
                                <div class="flow-node-box output">Concat + W<sup>O</sup></div>
                                <div class="flow-dim" id="flowOutputDim">[n Ã— d_model]</div>
                            </div>
                        </div>

                        <div id="headsParallelView" class="heads-parallel-view">
                            <!-- Dynamic heads will be inserted here -->
                        </div>

                        <div id="concatViz" class="concat-bar"></div>
                        <div class="concat-label">Concatenated Head Outputs â†’ Output Projection W<sup>O</sup></div>

                        <div class="arch-stats" id="archStats">
                            <!-- Dynamic stats will be inserted here -->
                        </div>
                    </div>

                    <div class="arch-info-panel">
                        <h4>ðŸ’¡ How It Connects to the Visualization Above</h4>
                        <p id="archConnectionInfo">
                            Click on any attention head above to see its specific attention pattern in the "Individual Heads" tab. 
                            Each head learns different patterns - some focus on local context (adjacent words), 
                            others capture long-range dependencies or syntactic structures.
                        </p>
                    </div>
                </div>
            </div>

            <div class="tabs">
                <button class="tab active" onclick="switchTab('overview')">Overview</button>
                <button class="tab" onclick="switchTab('sequence')">Step-by-Step Sequence</button>
                <button class="tab" onclick="switchTab('heads')">Individual Heads</button>
                <button class="tab" onclick="switchTab('mathematics')">Mathematics</button>
            </div>

            <div id="overview" class="tab-content active">
                <div class="section">
                    <div class="section-title">Combined Attention Pattern</div>
                    <div class="viz-card">
                        <h3>Average Attention Weights Across All Heads</h3>
                        <canvas id="combinedAttentionCanvas"></canvas>
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> This shows the average attention pattern across all heads. Each cell (i,j) shows how much token i attends to token j.
                    </div>
                </div>
            </div>

            <div id="sequence" class="tab-content">
                <div class="section">
                    <div class="section-title">Step-by-Step Computation Sequence</div>
                    
                    <div class="sequence-controls">
                        <button class="seq-btn" onclick="previousStep()">â—€ Previous</button>
                        <span id="stepIndicator" class="step-indicator">Step 1 of 9</span>
                        <button class="seq-btn" onclick="nextStep()">Next â–¶</button>
                        <button class="seq-btn-secondary" onclick="resetSequence()">Reset</button>
                    </div>

                    <div class="step-progress">
                        <div id="progressBar" class="progress-bar"></div>
                    </div>

                    <div id="sequenceVisualization" class="sequence-visualization">
                        <!-- Dynamic content will be inserted here -->
                    </div>

                    <div class="step-navigator">
                        <div class="nav-grid" id="stepNavGrid">
                            <!-- Step navigation buttons will be inserted here -->
                        </div>
                    </div>
                </div>
            </div>

            <div id="heads" class="tab-content">
                <div class="section">
                    <div class="section-title">Individual Head Attention Patterns</div>
                    <div id="headsContainer"></div>
                    <div class="info-box">
                        <strong>What's happening:</strong> Each head learns to attend to different aspects of the input. Some might focus on local patterns, others on long-range dependencies.
                    </div>
                </div>
            </div>

            <div id="mathematics" class="tab-content">
                <div class="section">
                    <div class="section-title">Step-by-Step Mathematical Breakdown</div>
                    <div id="mathBreakdown"></div>
                </div>
            </div>

            <div class="section">
                <div class="section-title">Deep Dive: Q, K, V Transformation</div>
                <div class="info-box">
                    <strong>Conceptual Understanding:</strong><br>
                    â€¢ <strong>Query (Q):</strong> "What am I looking for?" - Represents what each token wants to know<br>
                    â€¢ <strong>Key (K):</strong> "What do I offer?" - Represents what information each token provides<br>
                    â€¢ <strong>Value (V):</strong> "Here's my actual content" - The actual information to be aggregated<br><br>
                    The attention mechanism is like a database lookup: Q asks questions, K provides indices, and V contains the actual data.
                </div>
                <div id="qkvVisualization"></div>
            </div>

            <div class="section">
                <div class="section-title">Attention Mechanism Flow</div>
                <div id="attentionFlow"></div>
            </div>
        </div>
    </div>

    <script>
        let currentAttention = null;
        let currentStep = 0;
        let totalSteps = 9;
        let sequenceData = null;
        let currentArchView = 'encoder';
        let selectedHead = null;

        // ============== Transformer Architecture Visualization ==============

        function setArchView(view) {
            currentArchView = view;
            document.querySelectorAll('.arch-toggle-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.textContent.toLowerCase().includes(view)) {
                    btn.classList.add('active');
                }
            });
            renderArchitectureView();
        }

        function renderArchitectureView() {
            const container = document.getElementById('architectureView');
            const embedDim = parseInt(document.getElementById('embedDim').value) || 64;
            const numHeads = parseInt(document.getElementById('numHeads').value) || 4;
            const headDim = Math.floor(embedDim / numHeads);

            let html = '';

            if (currentArchView === 'encoder') {
                html = generateEncoderView(embedDim, numHeads, headDim);
            } else if (currentArchView === 'decoder') {
                html = generateDecoderView(embedDim, numHeads, headDim);
            } else {
                html = generateFullModelView(embedDim, numHeads, headDim);
            }

            container.innerHTML = html;
            updateMHADetailView(numHeads, embedDim, headDim);
        }

        function generateEncoderView(embedDim, numHeads, headDim) {
            return `
                <!-- Input -->
                <div class="layer-wrapper" style="background: rgba(168, 237, 234, 0.1); border-color: rgba(168, 237, 234, 0.3);">
                    <span class="layer-label" style="background: linear-gradient(135deg, #a8edea, #8fd3f4);">Input</span>
                    <div class="layer-content">
                        <div class="arch-component" style="border-color: #a8edea;">
                            <div class="component-icon">ðŸ“</div>
                            <div class="component-name">Token Embeddings</div>
                            <div class="component-detail">[seq_len Ã— ${embedDim}]</div>
                        </div>
                        <div class="arch-arrow">+</div>
                        <div class="arch-component" style="border-color: #a8edea;">
                            <div class="component-icon">ðŸ“</div>
                            <div class="component-name">Positional Encoding</div>
                            <div class="component-detail">[seq_len Ã— ${embedDim}]</div>
                        </div>
                    </div>
                </div>

                <div class="arch-arrow vertical">â†“</div>

                <!-- Encoder Block -->
                <div class="transformer-layer">
                    <div class="layer-wrapper highlighted">
                        <span class="layer-label">Encoder Block (NÃ—)</span>
                        
                        <div style="display: flex; align-items: stretch; gap: 15px; flex-wrap: wrap; justify-content: center;">
                            <!-- Residual Connection 1 -->
                            <div style="display: flex; flex-direction: column; align-items: center; flex: 1; min-width: 300px;">
                                <div style="color: rgba(255,255,255,0.5); font-size: 11px; margin-bottom: 10px;">Sublayer 1: Self-Attention</div>
                                
                                <div style="display: flex; align-items: center; gap: 10px; width: 100%; justify-content: center;">
                                    <div class="arch-component residual" style="padding: 4px 8px;">
                                        <div class="component-detail" style="color: #84fab0;">+ Residual</div>
                                    </div>
                                    <div style="display: flex; flex-direction: column; gap: 8px; align-items: center;">
                                        <div class="arch-component norm" onclick="highlightComponent('norm1')">
                                            <div class="component-name">LayerNorm</div>
                                        </div>
                                        <div class="arch-arrow">â†“</div>
                                        <div class="arch-component mha active" onclick="highlightMHA(); switchTab('heads');">
                                            <div class="component-icon">ðŸ§ </div>
                                            <div class="component-name">Multi-Head<br>Self-Attention</div>
                                            <div class="component-detail">${numHeads} heads Ã— ${headDim}d</div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="arch-arrow" style="align-self: center;">â†’</div>

                            <!-- Residual Connection 2 -->
                            <div style="display: flex; flex-direction: column; align-items: center; flex: 1; min-width: 300px;">
                                <div style="color: rgba(255,255,255,0.5); font-size: 11px; margin-bottom: 10px;">Sublayer 2: Feed-Forward</div>
                                
                                <div style="display: flex; align-items: center; gap: 10px; width: 100%; justify-content: center;">
                                    <div class="arch-component residual" style="padding: 4px 8px;">
                                        <div class="component-detail" style="color: #84fab0;">+ Residual</div>
                                    </div>
                                    <div style="display: flex; flex-direction: column; gap: 8px; align-items: center;">
                                        <div class="arch-component norm" onclick="highlightComponent('norm2')">
                                            <div class="component-name">LayerNorm</div>
                                        </div>
                                        <div class="arch-arrow">â†“</div>
                                        <div class="arch-component ffn" onclick="highlightComponent('ffn')">
                                            <div class="component-icon">âš¡</div>
                                            <div class="component-name">Feed-Forward<br>Network</div>
                                            <div class="component-detail">${embedDim} â†’ ${embedDim * 4} â†’ ${embedDim}</div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="layer-repeat-indicator">
                        <span>ðŸ”„</span>
                        <span>This block repeats <strong>N times</strong> (typically 6-12 layers in BERT/GPT)</span>
                    </div>
                </div>

                <div class="arch-arrow vertical">â†“</div>

                <!-- Output -->
                <div class="layer-wrapper" style="background: rgba(132, 250, 176, 0.1); border-color: rgba(132, 250, 176, 0.3);">
                    <span class="layer-label" style="background: linear-gradient(135deg, #84fab0, #8fd3f4);">Output</span>
                    <div class="layer-content">
                        <div class="arch-component" style="border-color: #84fab0;">
                            <div class="component-icon">âœ¨</div>
                            <div class="component-name">Contextualized Embeddings</div>
                            <div class="component-detail">[seq_len Ã— ${embedDim}]</div>
                        </div>
                    </div>
                </div>
            `;
        }

        function generateDecoderView(embedDim, numHeads, headDim) {
            return `
                <!-- Input -->
                <div class="layer-wrapper" style="background: rgba(168, 237, 234, 0.1); border-color: rgba(168, 237, 234, 0.3);">
                    <span class="layer-label" style="background: linear-gradient(135deg, #a8edea, #8fd3f4);">Input (Shifted Right)</span>
                    <div class="layer-content">
                        <div class="arch-component" style="border-color: #a8edea;">
                            <div class="component-icon">ðŸ“</div>
                            <div class="component-name">Output Embeddings</div>
                            <div class="component-detail">[seq_len Ã— ${embedDim}]</div>
                        </div>
                        <div class="arch-arrow">+</div>
                        <div class="arch-component" style="border-color: #a8edea;">
                            <div class="component-icon">ðŸ“</div>
                            <div class="component-name">Positional Encoding</div>
                            <div class="component-detail">[seq_len Ã— ${embedDim}]</div>
                        </div>
                    </div>
                </div>

                <div class="arch-arrow vertical">â†“</div>

                <!-- Decoder Block -->
                <div class="transformer-layer">
                    <div class="layer-wrapper highlighted">
                        <span class="layer-label">Decoder Block (NÃ—)</span>
                        
                        <div style="display: flex; flex-direction: column; gap: 20px;">
                            <!-- Masked Self-Attention -->
                            <div style="display: flex; align-items: center; gap: 15px; justify-content: center; flex-wrap: wrap;">
                                <div class="arch-component residual" style="padding: 4px 8px;">
                                    <div class="component-detail" style="color: #84fab0;">+ Residual</div>
                                </div>
                                <div class="arch-component norm">
                                    <div class="component-name">LayerNorm</div>
                                </div>
                                <div class="arch-arrow">â†’</div>
                                <div class="arch-component mha" style="background: linear-gradient(135deg, rgba(255,107,107,0.3), rgba(118,75,162,0.3)); border-color: #FF6B6B;">
                                    <div class="component-icon">ðŸŽ­</div>
                                    <div class="component-name">Masked Multi-Head<br>Self-Attention</div>
                                    <div class="component-detail">${numHeads} heads (causal mask)</div>
                                </div>
                            </div>

                            <!-- Cross-Attention -->
                            <div style="display: flex; align-items: center; gap: 15px; justify-content: center; flex-wrap: wrap;">
                                <div class="arch-component residual" style="padding: 4px 8px;">
                                    <div class="component-detail" style="color: #84fab0;">+ Residual</div>
                                </div>
                                <div class="arch-component norm">
                                    <div class="component-name">LayerNorm</div>
                                </div>
                                <div class="arch-arrow">â†’</div>
                                <div class="arch-component mha active" onclick="highlightMHA();">
                                    <div class="component-icon">ðŸ”—</div>
                                    <div class="component-name">Cross-Attention</div>
                                    <div class="component-detail">Q from decoder, K,V from encoder</div>
                                </div>
                            </div>

                            <!-- FFN -->
                            <div style="display: flex; align-items: center; gap: 15px; justify-content: center; flex-wrap: wrap;">
                                <div class="arch-component residual" style="padding: 4px 8px;">
                                    <div class="component-detail" style="color: #84fab0;">+ Residual</div>
                                </div>
                                <div class="arch-component norm">
                                    <div class="component-name">LayerNorm</div>
                                </div>
                                <div class="arch-arrow">â†’</div>
                                <div class="arch-component ffn">
                                    <div class="component-icon">âš¡</div>
                                    <div class="component-name">Feed-Forward Network</div>
                                    <div class="component-detail">${embedDim} â†’ ${embedDim * 4} â†’ ${embedDim}</div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="layer-repeat-indicator">
                        <span>ðŸ”„</span>
                        <span>This block repeats <strong>N times</strong></span>
                    </div>
                </div>

                <div class="arch-arrow vertical">â†“</div>

                <!-- Output -->
                <div class="layer-wrapper" style="background: rgba(132, 250, 176, 0.1); border-color: rgba(132, 250, 176, 0.3);">
                    <span class="layer-label" style="background: linear-gradient(135deg, #84fab0, #8fd3f4);">Output</span>
                    <div class="layer-content">
                        <div class="arch-component" style="border-color: #84fab0;">
                            <div class="component-icon">ðŸ“Š</div>
                            <div class="component-name">Linear + Softmax</div>
                            <div class="component-detail">Vocabulary probabilities</div>
                        </div>
                    </div>
                </div>
            `;
        }

        function generateFullModelView(embedDim, numHeads, headDim) {
            return `
                <div style="display: grid; grid-template-columns: 1fr 80px 1fr; gap: 20px; width: 100%;">
                    <!-- Encoder Side -->
                    <div>
                        <div style="text-align: center; color: #00d4ff; font-weight: 700; margin-bottom: 15px; font-size: 14px;">ENCODER</div>
                        
                        <div class="layer-wrapper" style="background: rgba(168, 237, 234, 0.05);">
                            <span class="layer-label" style="font-size: 9px;">Input</span>
                            <div class="layer-content" style="flex-direction: column; gap: 5px;">
                                <div class="arch-component" style="min-width: auto; padding: 8px 12px;">
                                    <div class="component-name" style="font-size: 11px;">Embeddings + Pos</div>
                                </div>
                            </div>
                        </div>

                        <div class="arch-arrow vertical" style="margin: 8px 0;">â†“</div>

                        <div class="layer-wrapper highlighted" style="padding: 12px;">
                            <span class="layer-label" style="font-size: 9px;">Encoder Ã—N</span>
                            <div style="display: flex; flex-direction: column; gap: 8px; align-items: center;">
                                <div class="arch-component mha active" style="min-width: auto; padding: 10px;" onclick="highlightMHA();">
                                    <div class="component-name" style="font-size: 10px;">Self-Attention</div>
                                    <div class="component-detail" style="font-size: 8px;">${numHeads} heads</div>
                                </div>
                                <div class="arch-component ffn" style="min-width: auto; padding: 10px;">
                                    <div class="component-name" style="font-size: 10px;">FFN</div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Connection Arrow -->
                    <div style="display: flex; align-items: center; justify-content: center;">
                        <div style="display: flex; flex-direction: column; align-items: center; gap: 5px;">
                            <div style="color: rgba(255,255,255,0.5); font-size: 10px;">K, V</div>
                            <div style="font-size: 24px; color: #ffc107;">â†’</div>
                        </div>
                    </div>

                    <!-- Decoder Side -->
                    <div>
                        <div style="text-align: center; color: #00d4ff; font-weight: 700; margin-bottom: 15px; font-size: 14px;">DECODER</div>
                        
                        <div class="layer-wrapper" style="background: rgba(168, 237, 234, 0.05);">
                            <span class="layer-label" style="font-size: 9px;">Output (shifted)</span>
                            <div class="layer-content" style="flex-direction: column; gap: 5px;">
                                <div class="arch-component" style="min-width: auto; padding: 8px 12px;">
                                    <div class="component-name" style="font-size: 11px;">Embeddings + Pos</div>
                                </div>
                            </div>
                        </div>

                        <div class="arch-arrow vertical" style="margin: 8px 0;">â†“</div>

                        <div class="layer-wrapper" style="padding: 12px;">
                            <span class="layer-label" style="font-size: 9px;">Decoder Ã—N</span>
                            <div style="display: flex; flex-direction: column; gap: 8px; align-items: center;">
                                <div class="arch-component mha" style="min-width: auto; padding: 10px; border-color: #FF6B6B;">
                                    <div class="component-name" style="font-size: 10px;">Masked Self-Attn</div>
                                </div>
                                <div class="arch-component mha" style="min-width: auto; padding: 10px;">
                                    <div class="component-name" style="font-size: 10px;">Cross-Attention</div>
                                </div>
                                <div class="arch-component ffn" style="min-width: auto; padding: 10px;">
                                    <div class="component-name" style="font-size: 10px;">FFN</div>
                                </div>
                            </div>
                        </div>

                        <div class="arch-arrow vertical" style="margin: 8px 0;">â†“</div>

                        <div class="layer-wrapper" style="background: rgba(132, 250, 176, 0.05);">
                            <span class="layer-label" style="font-size: 9px;">Output</span>
                            <div class="layer-content">
                                <div class="arch-component" style="min-width: auto; padding: 8px 12px; border-color: #84fab0;">
                                    <div class="component-name" style="font-size: 11px;">Softmax</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="layer-repeat-indicator" style="margin-top: 20px;">
                    <span>ðŸ“–</span>
                    <span>Original Transformer: Encoder maps input â†’ representations, Decoder generates output autoregressively</span>
                </div>
            `;
        }

        function updateMHADetailView(numHeads, embedDim, headDim) {
            const tokens = document.getElementById('inputText').value.trim().split(/\s+/);
            const seqLen = tokens.length;

            // Update flow dimensions
            document.getElementById('flowInputDim').textContent = `[${seqLen} Ã— ${embedDim}]`;
            document.getElementById('flowHeadsDim').textContent = `[${numHeads} Ã— ${seqLen} Ã— ${headDim}]`;
            document.getElementById('flowOutputDim').textContent = `[${seqLen} Ã— ${embedDim}]`;

            // Render parallel heads
            const headColors = ['#FF6B6B', '#4ECDC4', '#95E1D3', '#F7DC6F', '#DDA0DD', '#87CEEB', '#FFA07A', '#98D8C8'];
            const headsContainer = document.getElementById('headsParallelView');
            
            let headsHtml = '';
            for (let h = 0; h < numHeads; h++) {
                headsHtml += `
                    <div class="head-unit ${selectedHead === h ? 'active' : ''}" 
                         onclick="selectHead(${h})"
                         style="border-color: ${headColors[h % headColors.length]}40;">
                        <div class="head-number" style="color: ${headColors[h % headColors.length]};">Head ${h + 1}</div>
                        <div class="head-qkv">
                            <div class="head-qkv-item q">Q</div>
                            <div class="head-qkv-item k">K</div>
                            <div class="head-qkv-item v">V</div>
                        </div>
                        <div class="head-dim">${seqLen}Ã—${headDim}</div>
                    </div>
                `;
            }
            headsContainer.innerHTML = headsHtml;

            // Update concat bar gradient
            const concatBar = document.getElementById('concatViz');
            const gradientStops = headColors.slice(0, numHeads).map((color, i) => {
                const start = (i / numHeads * 100).toFixed(1);
                const end = ((i + 1) / numHeads * 100).toFixed(1);
                return `${color} ${start}%, ${color} ${end}%`;
            }).join(', ');
            concatBar.style.background = `linear-gradient(90deg, ${gradientStops})`;

            // Update stats
            const statsContainer = document.getElementById('archStats');
            statsContainer.innerHTML = `
                <div class="arch-stat">
                    <div class="arch-stat-value">${numHeads}</div>
                    <div class="arch-stat-label">Attention Heads</div>
                </div>
                <div class="arch-stat">
                    <div class="arch-stat-value">${headDim}</div>
                    <div class="arch-stat-label">Head Dimension (d_k)</div>
                </div>
                <div class="arch-stat">
                    <div class="arch-stat-value">${embedDim}</div>
                    <div class="arch-stat-label">Model Dimension</div>
                </div>
                <div class="arch-stat">
                    <div class="arch-stat-value">${seqLen}Ã—${seqLen}</div>
                    <div class="arch-stat-label">Attention Matrix</div>
                </div>
                <div class="arch-stat">
                    <div class="arch-stat-value">${(numHeads * headDim * headDim * 3 + numHeads * headDim * embedDim).toLocaleString()}</div>
                    <div class="arch-stat-label">MHA Parameters</div>
                </div>
            `;
        }

        function selectHead(headIndex) {
            selectedHead = headIndex;
            
            // Update visual selection
            document.querySelectorAll('.head-unit').forEach((el, idx) => {
                if (idx === headIndex) {
                    el.classList.add('active');
                } else {
                    el.classList.remove('active');
                }
            });

            // Switch to heads tab and scroll to the specific head
            switchTabDirect('heads');
            
            // Scroll to the specific head canvas after a short delay
            setTimeout(() => {
                const headCanvas = document.getElementById(`headCanvas${headIndex}`);
                if (headCanvas) {
                    headCanvas.scrollIntoView({ behavior: 'smooth', block: 'center' });
                    // Add a temporary highlight effect
                    const headContainer = headCanvas.closest('.head-container');
                    if (headContainer) {
                        headContainer.style.boxShadow = '0 0 30px rgba(0, 212, 255, 0.5)';
                        headContainer.style.borderColor = '#00d4ff';
                        setTimeout(() => {
                            headContainer.style.boxShadow = '';
                            headContainer.style.borderColor = '';
                        }, 2000);
                    }
                }
            }, 100);

            // Update info panel
            document.getElementById('archConnectionInfo').innerHTML = `
                <strong>Viewing Head ${headIndex + 1}:</strong> Scroll down to the "Individual Heads" section to see 
                this head's unique attention pattern. Notice how it might focus on different relationships than other heads - 
                this is the power of multi-head attention: each head can specialize in different linguistic patterns!
            `;
        }

        function highlightMHA() {
            // Visual feedback
            document.querySelectorAll('.arch-component.mha').forEach(el => {
                el.classList.add('active');
            });
            
            // Update info panel
            document.getElementById('archConnectionInfo').innerHTML = `
                <strong>Multi-Head Self-Attention</strong> is the heart of the Transformer! It allows each position 
                to attend to all positions in the previous layer. The "multi-head" aspect means we run attention 
                in parallel across ${document.getElementById('numHeads').value} different representation subspaces, 
                then concatenate and project the results. Explore the tabs below to see the full computation!
            `;
        }

        function highlightComponent(component) {
            // Could expand to show details about other components
            const descriptions = {
                'norm1': 'Layer Normalization stabilizes training by normalizing activations. Applied before attention (Pre-LN variant).',
                'norm2': 'Second Layer Normalization, applied before the feed-forward network.',
                'ffn': 'Feed-Forward Network: Two linear transformations with a ReLU/GELU activation in between. Expands then contracts dimensions.'
            };
            
            if (descriptions[component]) {
                document.getElementById('archConnectionInfo').innerHTML = `<strong>${component.toUpperCase()}:</strong> ${descriptions[component]}`;
            }
        }

        function switchTabDirect(tabName) {
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));

            document.querySelectorAll('.tab').forEach(t => {
                if (t.textContent.toLowerCase().includes(tabName)) {
                    t.classList.add('active');
                }
            });
            document.getElementById(tabName).classList.add('active');
        }

        function switchTab(tabName) {
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));

            event.target.classList.add('active');
            document.getElementById(tabName).classList.add('active');
        }

        function nextStep() {
            if (currentStep < totalSteps - 1) {
                currentStep++;
                renderStep();
            }
        }

        function previousStep() {
            if (currentStep > 0) {
                currentStep--;
                renderStep();
            }
        }

        function goToStep(step) {
            currentStep = step;
            renderStep();
        }

        function resetSequence() {
            currentStep = 0;
            renderStep();
        }

        function renderStep() {
            if (!sequenceData) return;

            const progress = ((currentStep + 1) / totalSteps) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
            document.getElementById('stepIndicator').textContent = `Step ${currentStep + 1} of ${totalSteps}`;

            // Update navigator
            document.querySelectorAll('.nav-step').forEach((el, idx) => {
                if (idx === currentStep) {
                    el.classList.add('active');
                } else {
                    el.classList.remove('active');
                }
            });

            // Render current step content
            const container = document.getElementById('sequenceVisualization');
            container.innerHTML = generateStepContent(currentStep, sequenceData);

            // Render step-specific canvases after DOM update
            setTimeout(() => {
                if (currentStep === 5) {
                    const canvas = document.getElementById('stepSoftmaxCanvas');
                    if (canvas) {
                        const head0 = sequenceData.result.headOutputs[0];
                        drawStepSoftmaxCanvas(canvas, sequenceData.tokens, sequenceData.focusToken, head0);
                    }
                }
            }, 10);
        }

        class MultiHeadSelfAttention {
            constructor(embedDim, numHeads, temperature = 1.0) {
                this.embedDim = embedDim;
                this.numHeads = numHeads;
                this.headDim = Math.floor(embedDim / numHeads);
                this.temperature = temperature;

                // Initialize weight matrices for each head
                this.initializeWeights();
            }

            initializeWeights() {
                this.heads = [];
                for (let h = 0; h < this.numHeads; h++) {
                    this.heads.push({
                        Wq: this.randomMatrix(this.embedDim, this.headDim),
                        Wk: this.randomMatrix(this.embedDim, this.headDim),
                        Wv: this.randomMatrix(this.embedDim, this.headDim)
                    });
                }
                // Output projection
                this.Wo = this.randomMatrix(this.numHeads * this.headDim, this.embedDim);
            }

            randomMatrix(rows, cols) {
                const matrix = [];
                const scale = Math.sqrt(2.0 / (rows + cols)); // Xavier initialization
                for (let i = 0; i < rows; i++) {
                    matrix[i] = [];
                    for (let j = 0; j < cols; j++) {
                        matrix[i][j] = (Math.random() - 0.5) * 2 * scale;
                    }
                }
                return matrix;
            }

            matmul(A, B) {
                const rowsA = A.length;
                const colsA = A[0].length;
                const colsB = B[0].length;
                const C = [];

                for (let i = 0; i < rowsA; i++) {
                    C[i] = [];
                    for (let j = 0; j < colsB; j++) {
                        let sum = 0;
                        for (let k = 0; k < colsA; k++) {
                            sum += A[i][k] * B[k][j];
                        }
                        C[i][j] = sum;
                    }
                }
                return C;
            }

            transpose(A) {
                const rows = A.length;
                const cols = A[0].length;
                const T = [];
                for (let j = 0; j < cols; j++) {
                    T[j] = [];
                    for (let i = 0; i < rows; i++) {
                        T[j][i] = A[i][j];
                    }
                }
                return T;
            }

            softmax(scores) {
                const result = [];
                for (let i = 0; i < scores.length; i++) {
                    const row = scores[i];
                    const maxScore = Math.max(...row);
                    const expScores = row.map(s => Math.exp((s - maxScore) / this.temperature));
                    const sumExp = expScores.reduce((a, b) => a + b, 0);
                    result[i] = expScores.map(e => e / sumExp);
                }
                return result;
            }

            scaledDotProductAttention(Q, K, V) {
                // Compute attention scores: Q @ K^T
                const KT = this.transpose(K);
                const scores = this.matmul(Q, KT);

                // Scale by sqrt(d_k)
                const scale = Math.sqrt(this.headDim);
                const scaledScores = scores.map(row =>
                    row.map(val => val / scale)
                );

                // Apply softmax
                const attentionWeights = this.softmax(scaledScores);

                // Apply attention to values: attention @ V
                const output = this.matmul(attentionWeights, V);

                return {
                    output,
                    attentionWeights,
                    scores: scaledScores
                };
            }

            forward(embeddings) {
                const seqLen = embeddings.length;
                const headOutputs = [];
                const headAttentions = [];

                // Process each head
                for (let h = 0; h < this.numHeads; h++) {
                    const head = this.heads[h];

                    // Project to Q, K, V
                    const Q = this.matmul(embeddings, head.Wq);
                    const K = this.matmul(embeddings, head.Wk);
                    const V = this.matmul(embeddings, head.Wv);

                    // Compute attention
                    const { output, attentionWeights, scores } =
                        this.scaledDotProductAttention(Q, K, V);

                    headOutputs.push({
                        Q, K, V, output, attentionWeights, scores
                    });
                    headAttentions.push(attentionWeights);
                }

                // Concatenate heads
                const concatenated = [];
                for (let i = 0; i < seqLen; i++) {
                    concatenated[i] = [];
                    for (let h = 0; h < this.numHeads; h++) {
                        concatenated[i].push(...headOutputs[h].output[i]);
                    }
                }

                // Final linear projection
                const finalOutput = this.matmul(concatenated, this.Wo);

                return {
                    output: finalOutput,
                    headOutputs,
                    headAttentions
                };
            }
        }

        function generateEmbeddings(tokens, embedDim) {
            const embeddings = [];
            for (let i = 0; i < tokens.length; i++) {
                const embedding = [];
                // Generate deterministic embeddings based on token hash
                const hash = tokens[i].split('').reduce((acc, char) =>
                    acc + char.charCodeAt(0), 0);
                for (let j = 0; j < embedDim; j++) {
                    const angle = (hash * 0.1 + j * 0.5) % (2 * Math.PI);
                    embedding[j] = Math.sin(angle) * 0.5;
                }
                embeddings.push(embedding);
            }
            return embeddings;
        }

        function drawAttentionHeatmap(canvas, attentionWeights, tokens) {
            const ctx = canvas.getContext('2d');
            const size = attentionWeights.length;
            const cellSize = Math.min(400 / size, 50);
            const margin = 80;

            canvas.width = size * cellSize + margin * 2;
            canvas.height = size * cellSize + margin * 2;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw heatmap
            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size; j++) {
                    const weight = attentionWeights[i][j];
                    const intensity = Math.floor(weight * 255);
                    ctx.fillStyle = `rgb(${255-intensity}, ${255-intensity/2}, 255)`;
                    ctx.fillRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw border
                    ctx.strokeStyle = '#dee2e6';
                    ctx.strokeRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw value
                    ctx.fillStyle = weight > 0.5 ? 'white' : 'black';
                    ctx.font = `${Math.min(cellSize/3, 12)}px Arial`;
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    ctx.fillText(
                        weight.toFixed(2),
                        margin + j * cellSize + cellSize/2,
                        margin + i * cellSize + cellSize/2
                    );
                }
            }

            // Draw labels
            ctx.fillStyle = '#495057';
            ctx.font = '14px Arial';
            ctx.textAlign = 'right';
            for (let i = 0; i < size; i++) {
                ctx.fillText(tokens[i], margin - 10, margin + i * cellSize + cellSize/2);
            }

            ctx.textAlign = 'center';
            for (let j = 0; j < size; j++) {
                ctx.save();
                ctx.translate(margin + j * cellSize + cellSize/2, margin - 10);
                ctx.rotate(-Math.PI / 4);
                ctx.fillText(tokens[j], 0, 0);
                ctx.restore();
            }

            // Draw axis labels
            ctx.font = 'bold 16px Arial';
            ctx.fillStyle = '#667eea';
            ctx.textAlign = 'center';
            ctx.fillText('Keys (attending to)', canvas.width / 2, 30);
            ctx.save();
            ctx.translate(30, canvas.height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Queries (attending from)', 0, 0);
            ctx.restore();
        }

        function formatMatrix(matrix, label) {
            let html = `<h4>${label}</h4><div class="matrix-display">`;
            html += matrix.map((row, i) =>
                `<div class="matrix-row">[${row.map(v =>
                    `<span class="matrix-value">${v.toFixed(4)}</span>`
                ).join(' ')}]</div>`
            ).join('');
            html += '</div>';
            return html;
        }

        function runAttention() {
            const inputText = document.getElementById('inputText').value;
            const embedDim = parseInt(document.getElementById('embedDim').value);
            const numHeads = parseInt(document.getElementById('numHeads').value);
            const temperature = parseFloat(document.getElementById('temperature').value);

            const tokens = inputText.trim().split(/\s+/);

            // Display tokens
            document.getElementById('tokenDisplay').innerHTML = tokens.map(
                (t, i) => `<div class="token">${i+1}. ${t}</div>`
            ).join('');

            // Generate embeddings
            const embeddings = generateEmbeddings(tokens, embedDim);

            // Create and run attention
            const mhsa = new MultiHeadSelfAttention(embedDim, numHeads, temperature);
            const result = mhsa.forward(embeddings);
            currentAttention = result;

            // Display architecture info
            document.getElementById('architectureInfo').innerHTML = `
                <div class="info-box">
                    <strong>Configuration:</strong><br>
                    â€¢ Sequence Length: ${tokens.length} tokens<br>
                    â€¢ Embedding Dimension (d_model): ${embedDim}<br>
                    â€¢ Number of Heads: ${numHeads}<br>
                    â€¢ Head Dimension (d_k = d_model/num_heads): ${mhsa.headDim}<br>
                    â€¢ Temperature: ${temperature}
                </div>
            `;

            // Draw combined attention
            const avgAttention = averageAttentions(result.headAttentions);
            const combinedCanvas = document.getElementById('combinedAttentionCanvas');
            drawAttentionHeatmap(combinedCanvas, avgAttention, tokens);

            // Draw individual heads
            const headsContainer = document.getElementById('headsContainer');
            headsContainer.innerHTML = '';
            for (let h = 0; h < numHeads; h++) {
                const headDiv = document.createElement('div');
                headDiv.className = 'head-container';
                headDiv.innerHTML = `
                    <div class="head-title">Head ${h + 1}</div>
                    <canvas id="headCanvas${h}"></canvas>
                `;
                headsContainer.appendChild(headDiv);

                setTimeout(() => {
                    const canvas = document.getElementById(`headCanvas${h}`);
                    drawAttentionHeatmap(canvas, result.headAttentions[h], tokens);
                }, 10);
            }

            // Generate mathematical breakdown
            generateMathBreakdown(tokens, embeddings, mhsa, result);

            // Generate Q/K/V visualizations
            generateQKVVisualization(tokens, embeddings, mhsa, result);

            // Generate attention flow visualization
            generateAttentionFlowVisualization(tokens, embeddings, mhsa, result);

            // Prepare sequence data
            prepareSequenceData(tokens, embeddings, mhsa, result);

            // Initialize step navigator
            initializeStepNavigator();

            // Render initial step
            renderStep();

            // Render architecture visualization
            renderArchitectureView();
        }

        function averageAttentions(attentions) {
            const numHeads = attentions.length;
            const seqLen = attentions[0].length;
            const avg = [];

            for (let i = 0; i < seqLen; i++) {
                avg[i] = [];
                for (let j = 0; j < seqLen; j++) {
                    let sum = 0;
                    for (let h = 0; h < numHeads; h++) {
                        sum += attentions[h][i][j];
                    }
                    avg[i][j] = sum / numHeads;
                }
            }
            return avg;
        }

        function generateMathBreakdown(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('mathBreakdown');

            let html = `
                <div class="math-step">
                    <h4>Step 1: Input Embeddings</h4>
                    <p>Each token is represented as a ${mhsa.embedDim}-dimensional vector.</p>
                    <div class="formula">X âˆˆ â„^(${tokens.length} Ã— ${mhsa.embedDim})</div>
                    ${formatMatrix(embeddings.map(e => e.slice(0, 8)), 'Embeddings (first 8 dimensions)')}
                </div>

                <div class="math-step">
                    <h4>Step 2: Linear Projections</h4>
                    <p>For each head h, we project the input into Query, Key, and Value spaces:</p>
                    <div class="formula">
                        Q_h = X Â· W^Q_h<br>
                        K_h = X Â· W^K_h<br>
                        V_h = X Â· W^V_h<br>
                        where W^Q_h, W^K_h, W^V_h âˆˆ â„^(${mhsa.embedDim} Ã— ${mhsa.headDim})
                    </div>
                    <div class="info-box">
                        Each head has its own projection matrices, allowing it to learn different representations.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 3: Scaled Dot-Product Attention</h4>
                    <p>For each head, we compute attention scores:</p>
                    <div class="formula">
                        scores = (Q Â· K^T) / âˆšd_k<br>
                        where d_k = ${mhsa.headDim}
                    </div>
                    <p>The scaling factor prevents the dot products from becoming too large.</p>
                </div>

                <div class="math-step">
                    <h4>Step 4: Softmax Normalization</h4>
                    <p>Apply softmax to get attention weights that sum to 1:</p>
                    <div class="formula">
                        Attention(Q, K, V) = softmax(scores / temperature) Â· V<br>
                        temperature = ${mhsa.temperature}
                    </div>
                    <p>Higher temperature makes the distribution more uniform; lower makes it more peaked.</p>
                </div>
            `;

            // Show first head's computations in detail
            const head0 = result.headOutputs[0];
            html += `
                <div class="math-step">
                    <h4>Example: Head 1 Detailed Computation</h4>
                    ${formatMatrix(head0.Q.map(q => q.slice(0, 4)), 'Q (first 4 dims)')}
                    ${formatMatrix(head0.K.map(k => k.slice(0, 4)), 'K (first 4 dims)')}
                    ${formatMatrix(head0.scores, 'Attention Scores (scaled)')}
                    ${formatMatrix(head0.attentionWeights, 'Attention Weights (after softmax)')}
                </div>

                <div class="math-step">
                    <h4>Step 5: Concatenate Heads</h4>
                    <p>Concatenate outputs from all ${mhsa.numHeads} heads:</p>
                    <div class="formula">
                        MultiHead = Concat(head_1, head_2, ..., head_${mhsa.numHeads})<br>
                        where each head_i âˆˆ â„^(${tokens.length} Ã— ${mhsa.headDim})
                    </div>
                    <p>Result: â„^(${tokens.length} Ã— ${mhsa.numHeads * mhsa.headDim})</p>
                </div>

                <div class="math-step">
                    <h4>Step 6: Output Projection</h4>
                    <p>Final linear transformation:</p>
                    <div class="formula">
                        Output = MultiHead Â· W^O<br>
                        where W^O âˆˆ â„^(${mhsa.numHeads * mhsa.headDim} Ã— ${mhsa.embedDim})
                    </div>
                    ${formatMatrix(result.output.map(o => o.slice(0, 8)), 'Final Output (first 8 dimensions)')}
                </div>

                <div class="info-box">
                    <strong>Key Insights:</strong><br>
                    â€¢ Each attention head can learn to focus on different relationships<br>
                    â€¢ The attention weights are position-dependent and learned from data<br>
                    â€¢ Self-attention allows every token to attend to every other token<br>
                    â€¢ Scaling by âˆšd_k stabilizes gradients during training<br>
                    â€¢ The final output combines information from all attention heads
                </div>
            `;

            container.innerHTML = html;
        }

        function generateQKVVisualization(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('qkvVisualization');
            const head0 = result.headOutputs[0];

            let html = `
                <div class="math-step">
                    <h4>Transformation Pipeline for First Token: "${tokens[0]}"</h4>
                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box token-box">Token<br>"${tokens[0]}"</div>
                            <small>Input word</small>
                        </div>
                        <div class="flow-arrow">â†’</div>
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                            <small>Dense vector</small>
                        </div>
                        <div class="flow-arrow">â†’</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #FF6B6B, #EE5A6F);">
                                Query<br>[${mhsa.headDim}d]
                            </div>
                            <small>What to look for</small>
                        </div>
                    </div>

                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                        </div>
                        <div class="flow-arrow">â†’</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #4ECDC4, #44A08D);">
                                Key<br>[${mhsa.headDim}d]
                            </div>
                            <small>What I offer</small>
                        </div>
                    </div>

                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                        </div>
                        <div class="flow-arrow">â†’</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #95E1D3, #74C7BB);">
                                Value<br>[${mhsa.headDim}d]
                            </div>
                            <small>Actual content</small>
                        </div>
                    </div>
                </div>

                <div class="meaning-box">
                    <h4>ðŸŽ¯ How Meaning Emerges</h4>
                    <p><strong>Step 1: Projection Matrices Learn Semantic Roles</strong></p>
                    <p>The weight matrices W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> are learned during training. They transform the generic embedding into specialized representations:</p>
                    <ul style="margin: 10px 0; padding-left: 20px;">
                        <li><span class="highlight-query">Query (Q)</span> encodes what information this token needs (e.g., "cat" might query for subjects or animals)</li>
                        <li><span class="highlight-key">Key (K)</span> encodes what information this token can provide (e.g., "sat" might provide action information)</li>
                        <li><span class="highlight-value">Value (V)</span> contains the semantic content to be retrieved</li>
                    </ul>
                </div>

                <div class="qkv-grid">
                    <div class="qkv-card qkv-query">
                        <h3>ðŸ” Query Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">Q = Embedding Ã— W<sup>Q</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.Q[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row shows what that token is "asking for"</small>
                    </div>

                    <div class="qkv-card qkv-key">
                        <h3>ðŸ”‘ Key Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">K = Embedding Ã— W<sup>K</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.K[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row shows what that token "offers"</small>
                    </div>

                    <div class="qkv-card qkv-value">
                        <h3>ðŸ’Ž Value Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">V = Embedding Ã— W<sup>V</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.V[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row contains the actual information</small>
                    </div>
                </div>

                <div class="math-step">
                    <h4>ðŸ“Š Projection Weight Matrices (Head 1)</h4>
                    <p>These matrices transform ${mhsa.embedDim}-dimensional embeddings into ${mhsa.headDim}-dimensional Q/K/V vectors:</p>
                    <canvas id="weightMatrixCanvas" class="interaction-canvas"></canvas>
                </div>
            `;

            container.innerHTML = html;

            // Draw weight matrix visualization
            setTimeout(() => {
                drawWeightMatrixVisualization(mhsa);
            }, 10);
        }

        function drawWeightMatrixVisualization(mhsa) {
            const canvas = document.getElementById('weightMatrixCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const head = mhsa.heads[0];

            canvas.width = 900;
            canvas.height = 250;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            const matrices = [
                { matrix: head.Wq, name: 'W^Q', color: '#FF6B6B', x: 50 },
                { matrix: head.Wk, name: 'W^K', color: '#4ECDC4', x: 350 },
                { matrix: head.Wv, name: 'W^V', color: '#95E1D3', x: 650 }
            ];

            matrices.forEach(({ matrix, name, color, x }) => {
                const rows = Math.min(matrix.length, 16);
                const cols = Math.min(matrix[0].length, 8);
                const cellSize = 8;

                // Draw matrix
                for (let i = 0; i < rows; i++) {
                    for (let j = 0; j < cols; j++) {
                        const val = matrix[i][j];
                        const normalized = (val + 0.5); // Normalize to [0, 1]
                        const intensity = Math.floor(normalized * 255);
                        ctx.fillStyle = `rgba(${color.slice(1, 3)}, ${color.slice(3, 5)}, ${color.slice(5, 7)}, ${normalized})`;
                        ctx.fillRect(x + j * cellSize, 80 + i * cellSize, cellSize - 1, cellSize - 1);
                    }
                }

                // Draw labels
                ctx.fillStyle = '#495057';
                ctx.font = 'bold 16px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(name, x + cols * cellSize / 2, 60);

                ctx.font = '12px Arial';
                ctx.fillText(`${matrix.length} Ã— ${matrix[0].length}`, x + cols * cellSize / 2, 220);

                // Draw border
                ctx.strokeStyle = color;
                ctx.lineWidth = 2;
                ctx.strokeRect(x, 80, cols * cellSize, rows * cellSize);
            });

            // Add description
            ctx.fillStyle = '#495057';
            ctx.font = '13px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Each matrix learns to extract different semantic aspects during training', canvas.width / 2, 245);
        }

        function generateAttentionFlowVisualization(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('attentionFlow');
            const head0 = result.headOutputs[0];

            // Pick an example token to focus on
            const focusIdx = Math.floor(tokens.length / 2);
            const focusToken = tokens[focusIdx];

            let html = `
                <div class="meaning-box">
                    <h4>ðŸ§  How Attention Extracts Meaning: Example with "${focusToken}"</h4>
                    <p>Let's trace how token "${focusToken}" (position ${focusIdx + 1}) gathers information from other tokens:</p>
                </div>

                <div class="math-step">
                    <h4>Step 1: Compute Attention Scores (Q Â· K<sup>T</sup>)</h4>
                    <p>The <span class="highlight-query">query</span> of "${focusToken}" is compared with the <span class="highlight-key">keys</span> of all tokens via dot product:</p>
                    <div class="flow-diagram" style="flex-wrap: wrap;">
            `;

            tokens.forEach((token, i) => {
                const score = head0.scores[focusIdx][i];
                const weight = head0.attentionWeights[focusIdx][i];
                html += `
                    <div class="flow-step" style="flex: 1; min-width: 120px;">
                        <div class="flow-box" style="background: rgba(102, 126, 234, ${weight});">
                            ${token}
                        </div>
                        <small>Score: ${score.toFixed(3)}<br>Weight: ${weight.toFixed(3)}</small>
                    </div>
                `;
            });

            html += `
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> High dot product (QÂ·K) means the query and key are aligned in the learned semantic space. 
                        For example, if "cat" learns to query for subjects, and "sat" has a key indicating it's a verb, their dot product will be high.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 2: Softmax Normalization</h4>
                    <p>Raw scores are converted to probabilities that sum to 1:</p>
                    <canvas id="softmaxCanvas" class="interaction-canvas"></canvas>
                </div>

                <div class="math-step">
                    <h4>Step 3: Weighted Aggregation of Values</h4>
                    <p>The attention weights determine how much of each token's <span class="highlight-value">value</span> vector to include:</p>
                    <div class="formula">
                        output[${focusToken}] = ${tokens.map((t, i) => 
                            `${head0.attentionWeights[focusIdx][i].toFixed(2)} Ã— V[${t}]`
                        ).join(' + ')}
                    </div>
                    <p style="margin-top: 10px;">This weighted sum creates a context-aware representation of "${focusToken}" based on the entire sequence.</p>
                </div>

                <div class="math-step">
                    <h4>Visualizing the Aggregation</h4>
                    <canvas id="aggregationCanvas" class="interaction-canvas"></canvas>
                </div>

                <div class="meaning-box">
                    <h4>ðŸŽ“ Key Insights on Semantic Understanding</h4>
                    <ol style="margin: 10px 0; padding-left: 20px; line-height: 1.8;">
                        <li><strong>Learned Representations:</strong> During training, the projection matrices learn to encode linguistic patterns. W<sup>Q</sup> might learn to encode "what grammatical role does this word need to find?", W<sup>K</sup> encodes "what role do I play?", and W<sup>V</sup> holds the actual semantic content.</li>
                        
                        <li><strong>Dynamic Contextualization:</strong> Unlike static word embeddings, attention creates context-dependent representations. The word "bank" gets different representations in "river bank" vs "bank account" because it attends to different neighboring words.</li>
                        
                        <li><strong>Multiple Heads = Multiple Perspectives:</strong> With ${mhsa.numHeads} heads, the model can simultaneously attend to different aspects: one head might capture syntax, another semantics, another long-range dependencies.</li>
                        
                        <li><strong>Information Routing:</strong> Attention acts as a soft routing mechanism, allowing information to flow from relevant tokens to where it's needed. High attention = "this token is relevant to understanding me".</li>
                        
                        <li><strong>Permutation Invariance:</strong> Self-attention doesn't care about the order of tokens inherently (that's why positional encodings are added separately). It learns relationships based purely on content.</li>
                    </ol>
                </div>

                <div class="math-step">
                    <h4>ðŸ”„ Comparison: Before and After Attention</h4>
                    <canvas id="comparisonCanvas" class="interaction-canvas"></canvas>
                    <p style="margin-top: 10px; text-align: center;">
                        <strong>Before:</strong> Static embedding based only on the token itself<br>
                        <strong>After:</strong> Contextualized representation incorporating information from the entire sequence
                    </p>
                </div>
            `;

            container.innerHTML = html;

            // Draw visualizations
            setTimeout(() => {
                drawSoftmaxVisualization(tokens, focusIdx, head0);
                drawAggregationVisualization(tokens, focusIdx, head0);
                drawComparisonVisualization(embeddings, result.output, focusIdx, tokens[focusIdx]);
            }, 10);
        }

        function drawSoftmaxVisualization(tokens, focusIdx, head0) {
            const canvas = document.getElementById('softmaxCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 300;

            const scores = head0.scores[focusIdx];
            const weights = head0.attentionWeights[focusIdx];

            const barWidth = 80;
            const gap = 20;
            const startX = (canvas.width - tokens.length * (barWidth + gap)) / 2;
            const baseY = 250;

            // Draw bars for before and after
            tokens.forEach((token, i) => {
                const x = startX + i * (barWidth + gap);
                
                // Raw score (scaled for visualization)
                const scoreHeight = Math.min(Math.abs(scores[i]) * 30, 60);
                ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
                ctx.fillRect(x, baseY - scoreHeight - 80, barWidth / 2 - 5, scoreHeight);

                // Softmax weight
                const weightHeight = weights[i] * 120;
                ctx.fillStyle = 'rgba(102, 126, 234, 0.8)';
                ctx.fillRect(x + barWidth / 2, baseY - weightHeight - 80, barWidth / 2 - 5, weightHeight);

                // Labels
                ctx.fillStyle = '#495057';
                ctx.font = '12px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(token, x + barWidth / 2, baseY - 60);
                ctx.font = '10px Arial';
                ctx.fillText(`${scores[i].toFixed(2)}`, x + barWidth / 4, baseY - scoreHeight - 85);
                ctx.fillText(`${weights[i].toFixed(2)}`, x + 3 * barWidth / 4, baseY - weightHeight - 85);
            });

            // Legend
            ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
            ctx.fillRect(20, 20, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Raw Scores (QÂ·K)', 40, 32);

            ctx.fillStyle = 'rgba(102, 126, 234, 0.8)';
            ctx.fillRect(20, 40, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.fillText('After Softmax (Attention Weights)', 40, 52);
        }

        function drawAggregationVisualization(tokens, focusIdx, head0) {
            const canvas = document.getElementById('aggregationCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;

            const weights = head0.attentionWeights[focusIdx];
            const values = head0.V;

            // Draw value vectors as bars
            const numDims = 8; // Show first 8 dimensions
            const barWidth = 60;
            const gap = 10;
            const dimHeight = 30;

            let startY = 50;

            tokens.forEach((token, tokenIdx) => {
                const weight = weights[tokenIdx];
                const x = 50;

                // Token label
                ctx.fillStyle = '#495057';
                ctx.font = '12px Arial';
                ctx.textAlign = 'right';
                ctx.fillText(`${token} (${weight.toFixed(2)}Ã—)`, x - 10, startY + 15);

                // Draw value vector bars
                for (let dim = 0; dim < numDims; dim++) {
                    const val = values[tokenIdx][dim];
                    const weightedVal = val * weight;
                    const barLen = Math.abs(val) * 40;
                    const weightedBarLen = Math.abs(weightedVal) * 40;

                    // Original value (faint)
                    ctx.fillStyle = 'rgba(149, 225, 211, 0.3)';
                    ctx.fillRect(x + dim * (barWidth / numDims), startY, barWidth / numDims - 2, 10);

                    // Weighted value
                    const hue = weightedVal > 0 ? 150 : 0;
                    ctx.fillStyle = `hsla(${hue}, 70%, 50%, ${Math.abs(weight)})`;
                    ctx.fillRect(x + dim * (barWidth / numDims), startY, (barWidth / numDims - 2) * Math.min(Math.abs(weightedVal) * 5, 1), 10);
                }

                startY += 20;
            });

            // Title
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Each row: Value vector Ã— Attention weight', 50, 30);

            ctx.font = '11px Arial';
            ctx.fillText('Higher attention weight = more contribution to final output', 50, startY + 20);
        }

        function drawComparisonVisualization(embeddings, outputs, focusIdx, token) {
            const canvas = document.getElementById('comparisonCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 300;

            const numDims = Math.min(embeddings[focusIdx].length, 32);
            const barWidth = 500 / numDims;
            const startX = 50;

            // Draw input embedding
            let y = 80;
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 13px Arial';
            ctx.textAlign = 'left';
            ctx.fillText(`Input Embedding of "${token}"`, startX, y - 10);

            for (let i = 0; i < numDims; i++) {
                const val = embeddings[focusIdx][i];
                const height = Math.abs(val) * 50;
                ctx.fillStyle = val > 0 ? 'rgba(102, 126, 234, 0.6)' : 'rgba(234, 102, 126, 0.6)';
                ctx.fillRect(startX + i * barWidth, y + 50 - height, barWidth - 1, height);
            }

            // Draw output
            y = 180;
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 13px Arial';
            ctx.fillText(`Contextualized Output of "${token}"`, startX, y - 10);

            for (let i = 0; i < numDims; i++) {
                const val = outputs[focusIdx][i];
                const height = Math.abs(val) * 50;
                ctx.fillStyle = val > 0 ? 'rgba(118, 75, 162, 0.8)' : 'rgba(162, 75, 118, 0.8)';
                ctx.fillRect(startX + i * barWidth, y + 50 - height, barWidth - 1, height);
            }

            // Arrow
            ctx.strokeStyle = '#667eea';
            ctx.lineWidth = 3;
            ctx.beginPath();
            ctx.moveTo(20, 110);
            ctx.lineTo(20, 190);
            ctx.stroke();

            // Arrowhead
            ctx.beginPath();
            ctx.moveTo(15, 180);
            ctx.lineTo(20, 190);
            ctx.lineTo(25, 180);
            ctx.fillStyle = '#667eea';
            ctx.fill();

            ctx.fillStyle = '#667eea';
            ctx.font = 'bold 12px Arial';
            ctx.save();
            ctx.translate(10, 150);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('ATTENTION', 0, 0);
            ctx.restore();
        }

        function drawStepSoftmaxCanvas(canvas, tokens, focusIdx, head0) {
            const ctx = canvas.getContext('2d');
            const scores = head0.scores[focusIdx];
            const weights = head0.attentionWeights[focusIdx];

            const barWidth = Math.min(80, 500 / tokens.length);
            const gap = 10;
            const startX = (canvas.width - tokens.length * (barWidth + gap)) / 2;
            const baseY = 250;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw bars
            tokens.forEach((token, i) => {
                const x = startX + i * (barWidth + gap);
                
                // Scaled score (before softmax)
                const scoreHeight = Math.min(Math.abs(scores[i]) * 40, 80);
                ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
                ctx.fillRect(x, baseY - scoreHeight - 80, barWidth / 2 - 2, scoreHeight);

                // Softmax weight
                const weightHeight = weights[i] * 120;
                ctx.fillStyle = 'rgba(102, 126, 234, 0.9)';
                ctx.fillRect(x + barWidth / 2, baseY - weightHeight - 80, barWidth / 2 - 2, weightHeight);

                // Labels
                ctx.fillStyle = '#495057';
                ctx.font = '11px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(token, x + barWidth / 2, baseY - 60);
                
                ctx.font = '9px Arial';
                ctx.fillText(scores[i].toFixed(2), x + barWidth / 4, baseY - scoreHeight - 85);
                ctx.fillText((weights[i] * 100).toFixed(0) + '%', x + 3 * barWidth / 4, baseY - weightHeight - 85);
            });

            // Legend
            ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
            ctx.fillRect(20, 20, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Scaled Scores', 40, 32);

            ctx.fillStyle = 'rgba(102, 126, 234, 0.9)';
            ctx.fillRect(20, 40, 15, 15);
            ctx.fillText('Attention Weights (%)', 40, 52);
        }

        function prepareSequenceData(tokens, embeddings, mhsa, result) {
            sequenceData = {
                tokens,
                embeddings,
                mhsa,
                result,
                focusToken: Math.floor(tokens.length / 2)
            };
        }

        function initializeStepNavigator() {
            const steps = [
                'Input Tokens',
                'Token Embeddings',
                'Linear Projections',
                'Query-Key Dot Product',
                'Scaling',
                'Softmax',
                'Attention Ã— Values',
                'Concatenate Heads',
                'Final Output'
            ];

            const navGrid = document.getElementById('stepNavGrid');
            navGrid.innerHTML = steps.map((name, idx) => `
                <div class="nav-step ${idx === 0 ? 'active' : ''}" onclick="goToStep(${idx})">
                    <div class="nav-step-num">Step ${idx + 1}</div>
                    <div class="nav-step-name">${name}</div>
                </div>
            `).join('');
        }

        function generateStepContent(step, data) {
            const { tokens, embeddings, mhsa, result, focusToken } = data;
            const head0 = result.headOutputs[0];

            switch(step) {
                case 0: return generateStep0_InputTokens(tokens);
                case 1: return generateStep1_Embeddings(tokens, embeddings, mhsa);
                case 2: return generateStep2_Projections(tokens, embeddings, mhsa, head0);
                case 3: return generateStep3_DotProduct(tokens, head0, focusToken);
                case 4: return generateStep4_Scaling(tokens, head0, mhsa, focusToken);
                case 5: return generateStep5_Softmax(tokens, head0, focusToken);
                case 6: return generateStep6_AttentionValues(tokens, head0, focusToken);
                case 7: return generateStep7_Concatenate(tokens, result, mhsa);
                case 8: return generateStep8_FinalOutput(tokens, embeddings, result);
                default: return '';
            }
        }

        function generateStep0_InputTokens(tokens) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">1</span>
                        Input Tokens
                    </div>
                    <div class="step-description">
                        We start with a sequence of tokens (words or subwords) from our input text.
                        Each token will be processed independently but will interact through attention.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Current Data Dimensions</h4>
                        <div class="dim-row">
                            <div class="dim-badge">
                                <span class="dim-label">Sequence Length:</span>
                                <span class="dim-shape">n = ${tokens.length}</span>
                            </div>
                            <div class="dim-badge">
                                <span class="dim-label">Data Type:</span>
                                <span class="dim-shape">Discrete Tokens</span>
                            </div>
                        </div>
                    </div>

                    <div class="computation-visual">
                        <h4 style="margin-bottom: 15px;">Input Sequence</h4>
                        <div class="data-flow">
                            ${tokens.map((token, idx) => `
                                <div class="data-box active">
                                    <div class="data-label">Token ${idx + 1}</div>
                                    <div class="data-value">"${token}"</div>
                                </div>
                            `).join('')}
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> Raw text string</li>
                            <li><strong>Output:</strong> ${tokens.length} discrete tokens</li>
                            <li><strong>Next:</strong> Convert tokens to dense vector embeddings</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>Each token is currently just a symbol (like a word). Before the attention mechanism can work, 
                        we need to convert these into continuous vectors that capture semantic meaning.</p>
                    </div>
                </div>
            `;
        }

        function generateStep1_Embeddings(tokens, embeddings, mhsa) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">2</span>
                        Token Embeddings
                    </div>
                    <div class="step-description">
                        Each token is converted into a dense vector of dimension d<sub>model</sub> = ${mhsa.embedDim}.
                        This embedding captures semantic information about the token.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Current Data Dimensions</h4>
                        <div class="dim-row">
                            <div class="dim-badge embedding">
                                <span class="dim-label">Embedding Matrix X:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${mhsa.embedDim}]</span>
                            </div>
                        </div>
                        <div class="dim-row">
                            <div class="dim-badge">
                                <span class="dim-label">${tokens.length} rows</span>
                                <span class="dim-shape">= ${tokens.length} tokens</span>
                            </div>
                            <div class="dim-badge">
                                <span class="dim-label">${mhsa.embedDim} columns</span>
                                <span class="dim-shape">= embedding dimension (d_model)</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect medium embedding" style="width: 40px; height: 80px;">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${mhsa.embedDim}</span>
                                X
                            </div>
                            <div class="matrix-name">Embedding Matrix</div>
                            <div class="matrix-dims">[n Ã— d_model]</div>
                        </div>
                    </div>

                    <div class="computation-visual">
                        <h4 style="margin-bottom: 15px;">Embedding Transformation</h4>
                        ${tokens.slice(0, 4).map((token, idx) => `
                            <div class="token-embedding-viz">
                                <div class="token-box">${token}</div>
                                <div class="projection-arrow">â†’</div>
                                <div class="embedding-viz">
                                    ${embeddings[idx].slice(0, 16).map(val => 
                                        `<div class="embedding-bar" style="height: ${Math.abs(val) * 60 + 15}px;"></div>`
                                    ).join('')}
                                </div>
                                <small style="margin-left: 10px; font-weight: 600; color: #667eea;">[${mhsa.embedDim}d vector]</small>
                            </div>
                        `).join('')}
                        ${tokens.length > 4 ? `<div style="text-align: center; padding: 10px; color: #6c757d;">... and ${tokens.length - 4} more tokens</div>` : ''}
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> ${tokens.length} discrete tokens</li>
                            <li><strong>Operation:</strong> Lookup in embedding table</li>
                            <li><strong>Output:</strong> Matrix X of shape <code>[${tokens.length} Ã— ${mhsa.embedDim}]</code></li>
                            <li><strong>Next:</strong> Project embeddings to Q, K, V spaces</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>Each token is now a ${mhsa.embedDim}-dimensional vector. Semantically similar words will have similar vectors. 
                        This dense representation allows mathematical operations on word meanings.</p>
                    </div>
                </div>
            `;
        }

        function generateStep2_Projections(tokens, embeddings, mhsa, head0) {
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">3</span>
                        Linear Projections to Q, K, V
                    </div>
                    <div class="step-description">
                        Each embedding is transformed into three different representations using learned weight matrices:
                        <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Matrix Dimensions (Per Head)</h4>
                        <div class="dim-row">
                            <div class="dim-badge embedding">
                                <span class="dim-label">Input X:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${mhsa.embedDim}]</span>
                            </div>
                        </div>
                        <div class="dim-row">
                            <div class="dim-badge query">
                                <span class="dim-label">W<sup>Q</sup>:</span>
                                <span class="dim-shape">[${mhsa.embedDim} Ã— ${mhsa.headDim}]</span>
                            </div>
                            <div class="dim-badge key">
                                <span class="dim-label">W<sup>K</sup>:</span>
                                <span class="dim-shape">[${mhsa.embedDim} Ã— ${mhsa.headDim}]</span>
                            </div>
                            <div class="dim-badge value">
                                <span class="dim-label">W<sup>V</sup>:</span>
                                <span class="dim-shape">[${mhsa.embedDim} Ã— ${mhsa.headDim}]</span>
                            </div>
                        </div>
                        <div class="dim-row">
                            <div class="dim-badge query">
                                <span class="dim-label">Q = XÂ·W<sup>Q</sup>:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${mhsa.headDim}]</span>
                            </div>
                            <div class="dim-badge key">
                                <span class="dim-label">K = XÂ·W<sup>K</sup>:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${mhsa.headDim}]</span>
                            </div>
                            <div class="dim-badge value">
                                <span class="dim-label">V = XÂ·W<sup>V</sup>:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${mhsa.headDim}]</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect medium embedding">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${mhsa.embedDim}</span>
                                X
                            </div>
                            <div class="matrix-name">Embeddings</div>
                        </div>
                        <div class="op-symbol">Ã—</div>
                        <div class="matrix-block">
                            <div class="matrix-rect small query">
                                <span class="matrix-dim-label top">${mhsa.embedDim}</span>
                                <span class="matrix-dim-label right">${mhsa.headDim}</span>
                                W<sup>Q</sup>
                            </div>
                            <div class="matrix-name">Query Weights</div>
                        </div>
                        <div class="op-symbol">=</div>
                        <div class="matrix-block">
                            <div class="matrix-rect medium query">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${mhsa.headDim}</span>
                                Q
                            </div>
                            <div class="matrix-name">Queries</div>
                        </div>
                    </div>

                    <div class="qkv-grid">
                        <div class="qkv-card qkv-query">
                            <h3>ðŸ” Queries <span style="font-weight: normal; font-size: 12px;">[${tokens.length}Ã—${mhsa.headDim}]</span></h3>
                            <div class="mini-matrix">
                                ${tokens.slice(0, 3).map((t, i) => 
                                    `<strong>${t}:</strong> [${head0.Q[i].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]`
                                ).join('<br>')}
                                ${tokens.length > 3 ? '<br>...' : ''}
                            </div>
                        </div>
                        <div class="qkv-card qkv-key">
                            <h3>ðŸ”‘ Keys <span style="font-weight: normal; font-size: 12px;">[${tokens.length}Ã—${mhsa.headDim}]</span></h3>
                            <div class="mini-matrix">
                                ${tokens.slice(0, 3).map((t, i) => 
                                    `<strong>${t}:</strong> [${head0.K[i].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]`
                                ).join('<br>')}
                                ${tokens.length > 3 ? '<br>...' : ''}
                            </div>
                        </div>
                        <div class="qkv-card qkv-value">
                            <h3>ðŸ’Ž Values <span style="font-weight: normal; font-size: 12px;">[${tokens.length}Ã—${mhsa.headDim}]</span></h3>
                            <div class="mini-matrix">
                                ${tokens.slice(0, 3).map((t, i) => 
                                    `<strong>${t}:</strong> [${head0.V[i].slice(0, 4).map(v => v.toFixed(2)).join(', ')}...]`
                                ).join('<br>')}
                                ${tokens.length > 3 ? '<br>...' : ''}
                            </div>
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> Embedding matrix X <code>[${tokens.length} Ã— ${mhsa.embedDim}]</code></li>
                            <li><strong>Operation:</strong> Three matrix multiplications (one per Q, K, V)</li>
                            <li><strong>Output:</strong> Q, K, V matrices, each <code>[${tokens.length} Ã— ${mhsa.headDim}]</code></li>
                            <li><strong>Per Head:</strong> Dimension reduced from ${mhsa.embedDim} â†’ ${mhsa.headDim}</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p><strong>Query:</strong> "What am I looking for?" | <strong>Key:</strong> "What do I offer?" | <strong>Value:</strong> "What's my content?"
                        <br>These three projections let each token play different roles in the attention mechanism.</p>
                    </div>
                </div>
            `;
        }

        function generateStep3_DotProduct(tokens, head0, focusToken) {
            const focusTokenName = tokens[focusToken];
            const scores = tokens.map((_, idx) => 
                head0.Q[focusToken].reduce((sum, q, i) => sum + q * head0.K[idx][i], 0)
            );
            const maxScore = Math.max(...scores.map(Math.abs));
            
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">4</span>
                        Compute Query-Key Dot Product (Q Â· K<sup>T</sup>)
                    </div>
                    <div class="step-description">
                        For each query vector, we compute its dot product with all key vectors.
                        This measures how similar/relevant each key is to the query.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Matrix Dimensions</h4>
                        <div class="dim-row">
                            <div class="dim-badge query">
                                <span class="dim-label">Q:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${head0.Q[0].length}]</span>
                            </div>
                            <div class="dim-operator">Ã—</div>
                            <div class="dim-badge key">
                                <span class="dim-label">K<sup>T</sup>:</span>
                                <span class="dim-shape">[${head0.K[0].length} Ã— ${tokens.length}]</span>
                            </div>
                            <div class="dim-operator">=</div>
                            <div class="dim-badge scores">
                                <span class="dim-label">Scores:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${tokens.length}]</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect medium query">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${head0.Q[0].length}</span>
                                Q
                            </div>
                            <div class="matrix-name">Queries</div>
                            <div class="matrix-dims">[n Ã— d_k]</div>
                        </div>
                        <div class="op-symbol">Ã—</div>
                        <div class="matrix-block">
                            <div class="matrix-rect medium key">
                                <span class="matrix-dim-label top">${head0.K[0].length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                K<sup>T</sup>
                            </div>
                            <div class="matrix-name">Keys (Transposed)</div>
                            <div class="matrix-dims">[d_k Ã— n]</div>
                        </div>
                        <div class="op-symbol">=</div>
                        <div class="matrix-block">
                            <div class="matrix-rect square scores">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                S
                            </div>
                            <div class="matrix-name">Score Matrix</div>
                            <div class="matrix-dims">[n Ã— n]</div>
                        </div>
                    </div>

                    <div class="matrix-visual-box" data-label="Focus: Token '${focusTokenName}' (Row ${focusToken + 1})">
                        <h4 style="margin-bottom: 15px; color: #667eea;">Computing one row of the score matrix:</h4>
                        <div style="background: #f8f9fa; padding: 20px; border-radius: 8px;">
                            <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 20px; flex-wrap: wrap;">
                                <div style="background: #FF6B6B; color: white; padding: 8px 16px; border-radius: 6px; font-weight: 600;">
                                    Query: "${focusTokenName}"
                                </div>
                                <span style="font-size: 20px; color: #667eea;">âŠ™</span>
                                <span style="color: #4ECDC4; font-weight: 600;">each Key</span>
                                <span style="font-size: 20px; color: #667eea;">â†’</span>
                                <span style="color: #ffc107; font-weight: 600;">Similarity Score</span>
                            </div>
                            
                            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(140px, 1fr)); gap: 10px;">
                                ${tokens.map((token, idx) => {
                                    const score = scores[idx];
                                    const normalizedScore = Math.abs(score) / (maxScore + 0.001);
                                    const bgColor = `rgba(255, 193, 7, ${0.1 + normalizedScore * 0.6})`;
                                    return `
                                        <div style="background: ${bgColor}; padding: 12px; border-radius: 8px; text-align: center; border: 2px solid ${normalizedScore > 0.7 ? '#ffc107' : '#dee2e6'};">
                                            <div style="font-weight: 600; color: #4ECDC4; font-size: 13px;">${token}</div>
                                            <div style="font-size: 18px; font-weight: 700; color: #333; margin-top: 5px;">${score.toFixed(3)}</div>
                                        </div>
                                    `;
                                }).join('')}
                            </div>
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> Q <code>[${tokens.length}Ã—${head0.Q[0].length}]</code> and K <code>[${tokens.length}Ã—${head0.K[0].length}]</code></li>
                            <li><strong>Operation:</strong> Matrix multiplication Q Ã— K<sup>T</sup></li>
                            <li><strong>Output:</strong> Score matrix <code>[${tokens.length} Ã— ${tokens.length}]</code> (one score per token pair)</li>
                            <li><strong>Meaning:</strong> scores[i][j] = how relevant token j is to token i</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>Each query asks "who is relevant to me?" and each key answers. 
                        High dot product = high similarity = that token is relevant. 
                        The result is an <strong>${tokens.length}Ã—${tokens.length}</strong> matrix where each row shows one token's "relevance scores" for all other tokens.</p>
                    </div>
                </div>
            `;
        }

        function generateStep4_Scaling(tokens, head0, mhsa, focusToken) {
            const focusTokenName = tokens[focusToken];
            const scale = Math.sqrt(mhsa.headDim);
            const rawScores = tokens.map((_, idx) => 
                head0.Q[focusToken].reduce((sum, q, i) => sum + q * head0.K[idx][i], 0)
            );
            
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">5</span>
                        Scale by âˆšd<sub>k</sub>
                    </div>
                    <div class="step-description">
                        Divide attention scores by âˆšd<sub>k</sub> = âˆš${mhsa.headDim} = <strong>${scale.toFixed(2)}</strong>.
                        This prevents dot products from becoming too large before softmax.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Matrix Dimensions (Unchanged)</h4>
                        <div class="dim-row">
                            <div class="dim-badge scores">
                                <span class="dim-label">Raw Scores:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${tokens.length}]</span>
                            </div>
                            <div class="dim-operator">Ã·</div>
                            <div class="dim-badge">
                                <span class="dim-label">Scale Factor:</span>
                                <span class="dim-shape">âˆš${mhsa.headDim} = ${scale.toFixed(2)}</span>
                            </div>
                            <div class="dim-operator">=</div>
                            <div class="dim-badge scores">
                                <span class="dim-label">Scaled Scores:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${tokens.length}]</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect square scores">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                S
                            </div>
                            <div class="matrix-name">Raw Scores</div>
                        </div>
                        <div class="op-symbol">Ã·</div>
                        <div class="matrix-block">
                            <div style="width: 60px; height: 60px; border: 3px solid #667eea; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; background: white;">
                                ${scale.toFixed(1)}
                            </div>
                            <div class="matrix-name">âˆšd_k</div>
                        </div>
                        <div class="op-symbol">=</div>
                        <div class="matrix-block">
                            <div class="matrix-rect square scores" style="background: linear-gradient(135deg, rgba(255,193,7,0.2), rgba(255,193,7,0.3));">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                S'
                            </div>
                            <div class="matrix-name">Scaled Scores</div>
                        </div>
                    </div>

                    <div class="matrix-visual-box" data-label="Before vs After Scaling (Row: '${focusTokenName}')">
                        <div style="display: grid; grid-template-columns: 1fr auto 1fr; gap: 20px; align-items: start;">
                            <div>
                                <h4 style="color: #dc3545; margin-bottom: 15px;">âŒ Before Scaling</h4>
                                <div style="display: grid; gap: 8px;">
                                    ${tokens.map((token, idx) => `
                                        <div style="display: flex; align-items: center; gap: 10px; padding: 8px 12px; background: #fff5f5; border-radius: 6px;">
                                            <span style="font-weight: 600; min-width: 50px;">${token}</span>
                                            <div style="flex: 1; height: 8px; background: #eee; border-radius: 4px; overflow: hidden;">
                                                <div style="width: ${Math.min(Math.abs(rawScores[idx]) * 20, 100)}%; height: 100%; background: #dc3545;"></div>
                                            </div>
                                            <span style="font-family: monospace; font-size: 12px;">${rawScores[idx].toFixed(3)}</span>
                                        </div>
                                    `).join('')}
                                </div>
                            </div>
                            <div style="display: flex; flex-direction: column; align-items: center; justify-content: center; padding: 20px;">
                                <div style="font-size: 28px; color: #667eea;">Ã· ${scale.toFixed(1)}</div>
                                <div style="font-size: 32px; color: #667eea; margin-top: 10px;">â†’</div>
                            </div>
                            <div>
                                <h4 style="color: #28a745; margin-bottom: 15px;">âœ“ After Scaling</h4>
                                <div style="display: grid; gap: 8px;">
                                    ${tokens.map((token, idx) => `
                                        <div style="display: flex; align-items: center; gap: 10px; padding: 8px 12px; background: #f0fff4; border-radius: 6px;">
                                            <span style="font-weight: 600; min-width: 50px;">${token}</span>
                                            <div style="flex: 1; height: 8px; background: #eee; border-radius: 4px; overflow: hidden;">
                                                <div style="width: ${Math.min(Math.abs(head0.scores[focusToken][idx]) * 40, 100)}%; height: 100%; background: #28a745;"></div>
                                            </div>
                                            <span style="font-family: monospace; font-size: 12px;">${head0.scores[focusToken][idx].toFixed(3)}</span>
                                        </div>
                                    `).join('')}
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> Raw score matrix <code>[${tokens.length} Ã— ${tokens.length}]</code></li>
                            <li><strong>Operation:</strong> Divide each element by âˆš${mhsa.headDim} = ${scale.toFixed(2)}</li>
                            <li><strong>Output:</strong> Scaled score matrix <code>[${tokens.length} Ã— ${tokens.length}]</code></li>
                            <li><strong>Why:</strong> Prevents extreme values that would cause vanishing gradients</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>As the dimension d_k gets larger, dot products tend to get larger too. 
                        Without scaling, softmax would produce nearly one-hot distributions (all attention on one token). 
                        Scaling keeps the scores in a reasonable range for stable training.</p>
                    </div>
                </div>
            `;
        }

        function generateStep5_Softmax(tokens, head0, focusToken) {
            const focusTokenName = tokens[focusToken];
            const weights = head0.attentionWeights[focusToken];
            const maxWeight = Math.max(...weights);
            
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">6</span>
                        Apply Softmax Normalization
                    </div>
                    <div class="step-description">
                        Convert scaled scores to a probability distribution using softmax.
                        The attention weights now sum to 1 and represent "how much to attend to each token."
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Matrix Dimensions</h4>
                        <div class="dim-row">
                            <div class="dim-badge scores">
                                <span class="dim-label">Scaled Scores:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${tokens.length}]</span>
                            </div>
                            <div class="dim-operator">â†’</div>
                            <div class="dim-badge">
                                <span class="dim-label">softmax()</span>
                                <span class="dim-shape">row-wise</span>
                            </div>
                            <div class="dim-operator">â†’</div>
                            <div class="dim-badge attention">
                                <span class="dim-label">Attention Weights:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${tokens.length}]</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect square scores">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                S'
                            </div>
                            <div class="matrix-name">Scaled Scores</div>
                            <div class="matrix-dims">any values</div>
                        </div>
                        <div class="equals-arrow">
                            <div class="text">softmax</div>
                            <div class="arrow">âŸ¹</div>
                        </div>
                        <div class="matrix-block">
                            <div class="matrix-rect square attention">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                A
                            </div>
                            <div class="matrix-name">Attention Weights</div>
                            <div class="matrix-dims">each row sums to 1</div>
                        </div>
                    </div>

                    <div class="computation-visual">
                        <h4 style="margin-bottom: 20px;">Softmax Transformation for "${focusTokenName}"</h4>
                        <canvas id="stepSoftmaxCanvas" width="600" height="300" style="display: block; margin: 0 auto;"></canvas>
                    </div>

                    <div class="matrix-visual-box" data-label="Attention Distribution for '${focusTokenName}'">
                        <div style="display: flex; flex-wrap: wrap; gap: 12px; justify-content: center; padding: 10px;">
                            ${tokens.map((token, idx) => {
                                const weight = weights[idx];
                                const barHeight = Math.max(weight / maxWeight * 100, 5);
                                const isHighest = weight === maxWeight;
                                return `
                                    <div style="display: flex; flex-direction: column; align-items: center; min-width: 70px;">
                                        <div style="height: 120px; display: flex; align-items: flex-end;">
                                            <div style="width: 45px; height: ${barHeight}%; background: ${isHighest ? 'linear-gradient(to top, #667eea, #764ba2)' : 'linear-gradient(to top, #a8c0ff, #3f5efb)'}; border-radius: 6px 6px 0 0; transition: height 0.3s;"></div>
                                        </div>
                                        <div style="font-weight: 700; margin-top: 8px; font-size: 11px; color: #495057;">${token}</div>
                                        <div style="font-size: 14px; font-weight: 700; color: ${isHighest ? '#667eea' : '#6c757d'};">${(weight * 100).toFixed(1)}%</div>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                        <div style="text-align: center; margin-top: 15px; padding: 12px; background: #e7f3ff; border-radius: 8px; font-weight: 600;">
                            âœ“ Sum of all weights = ${weights.reduce((a, b) => a + b, 0).toFixed(4)} â‰ˆ 1.0
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> Scaled scores <code>[${tokens.length} Ã— ${tokens.length}]</code></li>
                            <li><strong>Operation:</strong> Apply softmax to each row independently</li>
                            <li><strong>Output:</strong> Attention weights <code>[${tokens.length} Ã— ${tokens.length}]</code></li>
                            <li><strong>Property:</strong> Each row is a probability distribution (sums to 1)</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>Softmax converts raw scores into probabilities. For token "${focusTokenName}", it pays 
                        <strong>${(maxWeight * 100).toFixed(1)}%</strong> attention to the most relevant token. 
                        All ${tokens.length} weights sum to exactly 1, creating a weighted average in the next step.</p>
                    </div>
                </div>
            `;
        }

        function generateStep6_AttentionValues(tokens, head0, focusToken) {
            const focusTokenName = tokens[focusToken];
            const weights = head0.attentionWeights[focusToken];
            const headDim = head0.V[0].length;
            
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">7</span>
                        Multiply Attention Ã— Values
                    </div>
                    <div class="step-description">
                        Use attention weights to compute a weighted sum of value vectors.
                        This creates a <strong>context-aware representation</strong> for each token.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Matrix Dimensions</h4>
                        <div class="dim-row">
                            <div class="dim-badge attention">
                                <span class="dim-label">Attention A:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${tokens.length}]</span>
                            </div>
                            <div class="dim-operator">Ã—</div>
                            <div class="dim-badge value">
                                <span class="dim-label">Values V:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${headDim}]</span>
                            </div>
                            <div class="dim-operator">=</div>
                            <div class="dim-badge output">
                                <span class="dim-label">Head Output:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${headDim}]</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect square attention">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${tokens.length}</span>
                                A
                            </div>
                            <div class="matrix-name">Attention</div>
                            <div class="matrix-dims">probabilities</div>
                        </div>
                        <div class="op-symbol">Ã—</div>
                        <div class="matrix-block">
                            <div class="matrix-rect medium value">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${headDim}</span>
                                V
                            </div>
                            <div class="matrix-name">Values</div>
                            <div class="matrix-dims">content</div>
                        </div>
                        <div class="op-symbol">=</div>
                        <div class="matrix-block">
                            <div class="matrix-rect medium output">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${headDim}</span>
                                O
                            </div>
                            <div class="matrix-name">Head Output</div>
                            <div class="matrix-dims">contextualized</div>
                        </div>
                    </div>

                    <div class="matrix-visual-box" data-label="Weighted Sum for '${focusTokenName}'">
                        <div style="padding: 10px;">
                            <div style="text-align: center; margin-bottom: 20px; font-size: 14px; color: #495057;">
                                <strong>output[${focusTokenName}]</strong> = weighted combination of all value vectors
                            </div>
                            
                            ${tokens.map((token, idx) => {
                                const weight = weights[idx];
                                const isSignificant = weight > 0.1;
                                return `
                                    <div style="display: grid; grid-template-columns: 80px 100px 1fr 180px; gap: 15px; align-items: center; 
                                                margin: 12px 0; padding: 12px 15px; 
                                                background: ${isSignificant ? 'linear-gradient(90deg, rgba(102,126,234,0.1), rgba(102,126,234,0.05))' : '#f8f9fa'}; 
                                                border-radius: 8px; border-left: 4px solid ${isSignificant ? '#667eea' : '#dee2e6'};">
                                        <div style="font-weight: 700; color: ${isSignificant ? '#667eea' : '#6c757d'};">${token}</div>
                                        <div style="text-align: center;">
                                            <div style="font-size: 18px; font-weight: 700; color: ${isSignificant ? '#667eea' : '#aaa'};">
                                                ${(weight * 100).toFixed(1)}%
                                            </div>
                                        </div>
                                        <div style="position: relative; height: 20px; background: #e9ecef; border-radius: 10px; overflow: hidden;">
                                            <div style="position: absolute; left: 0; top: 0; height: 100%; width: ${weight * 100}%; 
                                                        background: ${isSignificant ? 'linear-gradient(90deg, #667eea, #764ba2)' : '#adb5bd'}; 
                                                        border-radius: 10px; transition: width 0.3s;"></div>
                                        </div>
                                        <div style="font-family: monospace; font-size: 10px; color: #6c757d; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;">
                                            Ã— [${head0.V[idx].slice(0, 3).map(v => v.toFixed(2)).join(', ')}...]
                                        </div>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                    </div>

                    <div class="result-box">
                        <h4 style="margin-bottom: 10px;">ðŸŽ¯ Output Vector for "${focusTokenName}"</h4>
                        <div style="font-family: monospace; font-size: 13px; background: rgba(0,0,0,0.1); padding: 12px; border-radius: 6px; margin: 10px 0;">
                            [${head0.output[focusToken].slice(0, 8).map(v => v.toFixed(3)).join(', ')}...]
                        </div>
                        <div style="margin-top: 10px; font-size: 14px;">
                            Dimension: <strong>${headDim}</strong> (same as d_k)
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> Attention weights <code>[${tokens.length}Ã—${tokens.length}]</code> and Values <code>[${tokens.length}Ã—${headDim}]</code></li>
                            <li><strong>Operation:</strong> Matrix multiplication A Ã— V</li>
                            <li><strong>Output:</strong> Head output <code>[${tokens.length} Ã— ${headDim}]</code></li>
                            <li><strong>Each row:</strong> Weighted average of all value vectors</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>This is the core of attention: each token's output is a weighted combination of <em>all</em> value vectors. 
                        High attention weight = more contribution. Token "${focusTokenName}" now contains information 
                        from tokens it deemed relevant, creating a <strong>context-aware representation</strong>.</p>
                    </div>
                </div>
            `;
        }

        function generateStep7_Concatenate(tokens, result, mhsa) {
            const headDim = mhsa.headDim;
            const concatDim = mhsa.numHeads * headDim;
            const headColors = ['#FF6B6B', '#4ECDC4', '#95E1D3', '#F7DC6F', '#DDA0DD', '#87CEEB', '#FFA07A', '#98D8C8'];
            
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">8</span>
                        Concatenate Multiple Heads
                    </div>
                    <div class="step-description">
                        Each of the <strong>${mhsa.numHeads} heads</strong> has produced its own output.
                        We concatenate these outputs along the feature dimension to combine all perspectives.
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Matrix Dimensions</h4>
                        <div class="dim-row">
                            ${Array.from({length: Math.min(mhsa.numHeads, 4)}, (_, h) => `
                                <div class="dim-badge" style="border-color: ${headColors[h]}; background: ${headColors[h]}22;">
                                    <span class="dim-label">Head ${h + 1}:</span>
                                    <span class="dim-shape">[${tokens.length}Ã—${headDim}]</span>
                                </div>
                            `).join('')}
                            ${mhsa.numHeads > 4 ? `<div class="dim-badge"><span class="dim-shape">... +${mhsa.numHeads - 4} more</span></div>` : ''}
                        </div>
                        <div class="dim-row">
                            <div class="dim-operator">âŠ• CONCAT âŠ•</div>
                        </div>
                        <div class="dim-row">
                            <div class="dim-badge output">
                                <span class="dim-label">Combined:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${concatDim}]</span>
                            </div>
                            <div class="dim-badge">
                                <span class="dim-label">Calculation:</span>
                                <span class="dim-shape">${mhsa.numHeads} heads Ã— ${headDim} dims = ${concatDim}</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        ${Array.from({length: Math.min(mhsa.numHeads, 4)}, (_, h) => `
                            <div class="matrix-block">
                                <div class="matrix-rect small" style="border-color: ${headColors[h]}; background: ${headColors[h]}22;">
                                    <span class="matrix-dim-label top" style="font-size: 9px;">${tokens.length}</span>
                                    <span class="matrix-dim-label right" style="font-size: 9px;">${headDim}</span>
                                    H${h + 1}
                                </div>
                                <div class="matrix-name" style="font-size: 11px;">Head ${h + 1}</div>
                            </div>
                            ${h < Math.min(mhsa.numHeads, 4) - 1 ? '<div class="op-symbol" style="font-size: 20px;">âŠ•</div>' : ''}
                        `).join('')}
                        <div class="op-symbol">=</div>
                        <div class="matrix-block">
                            <div class="matrix-rect wide output">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${concatDim}</span>
                                MH
                            </div>
                            <div class="matrix-name">Multi-Head</div>
                            <div class="matrix-dims">[n Ã— (hÂ·d_k)]</div>
                        </div>
                    </div>

                    <div class="matrix-visual-box" data-label="Visual: Concatenation Process">
                        <div style="text-align: center; margin-bottom: 20px;">
                            <strong>Each token gets a longer vector by joining all head outputs:</strong>
                        </div>
                        <div style="display: flex; justify-content: center; align-items: center; gap: 15px; flex-wrap: wrap; padding: 20px; background: #f8f9fa; border-radius: 8px;">
                            ${Array.from({length: mhsa.numHeads}, (_, h) => `
                                <div style="width: ${Math.max(30, 80/mhsa.numHeads)}px; height: 80px; background: ${headColors[h]}; border-radius: 4px; display: flex; align-items: center; justify-content: center; color: white; font-weight: 700; font-size: 12px;">
                                    ${headDim}d
                                </div>
                            `).join('<div style="font-size: 20px; color: #667eea;">+</div>')}
                            <div style="font-size: 28px; color: #667eea; margin: 0 15px;">=</div>
                            <div style="width: 120px; height: 80px; background: linear-gradient(90deg, ${headColors.slice(0, mhsa.numHeads).join(', ')}); border-radius: 4px; display: flex; align-items: center; justify-content: center; color: white; font-weight: 700; font-size: 14px;">
                                ${concatDim}d
                            </div>
                        </div>
                    </div>

                    <div style="margin: 30px 0;">
                        <h4 style="color: #667eea; margin-bottom: 15px;">ðŸ§  Why Multiple Heads?</h4>
                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 12px;">
                            ${Array.from({length: Math.min(mhsa.numHeads, 4)}, (_, h) => {
                                const focuses = [
                                    'Syntactic patterns<br>(grammar, structure)',
                                    'Semantic relations<br>(meaning, concepts)',
                                    'Long-range deps<br>(distant context)',
                                    'Local context<br>(adjacent words)'
                                ];
                                return `
                                    <div style="background: white; padding: 15px; border-radius: 8px; border-left: 4px solid ${headColors[h]};">
                                        <strong style="color: ${headColors[h]};">Head ${h + 1}</strong><br>
                                        <span style="font-size: 12px; color: #6c757d;">${focuses[h]}</span>
                                    </div>
                                `;
                            }).join('')}
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Step Summary</h4>
                        <ul>
                            <li><strong>Input:</strong> ${mhsa.numHeads} head outputs, each <code>[${tokens.length}Ã—${headDim}]</code></li>
                            <li><strong>Operation:</strong> Concatenate along feature dimension (axis=1)</li>
                            <li><strong>Output:</strong> Combined matrix <code>[${tokens.length} Ã— ${concatDim}]</code></li>
                            <li><strong>Purpose:</strong> Combine different attention perspectives</li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ What's Happening</h4>
                        <p>By running ${mhsa.numHeads} attention mechanisms in parallel, each head can learn to focus on different aspects. 
                        Concatenation preserves all this information. One head might capture syntax, another semantics, 
                        another long-range dependenciesâ€”all combined into one rich representation.</p>
                    </div>
                </div>
            `;
        }

        function generateStep8_FinalOutput(tokens, embeddings, result) {
            const numHeads = result.headOutputs.length;
            const headDim = result.headOutputs[0].output[0].length;
            const concatDim = numHeads * headDim;
            const embedDim = embeddings[0].length;
            
            return `
                <div class="step-content">
                    <div class="step-title">
                        <span class="step-number">9</span>
                        Final Output Projection
                    </div>
                    <div class="step-description">
                        Apply a final linear transformation W<sup>O</sup> to project the concatenated heads
                        back to the original embedding dimension. This completes the multi-head self-attention!
                    </div>

                    <div class="dimension-tracker">
                        <h4>ðŸ“ Final Matrix Dimensions</h4>
                        <div class="dim-row">
                            <div class="dim-badge" style="border-color: #764ba2; background: rgba(118,75,162,0.15);">
                                <span class="dim-label">Concatenated:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${concatDim}]</span>
                            </div>
                            <div class="dim-operator">Ã—</div>
                            <div class="dim-badge">
                                <span class="dim-label">W<sup>O</sup>:</span>
                                <span class="dim-shape">[${concatDim} Ã— ${embedDim}]</span>
                            </div>
                            <div class="dim-operator">=</div>
                            <div class="dim-badge output">
                                <span class="dim-label">Final Output:</span>
                                <span class="dim-shape">[${tokens.length} Ã— ${embedDim}]</span>
                            </div>
                        </div>
                        <div class="dim-row" style="margin-top: 15px;">
                            <div class="dim-badge" style="background: rgba(132,250,176,0.2); border-color: #28a745;">
                                <span class="dim-shape">âœ“ Output shape = Input shape: [${tokens.length} Ã— ${embedDim}]</span>
                            </div>
                        </div>
                    </div>

                    <div class="matrix-shape-display">
                        <div class="matrix-block">
                            <div class="matrix-rect wide" style="border-color: #764ba2; background: linear-gradient(135deg, rgba(118,75,162,0.1), rgba(118,75,162,0.2));">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${concatDim}</span>
                                MH
                            </div>
                            <div class="matrix-name">Multi-Head</div>
                            <div class="matrix-dims">[n Ã— hÂ·d_k]</div>
                        </div>
                        <div class="op-symbol">Ã—</div>
                        <div class="matrix-block">
                            <div class="matrix-rect medium" style="border-color: #6c757d;">
                                <span class="matrix-dim-label top">${concatDim}</span>
                                <span class="matrix-dim-label right">${embedDim}</span>
                                W<sup>O</sup>
                            </div>
                            <div class="matrix-name">Output Weights</div>
                            <div class="matrix-dims">[hÂ·d_k Ã— d_model]</div>
                        </div>
                        <div class="op-symbol">=</div>
                        <div class="matrix-block">
                            <div class="matrix-rect medium output" style="box-shadow: 0 0 20px rgba(132,250,176,0.5);">
                                <span class="matrix-dim-label top">${tokens.length}</span>
                                <span class="matrix-dim-label right">${embedDim}</span>
                                Y
                            </div>
                            <div class="matrix-name">Final Output</div>
                            <div class="matrix-dims">[n Ã— d_model]</div>
                        </div>
                    </div>

                    <div class="matrix-visual-box" data-label="Before vs After: Input â†’ Output Comparison">
                        <div style="overflow-x: auto;">
                            ${tokens.slice(0, Math.min(tokens.length, 4)).map((token, idx) => `
                                <div style="margin: 15px 0; padding: 18px; background: linear-gradient(135deg, #f8f9fa, #fff); border-radius: 10px; border: 2px solid #e9ecef;">
                                    <div style="display: flex; align-items: center; gap: 10px; margin-bottom: 12px;">
                                        <span style="background: #667eea; color: white; padding: 4px 12px; border-radius: 20px; font-size: 12px; font-weight: 600;">
                                            Token ${idx + 1}
                                        </span>
                                        <span style="font-weight: 700; font-size: 16px; color: #333;">"${token}"</span>
                                    </div>
                                    <div style="display: grid; grid-template-columns: 1fr 50px 1fr; gap: 10px; align-items: center;">
                                        <div>
                                            <div style="font-size: 11px; color: #6c757d; margin-bottom: 6px; text-transform: uppercase; letter-spacing: 0.5px;">Input Embedding</div>
                                            <div style="font-family: monospace; font-size: 10px; background: #f1f3f5; padding: 10px; border-radius: 6px; border-left: 3px solid #adb5bd;">
                                                [${embeddings[idx].slice(0, 5).map(v => v.toFixed(2)).join(', ')}...]
                                            </div>
                                            <div style="font-size: 10px; color: #868e96; margin-top: 4px;">Static â€¢ [${embedDim}d]</div>
                                        </div>
                                        <div style="text-align: center; font-size: 24px; color: #667eea;">â†’</div>
                                        <div>
                                            <div style="font-size: 11px; color: #667eea; margin-bottom: 6px; text-transform: uppercase; letter-spacing: 0.5px; font-weight: 600;">Output (Contextualized)</div>
                                            <div style="font-family: monospace; font-size: 10px; background: linear-gradient(135deg, #e7f3ff, #f0e7ff); padding: 10px; border-radius: 6px; border-left: 3px solid #667eea;">
                                                [${result.output[idx].slice(0, 5).map(v => v.toFixed(2)).join(', ')}...]
                                            </div>
                                            <div style="font-size: 10px; color: #667eea; margin-top: 4px; font-weight: 600;">Context-aware âœ¨ â€¢ [${embedDim}d]</div>
                                        </div>
                                    </div>
                                </div>
                            `).join('')}
                            ${tokens.length > 4 ? `<div style="text-align: center; color: #6c757d; padding: 10px;">... and ${tokens.length - 4} more tokens</div>` : ''}
                        </div>
                    </div>

                    <div class="result-box" style="background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 50%, #a8edea 100%);">
                        <h4 style="margin-bottom: 15px; font-size: 20px;">ðŸŽ‰ Multi-Head Self-Attention Complete!</h4>
                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; text-align: left; font-size: 14px;">
                            <div style="background: rgba(255,255,255,0.3); padding: 12px; border-radius: 8px;">
                                âœ“ <strong>${tokens.length} tokens</strong> processed
                            </div>
                            <div style="background: rgba(255,255,255,0.3); padding: 12px; border-radius: 8px;">
                                âœ“ <strong>${numHeads} attention heads</strong> combined
                            </div>
                            <div style="background: rgba(255,255,255,0.3); padding: 12px; border-radius: 8px;">
                                âœ“ Output: <strong>[${tokens.length} Ã— ${embedDim}]</strong>
                            </div>
                            <div style="background: rgba(255,255,255,0.3); padding: 12px; border-radius: 8px;">
                                âœ“ <strong>Contextualized</strong> representations
                            </div>
                        </div>
                    </div>

                    <div class="step-summary">
                        <h4>ðŸ“‹ Complete Pipeline Summary</h4>
                        <ul>
                            <li><strong>Step 1-2:</strong> Tokens â†’ Embeddings <code>[${tokens.length}Ã—${embedDim}]</code></li>
                            <li><strong>Step 3:</strong> Embeddings â†’ Q, K, V <code>[${tokens.length}Ã—${headDim}]</code> per head</li>
                            <li><strong>Step 4-5:</strong> QÃ—K<sup>T</sup> â†’ Scaled Scores <code>[${tokens.length}Ã—${tokens.length}]</code></li>
                            <li><strong>Step 6:</strong> Scores â†’ Attention Weights (softmax)</li>
                            <li><strong>Step 7:</strong> Attention Ã— V â†’ Head Outputs <code>[${tokens.length}Ã—${headDim}]</code></li>
                            <li><strong>Step 8:</strong> Concatenate ${numHeads} heads â†’ <code>[${tokens.length}Ã—${concatDim}]</code></li>
                            <li><strong>Step 9:</strong> Project â†’ Final Output <code>[${tokens.length}Ã—${embedDim}]</code></li>
                        </ul>
                    </div>

                    <div class="whats-happening">
                        <h4>ðŸŽ¯ The Big Picture</h4>
                        <p>Each token started as an isolated embedding. After self-attention, each token's representation 
                        now contains information from the <strong>entire sequence</strong>, weighted by relevance. 
                        This is how transformers understand context and relationships between words!</p>
                    </div>
                </div>
            `;
        }

        // Run on page load
        window.onload = function() {
            runAttention();
            renderArchitectureView();
        };
    </script>
</body>
</html>


