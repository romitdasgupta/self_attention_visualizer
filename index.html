<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Self-Attention Visualizer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .controls {
            padding: 25px;
            background: #f8f9fa;
            border-bottom: 2px solid #dee2e6;
        }

        .control-group {
            margin-bottom: 20px;
        }

        label {
            display: block;
            font-weight: 600;
            margin-bottom: 8px;
            color: #495057;
        }

        input[type="text"],
        input[type="number"],
        select {
            width: 100%;
            padding: 10px;
            border: 2px solid #dee2e6;
            border-radius: 6px;
            font-size: 14px;
            transition: border-color 0.3s;
        }

        input[type="text"]:focus,
        input[type="number"]:focus,
        select:focus {
            outline: none;
            border-color: #667eea;
        }

        .control-row {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }

        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        button:active {
            transform: translateY(0);
        }

        .content {
            padding: 25px;
        }

        .section {
            margin-bottom: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }

        .section-title {
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #495057;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section-title::before {
            content: '';
            width: 8px;
            height: 8px;
            background: #667eea;
            border-radius: 50%;
        }

        .visualization-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 15px;
        }

        .viz-card {
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .viz-card h3 {
            margin-bottom: 12px;
            color: #495057;
            font-size: 1.1em;
        }

        canvas {
            display: block;
            width: 100%;
            border-radius: 4px;
            border: 1px solid #dee2e6;
        }

        .matrix-display {
            font-family: 'Courier New', monospace;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 12px;
            line-height: 1.6;
        }

        .matrix-row {
            white-space: nowrap;
        }

        .matrix-value {
            display: inline-block;
            min-width: 60px;
            text-align: right;
            padding: 2px 4px;
        }

        .token-display {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin: 15px 0;
        }

        .token {
            background: white;
            padding: 8px 16px;
            border-radius: 20px;
            border: 2px solid #667eea;
            font-weight: 600;
            color: #667eea;
        }

        .math-step {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 6px;
            border-left: 3px solid #667eea;
        }

        .math-step h4 {
            color: #667eea;
            margin-bottom: 8px;
        }

        .formula {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            margin: 8px 0;
            overflow-x: auto;
        }

        .info-box {
            background: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .info-box strong {
            color: #1976D2;
        }

        .attention-grid {
            display: grid;
            gap: 2px;
            margin: 15px 0;
        }

        .attention-cell {
            padding: 8px;
            text-align: center;
            font-size: 11px;
            border-radius: 2px;
            color: white;
            font-weight: 600;
        }

        .head-container {
            margin-bottom: 20px;
            padding: 15px;
            background: white;
            border-radius: 8px;
            border: 2px solid #dee2e6;
        }

        .head-title {
            font-weight: 700;
            color: #667eea;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
            border-bottom: 2px solid #dee2e6;
        }

        .tab {
            padding: 10px 20px;
            background: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            cursor: pointer;
            font-weight: 600;
            color: #6c757d;
            transition: all 0.3s;
        }

        .tab.active {
            color: #667eea;
            border-bottom-color: #667eea;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .flow-diagram {
            display: flex;
            align-items: center;
            justify-content: space-around;
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 8px;
            overflow-x: auto;
        }

        .flow-step {
            text-align: center;
            padding: 15px;
            min-width: 120px;
        }

        .flow-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            font-weight: 600;
            margin-bottom: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .flow-arrow {
            font-size: 2em;
            color: #667eea;
            margin: 0 10px;
        }

        .qkv-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .qkv-card {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .qkv-card h3 {
            margin-bottom: 15px;
            color: #495057;
        }

        .qkv-query {
            border-left: 4px solid #FF6B6B;
        }

        .qkv-key {
            border-left: 4px solid #4ECDC4;
        }

        .qkv-value {
            border-left: 4px solid #95E1D3;
        }

        .vector-display {
            font-family: 'Courier New', monospace;
            background: #f8f9fa;
            padding: 10px;
            border-radius: 4px;
            font-size: 11px;
            margin: 8px 0;
            overflow-x: auto;
            max-height: 150px;
            overflow-y: auto;
        }

        .vector-row {
            margin: 4px 0;
        }

        .interaction-canvas {
            width: 100%;
            max-width: 600px;
            margin: 20px auto;
            display: block;
        }

        .meaning-box {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
        }

        .meaning-box h4 {
            margin-bottom: 10px;
        }

        .highlight-query {
            background: rgba(255, 107, 107, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .highlight-key {
            background: rgba(78, 205, 196, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .highlight-value {
            background: rgba(149, 225, 211, 0.3);
            padding: 2px 4px;
            border-radius: 3px;
        }

        .token-embedding-viz {
            display: flex;
            align-items: center;
            margin: 15px 0;
            background: white;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
        }

        .token-box {
            background: #667eea;
            color: white;
            padding: 10px 15px;
            border-radius: 6px;
            font-weight: 600;
            min-width: 80px;
            text-align: center;
        }

        .embedding-viz {
            display: flex;
            gap: 2px;
            margin: 0 15px;
        }

        .embedding-bar {
            width: 8px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 2px;
        }

        .projection-arrow {
            margin: 0 10px;
            font-size: 1.5em;
            color: #667eea;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Multi-Head Self-Attention Visualizer</h1>
            <p class="subtitle">Understanding Transformer Attention from First Principles</p>
        </header>

        <div class="controls">
            <div class="control-group">
                <label for="inputText">Input Sequence (space-separated tokens):</label>
                <input type="text" id="inputText" value="The cat sat on the mat" placeholder="Enter your text...">
            </div>

            <div class="control-row">
                <div class="control-group">
                    <label for="embedDim">Embedding Dimension:</label>
                    <input type="number" id="embedDim" value="64" min="8" max="512" step="8">
                </div>

                <div class="control-group">
                    <label for="numHeads">Number of Heads:</label>
                    <input type="number" id="numHeads" value="4" min="1" max="8">
                </div>

                <div class="control-group">
                    <label for="temperature">Temperature (for attention):</label>
                    <input type="number" id="temperature" value="1.0" min="0.1" max="2.0" step="0.1">
                </div>
            </div>

            <div class="control-group" style="margin-top: 15px;">
                <button onclick="runAttention()">Compute Attention</button>
            </div>
        </div>

        <div class="content">
            <div class="section">
                <div class="section-title">Input Tokens</div>
                <div id="tokenDisplay" class="token-display"></div>
                <div class="info-box">
                    <strong>What's happening:</strong> Each token is converted to a dense vector representation (embedding) of dimension d_model.
                </div>
            </div>

            <div class="section">
                <div class="section-title">Architecture Overview</div>
                <div id="architectureInfo"></div>
            </div>

            <div class="tabs">
                <button class="tab active" onclick="switchTab('overview')">Overview</button>
                <button class="tab" onclick="switchTab('heads')">Individual Heads</button>
                <button class="tab" onclick="switchTab('mathematics')">Mathematics</button>
            </div>

            <div id="overview" class="tab-content active">
                <div class="section">
                    <div class="section-title">Combined Attention Pattern</div>
                    <div class="viz-card">
                        <h3>Average Attention Weights Across All Heads</h3>
                        <canvas id="combinedAttentionCanvas"></canvas>
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> This shows the average attention pattern across all heads. Each cell (i,j) shows how much token i attends to token j.
                    </div>
                </div>
            </div>

            <div id="heads" class="tab-content">
                <div class="section">
                    <div class="section-title">Individual Head Attention Patterns</div>
                    <div id="headsContainer"></div>
                    <div class="info-box">
                        <strong>What's happening:</strong> Each head learns to attend to different aspects of the input. Some might focus on local patterns, others on long-range dependencies.
                    </div>
                </div>
            </div>

            <div id="mathematics" class="tab-content">
                <div class="section">
                    <div class="section-title">Step-by-Step Mathematical Breakdown</div>
                    <div id="mathBreakdown"></div>
                </div>
            </div>

            <div class="section">
                <div class="section-title">Deep Dive: Q, K, V Transformation</div>
                <div class="info-box">
                    <strong>Conceptual Understanding:</strong><br>
                    ‚Ä¢ <strong>Query (Q):</strong> "What am I looking for?" - Represents what each token wants to know<br>
                    ‚Ä¢ <strong>Key (K):</strong> "What do I offer?" - Represents what information each token provides<br>
                    ‚Ä¢ <strong>Value (V):</strong> "Here's my actual content" - The actual information to be aggregated<br><br>
                    The attention mechanism is like a database lookup: Q asks questions, K provides indices, and V contains the actual data.
                </div>
                <div id="qkvVisualization"></div>
            </div>

            <div class="section">
                <div class="section-title">Attention Mechanism Flow</div>
                <div id="attentionFlow"></div>
            </div>
        </div>
    </div>

    <script>
        let currentAttention = null;

        function switchTab(tabName) {
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));

            event.target.classList.add('active');
            document.getElementById(tabName).classList.add('active');
        }

        class MultiHeadSelfAttention {
            constructor(embedDim, numHeads, temperature = 1.0) {
                this.embedDim = embedDim;
                this.numHeads = numHeads;
                this.headDim = Math.floor(embedDim / numHeads);
                this.temperature = temperature;

                // Initialize weight matrices for each head
                this.initializeWeights();
            }

            initializeWeights() {
                this.heads = [];
                for (let h = 0; h < this.numHeads; h++) {
                    this.heads.push({
                        Wq: this.randomMatrix(this.embedDim, this.headDim),
                        Wk: this.randomMatrix(this.embedDim, this.headDim),
                        Wv: this.randomMatrix(this.embedDim, this.headDim)
                    });
                }
                // Output projection
                this.Wo = this.randomMatrix(this.numHeads * this.headDim, this.embedDim);
            }

            randomMatrix(rows, cols) {
                const matrix = [];
                const scale = Math.sqrt(2.0 / (rows + cols)); // Xavier initialization
                for (let i = 0; i < rows; i++) {
                    matrix[i] = [];
                    for (let j = 0; j < cols; j++) {
                        matrix[i][j] = (Math.random() - 0.5) * 2 * scale;
                    }
                }
                return matrix;
            }

            matmul(A, B) {
                const rowsA = A.length;
                const colsA = A[0].length;
                const colsB = B[0].length;
                const C = [];

                for (let i = 0; i < rowsA; i++) {
                    C[i] = [];
                    for (let j = 0; j < colsB; j++) {
                        let sum = 0;
                        for (let k = 0; k < colsA; k++) {
                            sum += A[i][k] * B[k][j];
                        }
                        C[i][j] = sum;
                    }
                }
                return C;
            }

            transpose(A) {
                const rows = A.length;
                const cols = A[0].length;
                const T = [];
                for (let j = 0; j < cols; j++) {
                    T[j] = [];
                    for (let i = 0; i < rows; i++) {
                        T[j][i] = A[i][j];
                    }
                }
                return T;
            }

            softmax(scores) {
                const result = [];
                for (let i = 0; i < scores.length; i++) {
                    const row = scores[i];
                    const maxScore = Math.max(...row);
                    const expScores = row.map(s => Math.exp((s - maxScore) / this.temperature));
                    const sumExp = expScores.reduce((a, b) => a + b, 0);
                    result[i] = expScores.map(e => e / sumExp);
                }
                return result;
            }

            scaledDotProductAttention(Q, K, V) {
                // Compute attention scores: Q @ K^T
                const KT = this.transpose(K);
                const scores = this.matmul(Q, KT);

                // Scale by sqrt(d_k)
                const scale = Math.sqrt(this.headDim);
                const scaledScores = scores.map(row =>
                    row.map(val => val / scale)
                );

                // Apply softmax
                const attentionWeights = this.softmax(scaledScores);

                // Apply attention to values: attention @ V
                const output = this.matmul(attentionWeights, V);

                return {
                    output,
                    attentionWeights,
                    scores: scaledScores
                };
            }

            forward(embeddings) {
                const seqLen = embeddings.length;
                const headOutputs = [];
                const headAttentions = [];

                // Process each head
                for (let h = 0; h < this.numHeads; h++) {
                    const head = this.heads[h];

                    // Project to Q, K, V
                    const Q = this.matmul(embeddings, head.Wq);
                    const K = this.matmul(embeddings, head.Wk);
                    const V = this.matmul(embeddings, head.Wv);

                    // Compute attention
                    const { output, attentionWeights, scores } =
                        this.scaledDotProductAttention(Q, K, V);

                    headOutputs.push({
                        Q, K, V, output, attentionWeights, scores
                    });
                    headAttentions.push(attentionWeights);
                }

                // Concatenate heads
                const concatenated = [];
                for (let i = 0; i < seqLen; i++) {
                    concatenated[i] = [];
                    for (let h = 0; h < this.numHeads; h++) {
                        concatenated[i].push(...headOutputs[h].output[i]);
                    }
                }

                // Final linear projection
                const finalOutput = this.matmul(concatenated, this.Wo);

                return {
                    output: finalOutput,
                    headOutputs,
                    headAttentions
                };
            }
        }

        function generateEmbeddings(tokens, embedDim) {
            const embeddings = [];
            for (let i = 0; i < tokens.length; i++) {
                const embedding = [];
                // Generate deterministic embeddings based on token hash
                const hash = tokens[i].split('').reduce((acc, char) =>
                    acc + char.charCodeAt(0), 0);
                for (let j = 0; j < embedDim; j++) {
                    const angle = (hash * 0.1 + j * 0.5) % (2 * Math.PI);
                    embedding[j] = Math.sin(angle) * 0.5;
                }
                embeddings.push(embedding);
            }
            return embeddings;
        }

        function drawAttentionHeatmap(canvas, attentionWeights, tokens) {
            const ctx = canvas.getContext('2d');
            const size = attentionWeights.length;
            const cellSize = Math.min(400 / size, 50);
            const margin = 80;

            canvas.width = size * cellSize + margin * 2;
            canvas.height = size * cellSize + margin * 2;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw heatmap
            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size; j++) {
                    const weight = attentionWeights[i][j];
                    const intensity = Math.floor(weight * 255);
                    ctx.fillStyle = `rgb(${255-intensity}, ${255-intensity/2}, 255)`;
                    ctx.fillRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw border
                    ctx.strokeStyle = '#dee2e6';
                    ctx.strokeRect(margin + j * cellSize, margin + i * cellSize, cellSize, cellSize);

                    // Draw value
                    ctx.fillStyle = weight > 0.5 ? 'white' : 'black';
                    ctx.font = `${Math.min(cellSize/3, 12)}px Arial`;
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    ctx.fillText(
                        weight.toFixed(2),
                        margin + j * cellSize + cellSize/2,
                        margin + i * cellSize + cellSize/2
                    );
                }
            }

            // Draw labels
            ctx.fillStyle = '#495057';
            ctx.font = '14px Arial';
            ctx.textAlign = 'right';
            for (let i = 0; i < size; i++) {
                ctx.fillText(tokens[i], margin - 10, margin + i * cellSize + cellSize/2);
            }

            ctx.textAlign = 'center';
            for (let j = 0; j < size; j++) {
                ctx.save();
                ctx.translate(margin + j * cellSize + cellSize/2, margin - 10);
                ctx.rotate(-Math.PI / 4);
                ctx.fillText(tokens[j], 0, 0);
                ctx.restore();
            }

            // Draw axis labels
            ctx.font = 'bold 16px Arial';
            ctx.fillStyle = '#667eea';
            ctx.textAlign = 'center';
            ctx.fillText('Keys (attending to)', canvas.width / 2, 30);
            ctx.save();
            ctx.translate(30, canvas.height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Queries (attending from)', 0, 0);
            ctx.restore();
        }

        function formatMatrix(matrix, label) {
            let html = `<h4>${label}</h4><div class="matrix-display">`;
            html += matrix.map((row, i) =>
                `<div class="matrix-row">[${row.map(v =>
                    `<span class="matrix-value">${v.toFixed(4)}</span>`
                ).join(' ')}]</div>`
            ).join('');
            html += '</div>';
            return html;
        }

        function runAttention() {
            const inputText = document.getElementById('inputText').value;
            const embedDim = parseInt(document.getElementById('embedDim').value);
            const numHeads = parseInt(document.getElementById('numHeads').value);
            const temperature = parseFloat(document.getElementById('temperature').value);

            const tokens = inputText.trim().split(/\s+/);

            // Display tokens
            document.getElementById('tokenDisplay').innerHTML = tokens.map(
                (t, i) => `<div class="token">${i+1}. ${t}</div>`
            ).join('');

            // Generate embeddings
            const embeddings = generateEmbeddings(tokens, embedDim);

            // Create and run attention
            const mhsa = new MultiHeadSelfAttention(embedDim, numHeads, temperature);
            const result = mhsa.forward(embeddings);
            currentAttention = result;

            // Display architecture info
            document.getElementById('architectureInfo').innerHTML = `
                <div class="info-box">
                    <strong>Configuration:</strong><br>
                    ‚Ä¢ Sequence Length: ${tokens.length} tokens<br>
                    ‚Ä¢ Embedding Dimension (d_model): ${embedDim}<br>
                    ‚Ä¢ Number of Heads: ${numHeads}<br>
                    ‚Ä¢ Head Dimension (d_k = d_model/num_heads): ${mhsa.headDim}<br>
                    ‚Ä¢ Temperature: ${temperature}
                </div>
            `;

            // Draw combined attention
            const avgAttention = averageAttentions(result.headAttentions);
            const combinedCanvas = document.getElementById('combinedAttentionCanvas');
            drawAttentionHeatmap(combinedCanvas, avgAttention, tokens);

            // Draw individual heads
            const headsContainer = document.getElementById('headsContainer');
            headsContainer.innerHTML = '';
            for (let h = 0; h < numHeads; h++) {
                const headDiv = document.createElement('div');
                headDiv.className = 'head-container';
                headDiv.innerHTML = `
                    <div class="head-title">Head ${h + 1}</div>
                    <canvas id="headCanvas${h}"></canvas>
                `;
                headsContainer.appendChild(headDiv);

                setTimeout(() => {
                    const canvas = document.getElementById(`headCanvas${h}`);
                    drawAttentionHeatmap(canvas, result.headAttentions[h], tokens);
                }, 10);
            }

            // Generate mathematical breakdown
            generateMathBreakdown(tokens, embeddings, mhsa, result);

            // Generate Q/K/V visualizations
            generateQKVVisualization(tokens, embeddings, mhsa, result);

            // Generate attention flow visualization
            generateAttentionFlowVisualization(tokens, embeddings, mhsa, result);
        }

        function averageAttentions(attentions) {
            const numHeads = attentions.length;
            const seqLen = attentions[0].length;
            const avg = [];

            for (let i = 0; i < seqLen; i++) {
                avg[i] = [];
                for (let j = 0; j < seqLen; j++) {
                    let sum = 0;
                    for (let h = 0; h < numHeads; h++) {
                        sum += attentions[h][i][j];
                    }
                    avg[i][j] = sum / numHeads;
                }
            }
            return avg;
        }

        function generateMathBreakdown(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('mathBreakdown');

            let html = `
                <div class="math-step">
                    <h4>Step 1: Input Embeddings</h4>
                    <p>Each token is represented as a ${mhsa.embedDim}-dimensional vector.</p>
                    <div class="formula">X ‚àà ‚Ñù^(${tokens.length} √ó ${mhsa.embedDim})</div>
                    ${formatMatrix(embeddings.map(e => e.slice(0, 8)), 'Embeddings (first 8 dimensions)')}
                </div>

                <div class="math-step">
                    <h4>Step 2: Linear Projections</h4>
                    <p>For each head h, we project the input into Query, Key, and Value spaces:</p>
                    <div class="formula">
                        Q_h = X ¬∑ W^Q_h<br>
                        K_h = X ¬∑ W^K_h<br>
                        V_h = X ¬∑ W^V_h<br>
                        where W^Q_h, W^K_h, W^V_h ‚àà ‚Ñù^(${mhsa.embedDim} √ó ${mhsa.headDim})
                    </div>
                    <div class="info-box">
                        Each head has its own projection matrices, allowing it to learn different representations.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 3: Scaled Dot-Product Attention</h4>
                    <p>For each head, we compute attention scores:</p>
                    <div class="formula">
                        scores = (Q ¬∑ K^T) / ‚àöd_k<br>
                        where d_k = ${mhsa.headDim}
                    </div>
                    <p>The scaling factor prevents the dot products from becoming too large.</p>
                </div>

                <div class="math-step">
                    <h4>Step 4: Softmax Normalization</h4>
                    <p>Apply softmax to get attention weights that sum to 1:</p>
                    <div class="formula">
                        Attention(Q, K, V) = softmax(scores / temperature) ¬∑ V<br>
                        temperature = ${mhsa.temperature}
                    </div>
                    <p>Higher temperature makes the distribution more uniform; lower makes it more peaked.</p>
                </div>
            `;

            // Show first head's computations in detail
            const head0 = result.headOutputs[0];
            html += `
                <div class="math-step">
                    <h4>Example: Head 1 Detailed Computation</h4>
                    ${formatMatrix(head0.Q.map(q => q.slice(0, 4)), 'Q (first 4 dims)')}
                    ${formatMatrix(head0.K.map(k => k.slice(0, 4)), 'K (first 4 dims)')}
                    ${formatMatrix(head0.scores, 'Attention Scores (scaled)')}
                    ${formatMatrix(head0.attentionWeights, 'Attention Weights (after softmax)')}
                </div>

                <div class="math-step">
                    <h4>Step 5: Concatenate Heads</h4>
                    <p>Concatenate outputs from all ${mhsa.numHeads} heads:</p>
                    <div class="formula">
                        MultiHead = Concat(head_1, head_2, ..., head_${mhsa.numHeads})<br>
                        where each head_i ‚àà ‚Ñù^(${tokens.length} √ó ${mhsa.headDim})
                    </div>
                    <p>Result: ‚Ñù^(${tokens.length} √ó ${mhsa.numHeads * mhsa.headDim})</p>
                </div>

                <div class="math-step">
                    <h4>Step 6: Output Projection</h4>
                    <p>Final linear transformation:</p>
                    <div class="formula">
                        Output = MultiHead ¬∑ W^O<br>
                        where W^O ‚àà ‚Ñù^(${mhsa.numHeads * mhsa.headDim} √ó ${mhsa.embedDim})
                    </div>
                    ${formatMatrix(result.output.map(o => o.slice(0, 8)), 'Final Output (first 8 dimensions)')}
                </div>

                <div class="info-box">
                    <strong>Key Insights:</strong><br>
                    ‚Ä¢ Each attention head can learn to focus on different relationships<br>
                    ‚Ä¢ The attention weights are position-dependent and learned from data<br>
                    ‚Ä¢ Self-attention allows every token to attend to every other token<br>
                    ‚Ä¢ Scaling by ‚àöd_k stabilizes gradients during training<br>
                    ‚Ä¢ The final output combines information from all attention heads
                </div>
            `;

            container.innerHTML = html;
        }

        function generateQKVVisualization(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('qkvVisualization');
            const head0 = result.headOutputs[0];

            let html = `
                <div class="math-step">
                    <h4>Transformation Pipeline for First Token: "${tokens[0]}"</h4>
                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box token-box">Token<br>"${tokens[0]}"</div>
                            <small>Input word</small>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                            <small>Dense vector</small>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #FF6B6B, #EE5A6F);">
                                Query<br>[${mhsa.headDim}d]
                            </div>
                            <small>What to look for</small>
                        </div>
                    </div>

                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #4ECDC4, #44A08D);">
                                Key<br>[${mhsa.headDim}d]
                            </div>
                            <small>What I offer</small>
                        </div>
                    </div>

                    <div class="flow-diagram">
                        <div class="flow-step">
                            <div class="flow-box">Embedding<br>[${mhsa.embedDim}d]</div>
                        </div>
                        <div class="flow-arrow">‚Üí</div>
                        <div class="flow-step">
                            <div class="flow-box" style="background: linear-gradient(135deg, #95E1D3, #74C7BB);">
                                Value<br>[${mhsa.headDim}d]
                            </div>
                            <small>Actual content</small>
                        </div>
                    </div>
                </div>

                <div class="meaning-box">
                    <h4>üéØ How Meaning Emerges</h4>
                    <p><strong>Step 1: Projection Matrices Learn Semantic Roles</strong></p>
                    <p>The weight matrices W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> are learned during training. They transform the generic embedding into specialized representations:</p>
                    <ul style="margin: 10px 0; padding-left: 20px;">
                        <li><span class="highlight-query">Query (Q)</span> encodes what information this token needs (e.g., "cat" might query for subjects or animals)</li>
                        <li><span class="highlight-key">Key (K)</span> encodes what information this token can provide (e.g., "sat" might provide action information)</li>
                        <li><span class="highlight-value">Value (V)</span> contains the semantic content to be retrieved</li>
                    </ul>
                </div>

                <div class="qkv-grid">
                    <div class="qkv-card qkv-query">
                        <h3>üîç Query Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">Q = Embedding √ó W<sup>Q</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.Q[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row shows what that token is "asking for"</small>
                    </div>

                    <div class="qkv-card qkv-key">
                        <h3>üîë Key Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">K = Embedding √ó W<sup>K</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.K[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row shows what that token "offers"</small>
                    </div>

                    <div class="qkv-card qkv-value">
                        <h3>üíé Value Vectors (Head 1)</h3>
                        <p style="font-size: 0.9em; margin-bottom: 10px;">V = Embedding √ó W<sup>V</sup></p>
                        <div class="vector-display">
            `;

            tokens.forEach((token, i) => {
                html += `<div class="vector-row"><strong>${token}:</strong> [${head0.V[i].slice(0, 6).map(v => v.toFixed(3)).join(', ')}...]</div>`;
            });

            html += `
                        </div>
                        <small>Each row contains the actual information</small>
                    </div>
                </div>

                <div class="math-step">
                    <h4>üìä Projection Weight Matrices (Head 1)</h4>
                    <p>These matrices transform ${mhsa.embedDim}-dimensional embeddings into ${mhsa.headDim}-dimensional Q/K/V vectors:</p>
                    <canvas id="weightMatrixCanvas" class="interaction-canvas"></canvas>
                </div>
            `;

            container.innerHTML = html;

            // Draw weight matrix visualization
            setTimeout(() => {
                drawWeightMatrixVisualization(mhsa);
            }, 10);
        }

        function drawWeightMatrixVisualization(mhsa) {
            const canvas = document.getElementById('weightMatrixCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const head = mhsa.heads[0];

            canvas.width = 900;
            canvas.height = 250;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            const matrices = [
                { matrix: head.Wq, name: 'W^Q', color: '#FF6B6B', x: 50 },
                { matrix: head.Wk, name: 'W^K', color: '#4ECDC4', x: 350 },
                { matrix: head.Wv, name: 'W^V', color: '#95E1D3', x: 650 }
            ];

            matrices.forEach(({ matrix, name, color, x }) => {
                const rows = Math.min(matrix.length, 16);
                const cols = Math.min(matrix[0].length, 8);
                const cellSize = 8;

                // Draw matrix
                for (let i = 0; i < rows; i++) {
                    for (let j = 0; j < cols; j++) {
                        const val = matrix[i][j];
                        const normalized = (val + 0.5); // Normalize to [0, 1]
                        const intensity = Math.floor(normalized * 255);
                        ctx.fillStyle = `rgba(${color.slice(1, 3)}, ${color.slice(3, 5)}, ${color.slice(5, 7)}, ${normalized})`;
                        ctx.fillRect(x + j * cellSize, 80 + i * cellSize, cellSize - 1, cellSize - 1);
                    }
                }

                // Draw labels
                ctx.fillStyle = '#495057';
                ctx.font = 'bold 16px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(name, x + cols * cellSize / 2, 60);

                ctx.font = '12px Arial';
                ctx.fillText(`${matrix.length} √ó ${matrix[0].length}`, x + cols * cellSize / 2, 220);

                // Draw border
                ctx.strokeStyle = color;
                ctx.lineWidth = 2;
                ctx.strokeRect(x, 80, cols * cellSize, rows * cellSize);
            });

            // Add description
            ctx.fillStyle = '#495057';
            ctx.font = '13px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Each matrix learns to extract different semantic aspects during training', canvas.width / 2, 245);
        }

        function generateAttentionFlowVisualization(tokens, embeddings, mhsa, result) {
            const container = document.getElementById('attentionFlow');
            const head0 = result.headOutputs[0];

            // Pick an example token to focus on
            const focusIdx = Math.floor(tokens.length / 2);
            const focusToken = tokens[focusIdx];

            let html = `
                <div class="meaning-box">
                    <h4>üß† How Attention Extracts Meaning: Example with "${focusToken}"</h4>
                    <p>Let's trace how token "${focusToken}" (position ${focusIdx + 1}) gathers information from other tokens:</p>
                </div>

                <div class="math-step">
                    <h4>Step 1: Compute Attention Scores (Q ¬∑ K<sup>T</sup>)</h4>
                    <p>The <span class="highlight-query">query</span> of "${focusToken}" is compared with the <span class="highlight-key">keys</span> of all tokens via dot product:</p>
                    <div class="flow-diagram" style="flex-wrap: wrap;">
            `;

            tokens.forEach((token, i) => {
                const score = head0.scores[focusIdx][i];
                const weight = head0.attentionWeights[focusIdx][i];
                html += `
                    <div class="flow-step" style="flex: 1; min-width: 120px;">
                        <div class="flow-box" style="background: rgba(102, 126, 234, ${weight});">
                            ${token}
                        </div>
                        <small>Score: ${score.toFixed(3)}<br>Weight: ${weight.toFixed(3)}</small>
                    </div>
                `;
            });

            html += `
                    </div>
                    <div class="info-box">
                        <strong>What's happening:</strong> High dot product (Q¬∑K) means the query and key are aligned in the learned semantic space. 
                        For example, if "cat" learns to query for subjects, and "sat" has a key indicating it's a verb, their dot product will be high.
                    </div>
                </div>

                <div class="math-step">
                    <h4>Step 2: Softmax Normalization</h4>
                    <p>Raw scores are converted to probabilities that sum to 1:</p>
                    <canvas id="softmaxCanvas" class="interaction-canvas"></canvas>
                </div>

                <div class="math-step">
                    <h4>Step 3: Weighted Aggregation of Values</h4>
                    <p>The attention weights determine how much of each token's <span class="highlight-value">value</span> vector to include:</p>
                    <div class="formula">
                        output[${focusToken}] = ${tokens.map((t, i) => 
                            `${head0.attentionWeights[focusIdx][i].toFixed(2)} √ó V[${t}]`
                        ).join(' + ')}
                    </div>
                    <p style="margin-top: 10px;">This weighted sum creates a context-aware representation of "${focusToken}" based on the entire sequence.</p>
                </div>

                <div class="math-step">
                    <h4>Visualizing the Aggregation</h4>
                    <canvas id="aggregationCanvas" class="interaction-canvas"></canvas>
                </div>

                <div class="meaning-box">
                    <h4>üéì Key Insights on Semantic Understanding</h4>
                    <ol style="margin: 10px 0; padding-left: 20px; line-height: 1.8;">
                        <li><strong>Learned Representations:</strong> During training, the projection matrices learn to encode linguistic patterns. W<sup>Q</sup> might learn to encode "what grammatical role does this word need to find?", W<sup>K</sup> encodes "what role do I play?", and W<sup>V</sup> holds the actual semantic content.</li>
                        
                        <li><strong>Dynamic Contextualization:</strong> Unlike static word embeddings, attention creates context-dependent representations. The word "bank" gets different representations in "river bank" vs "bank account" because it attends to different neighboring words.</li>
                        
                        <li><strong>Multiple Heads = Multiple Perspectives:</strong> With ${mhsa.numHeads} heads, the model can simultaneously attend to different aspects: one head might capture syntax, another semantics, another long-range dependencies.</li>
                        
                        <li><strong>Information Routing:</strong> Attention acts as a soft routing mechanism, allowing information to flow from relevant tokens to where it's needed. High attention = "this token is relevant to understanding me".</li>
                        
                        <li><strong>Permutation Invariance:</strong> Self-attention doesn't care about the order of tokens inherently (that's why positional encodings are added separately). It learns relationships based purely on content.</li>
                    </ol>
                </div>

                <div class="math-step">
                    <h4>üîÑ Comparison: Before and After Attention</h4>
                    <canvas id="comparisonCanvas" class="interaction-canvas"></canvas>
                    <p style="margin-top: 10px; text-align: center;">
                        <strong>Before:</strong> Static embedding based only on the token itself<br>
                        <strong>After:</strong> Contextualized representation incorporating information from the entire sequence
                    </p>
                </div>
            `;

            container.innerHTML = html;

            // Draw visualizations
            setTimeout(() => {
                drawSoftmaxVisualization(tokens, focusIdx, head0);
                drawAggregationVisualization(tokens, focusIdx, head0);
                drawComparisonVisualization(embeddings, result.output, focusIdx, tokens[focusIdx]);
            }, 10);
        }

        function drawSoftmaxVisualization(tokens, focusIdx, head0) {
            const canvas = document.getElementById('softmaxCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 300;

            const scores = head0.scores[focusIdx];
            const weights = head0.attentionWeights[focusIdx];

            const barWidth = 80;
            const gap = 20;
            const startX = (canvas.width - tokens.length * (barWidth + gap)) / 2;
            const baseY = 250;

            // Draw bars for before and after
            tokens.forEach((token, i) => {
                const x = startX + i * (barWidth + gap);
                
                // Raw score (scaled for visualization)
                const scoreHeight = Math.min(Math.abs(scores[i]) * 30, 60);
                ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
                ctx.fillRect(x, baseY - scoreHeight - 80, barWidth / 2 - 5, scoreHeight);

                // Softmax weight
                const weightHeight = weights[i] * 120;
                ctx.fillStyle = 'rgba(102, 126, 234, 0.8)';
                ctx.fillRect(x + barWidth / 2, baseY - weightHeight - 80, barWidth / 2 - 5, weightHeight);

                // Labels
                ctx.fillStyle = '#495057';
                ctx.font = '12px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(token, x + barWidth / 2, baseY - 60);
                ctx.font = '10px Arial';
                ctx.fillText(`${scores[i].toFixed(2)}`, x + barWidth / 4, baseY - scoreHeight - 85);
                ctx.fillText(`${weights[i].toFixed(2)}`, x + 3 * barWidth / 4, baseY - weightHeight - 85);
            });

            // Legend
            ctx.fillStyle = 'rgba(255, 107, 107, 0.6)';
            ctx.fillRect(20, 20, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.font = '12px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Raw Scores (Q¬∑K)', 40, 32);

            ctx.fillStyle = 'rgba(102, 126, 234, 0.8)';
            ctx.fillRect(20, 40, 15, 15);
            ctx.fillStyle = '#495057';
            ctx.fillText('After Softmax (Attention Weights)', 40, 52);
        }

        function drawAggregationVisualization(tokens, focusIdx, head0) {
            const canvas = document.getElementById('aggregationCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;

            const weights = head0.attentionWeights[focusIdx];
            const values = head0.V;

            // Draw value vectors as bars
            const numDims = 8; // Show first 8 dimensions
            const barWidth = 60;
            const gap = 10;
            const dimHeight = 30;

            let startY = 50;

            tokens.forEach((token, tokenIdx) => {
                const weight = weights[tokenIdx];
                const x = 50;

                // Token label
                ctx.fillStyle = '#495057';
                ctx.font = '12px Arial';
                ctx.textAlign = 'right';
                ctx.fillText(`${token} (${weight.toFixed(2)}√ó)`, x - 10, startY + 15);

                // Draw value vector bars
                for (let dim = 0; dim < numDims; dim++) {
                    const val = values[tokenIdx][dim];
                    const weightedVal = val * weight;
                    const barLen = Math.abs(val) * 40;
                    const weightedBarLen = Math.abs(weightedVal) * 40;

                    // Original value (faint)
                    ctx.fillStyle = 'rgba(149, 225, 211, 0.3)';
                    ctx.fillRect(x + dim * (barWidth / numDims), startY, barWidth / numDims - 2, 10);

                    // Weighted value
                    const hue = weightedVal > 0 ? 150 : 0;
                    ctx.fillStyle = `hsla(${hue}, 70%, 50%, ${Math.abs(weight)})`;
                    ctx.fillRect(x + dim * (barWidth / numDims), startY, (barWidth / numDims - 2) * Math.min(Math.abs(weightedVal) * 5, 1), 10);
                }

                startY += 20;
            });

            // Title
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Each row: Value vector √ó Attention weight', 50, 30);

            ctx.font = '11px Arial';
            ctx.fillText('Higher attention weight = more contribution to final output', 50, startY + 20);
        }

        function drawComparisonVisualization(embeddings, outputs, focusIdx, token) {
            const canvas = document.getElementById('comparisonCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 300;

            const numDims = Math.min(embeddings[focusIdx].length, 32);
            const barWidth = 500 / numDims;
            const startX = 50;

            // Draw input embedding
            let y = 80;
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 13px Arial';
            ctx.textAlign = 'left';
            ctx.fillText(`Input Embedding of "${token}"`, startX, y - 10);

            for (let i = 0; i < numDims; i++) {
                const val = embeddings[focusIdx][i];
                const height = Math.abs(val) * 50;
                ctx.fillStyle = val > 0 ? 'rgba(102, 126, 234, 0.6)' : 'rgba(234, 102, 126, 0.6)';
                ctx.fillRect(startX + i * barWidth, y + 50 - height, barWidth - 1, height);
            }

            // Draw output
            y = 180;
            ctx.fillStyle = '#495057';
            ctx.font = 'bold 13px Arial';
            ctx.fillText(`Contextualized Output of "${token}"`, startX, y - 10);

            for (let i = 0; i < numDims; i++) {
                const val = outputs[focusIdx][i];
                const height = Math.abs(val) * 50;
                ctx.fillStyle = val > 0 ? 'rgba(118, 75, 162, 0.8)' : 'rgba(162, 75, 118, 0.8)';
                ctx.fillRect(startX + i * barWidth, y + 50 - height, barWidth - 1, height);
            }

            // Arrow
            ctx.strokeStyle = '#667eea';
            ctx.lineWidth = 3;
            ctx.beginPath();
            ctx.moveTo(20, 110);
            ctx.lineTo(20, 190);
            ctx.stroke();

            // Arrowhead
            ctx.beginPath();
            ctx.moveTo(15, 180);
            ctx.lineTo(20, 190);
            ctx.lineTo(25, 180);
            ctx.fillStyle = '#667eea';
            ctx.fill();

            ctx.fillStyle = '#667eea';
            ctx.font = 'bold 12px Arial';
            ctx.save();
            ctx.translate(10, 150);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('ATTENTION', 0, 0);
            ctx.restore();
        }

        // Run on page load
        window.onload = function() {
            runAttention();
        };
    </script>
</body>
</html>
